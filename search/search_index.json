{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#data-platform-playbook","title":"Data Platform Playbook","text":"<p>A production-grade handbook for building and operating modern data platforms at scale.</p>"},{"location":"#welcome","title":"Welcome! \ud83d\udc4b","text":"<p>\u0928\u092e\u0938\u094d\u0924\u0947! (Namaste - Welcome in India \ud83c\uddee\ud83c\uddf3)</p> <p>This playbook provides actionable, opinionated guidance for data engineering teams operating at enterprise scale. It covers the full spectrum from foundational principles to advanced platform architecture, with a focus on cost efficiency, reliability, and self-serve capabilities.</p>"},{"location":"#who-this-is-for","title":"Who This Is For","text":"<ul> <li>Data Engineers - Building and maintaining data pipelines and platforms</li> <li>Data Engineering Managers - Building and scaling data teams</li> <li>Data Platform Managers - Designing and operating platforms  </li> <li>Staff / Principal Data Engineers - Making architectural decisions</li> <li>Platform Architects - Designing enterprise data systems</li> </ul>"},{"location":"#quick-navigation","title":"Quick Navigation","text":""},{"location":"#core-topics","title":"\ud83c\udfaf Core Topics","text":"<ul> <li> <p> Data Engineering</p> <p>Foundations, lifecycle, platform thinking, and cost efficiency</p> <p> Get started</p> </li> <li> <p> Data Ingestion</p> <p>Batch vs streaming, CDC, push vs pull patterns</p> <p> Learn more</p> </li> <li> <p> Data Architecture</p> <p>Storage design, lakehouse patterns, ingestion architecture</p> <p> Explore</p> </li> <li> <p> Data Orchestration</p> <p>Airflow, dbt, workflow management</p> <p> Discover</p> </li> <li> <p> Data Processing</p> <p>Spark, BigQuery, distributed processing</p> <p> Process</p> </li> <li> <p> Data Quality</p> <p>Governance, quality checks, SLAs, observability</p> <p> Ensure quality</p> </li> </ul>"},{"location":"#core-principles","title":"Core Principles","text":"<p>This playbook is built on these foundational principles:</p> <ul> <li>\ud83d\udce6 Data as a Product - Treat data assets as first-class products with clear ownership, SLAs, and contracts</li> <li>\ud83d\udd00 Separation of Concerns - Clear boundaries between ingestion, transformation, storage, and serving</li> <li>\ud83d\ude80 Platform Thinking - Build self-serve capabilities that enable teams, not bottlenecks</li> <li>\ud83d\udcb0 Cost Awareness - Every architectural decision should consider cost implications</li> <li>\ud83d\udca1 Opinionated Guidance - Clear recommendations, not generic explanations</li> </ul>"},{"location":"#what-youll-learn","title":"What You'll Learn","text":"<p>This playbook covers:</p> <ol> <li>Data Engineering - Core concepts, lifecycle, platform thinking</li> <li>Data Ingestion - Patterns, tools, and trade-offs for getting data in</li> <li>Data Architecture - Storage design, lakehouse, partitioning</li> <li>Data Orchestration - Scheduling, coordinating pipelines</li> <li>Data Processing - Spark, BigQuery, distributed processing</li> <li>Data Quality - Governance, checks, SLAs, observability</li> </ol>"},{"location":"#quotes","title":"Quotes","text":"<p>\"Data is a precious thing and will last longer than the systems themselves.\"</p> <p>\u2014 Tim Berners-Lee</p> <p>\"The biggest opportunity for managers isn't better data \u2014 it's making data problems understandable.\"</p> <p>\"The next generation doesn't need more dashboards. They need better stories about why the data matters.\"</p> <p>\"Data problems aren't boring. They're just badly explained.\"</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>New to Data Engineering?</p> <p>Start with Data Engineering to understand core concepts and principles.</p> <p>Building a Platform?</p> <p>Read Data Engineering \u2192 Platform &amp; Operating Model first to design your operating model.</p> <p>Optimizing Costs?</p> <p>Jump to Data Engineering \u2192 Cost Efficiency for practical optimization strategies.</p> <p>Evaluating Architecture?</p> <p>See Reference \u2192 Leadership View for frameworks and metrics.</p>"},{"location":"#about-the-author","title":"About the Author","text":"<p>Learn more about the author and their experience in data platform architecture and engineering.</p> <p> About</p>"},{"location":"#contributing","title":"Contributing","text":"<p>This playbook is designed to evolve. Contributions, corrections, and improvements are welcome!</p> <p>Last Updated: 2024 Maintained by: Sunil Kumar T C</p>"},{"location":"about/","title":"About","text":""},{"location":"about/#about","title":"About","text":""},{"location":"about/#sunil-kumar","title":"SUNIL KUMAR","text":"<p>Senior Software Engineering Manager | Data Platform &amp; Analytics</p> <p>Bengaluru, India \ud83d\udce7 sunilkumar.tc89@gmail.com \ud83d\udd17 LinkedIn: linkedin.com/in/sunil-kumar \ud83d\udd17 GitHub: data-platform-playbook</p>"},{"location":"about/#summary","title":"SUMMARY","text":"<p>Data Platform &amp; Engineering Leader with 12+ years of experience designing, scaling, and operating cloud-native data platforms across e-commerce, retail, telecom, and financial services. Proven track record of building petabyte-scale data ecosystems, driving multi-million-dollar cost optimizations, and leading high-performing engineering teams across AWS and GCP.</p> <p>Expertise spans data ingestion, distributed processing, analytics platforms, cost optimization, governance, and platform reliability.</p> <p>\"The biggest opportunity for managers isn't better data \u2014 it's making data problems understandable.\"</p>"},{"location":"about/#core-skills","title":"CORE SKILLS","text":"<ul> <li>Data Platforms: Apache Spark, PrestoDB, Hive, Apache Beam, Kafka</li> <li>Cloud: Google Cloud Platform (GCP), AWS, Dataproc, BigQuery</li> <li>Data Engineering: Batch &amp; Streaming Pipelines, CDC, Data Lakes, Warehouses</li> <li>Architecture: Distributed Systems, Platform Design, Scalability</li> <li>Leadership: Engineering Management, Mentorship, Cross-functional Delivery</li> <li>Operations: Cost Optimization (FinOps), SLOs, Reliability, Governance</li> </ul>"},{"location":"about/#professional-experience","title":"PROFESSIONAL EXPERIENCE","text":""},{"location":"about/#wayfair-senior-software-engineering-manager","title":"Wayfair \u2014 Senior Software Engineering Manager","text":"<p>Oct 2023 \u2013 Present | Bengaluru</p> <ul> <li>Lead data platform engineering teams responsible for cloud-native ingestion and analytics platforms on GCP</li> <li>Designed and operated pipelines processing 10TB+ data per day with 99.9% uptime</li> <li>Drove cost optimization initiatives across ingestion and processing layers, reducing recurring infrastructure spend</li> <li>Established platform ownership, SLOs, and operational standards across teams</li> <li>Partnered with product, analytics, and infra stakeholders to improve data freshness and reliability</li> </ul> <p>\"Data freshness is just trust, measured in minutes.\"</p>"},{"location":"about/#walmart-global-tech-senior-engineering-manager","title":"Walmart Global Tech \u2014 Senior Engineering Manager","text":"<p>Sep 2021 \u2013 Sep 2023 | Bengaluru</p> <ul> <li>Led teams managing data platforms storing 100+ PB of data and processing billions of events per day</li> <li>Scaled infrastructure supporting 50M+ daily transactions</li> <li>Reduced data processing costs by ~40% through optimization of Hadoop, Hive, and Spark workloads</li> <li>Built and mentored a team of 12+ engineers across data engineering and platform roles</li> </ul> <p>\"Cost explosions happen when no one feels ownership.\"</p>"},{"location":"about/#jio-platforms-reliance-senior-manager-data-engineering","title":"Jio Platforms (Reliance) \u2014 Senior Manager, Data Engineering","text":"<p>Mar 2018 \u2013 Sep 2021 | Bengaluru</p> <ul> <li>Architected and led the Jio Financial Services Data Lake, serving 400M+ users</li> <li>Designed ingestion from 20+ internal and external systems (banking, payments, NPCI, SAP, POS)</li> <li>Implemented secure, compliant data platforms aligned with RBI regulations</li> <li>Built real-time and batch pipelines using Kafka, Spark, Hive, PrestoDB</li> <li>Led teams across data engineering, backend, and DevOps</li> </ul> <p>\"Data engineering isn't plumbing. It's product design with consequences.\"</p>"},{"location":"about/#earlier-roles","title":"Earlier Roles","text":"<ul> <li>Lead Software Engineer, Intutel (Founding Team)</li> <li>Senior Software Engineer, Snapdeal</li> <li>System Engineer, Infosys (Finacle Product Team)</li> </ul>"},{"location":"about/#education","title":"EDUCATION","text":"<p>Birla Institute of Technology &amp; Science (BITS), Pilani Master's Degree \u2014 Data Processing / Software Systems</p> <p>Visvesvaraya Technological University Bachelor of Engineering \u2014 Computer Science</p>"},{"location":"about/#leadership-impact-highlights","title":"LEADERSHIP &amp; IMPACT HIGHLIGHTS","text":"<ul> <li>Built platforms serving 400M+ users</li> <li>Managed and mentored 20+ engineers</li> <li>Delivered $10M+ cost savings via cloud and pipeline optimization</li> <li>Operated mission-critical platforms with enterprise-grade reliability</li> </ul> <p>\"The next generation doesn't need more dashboards. They need better stories about why the data matters.\"</p>"},{"location":"about/#philosophy","title":"PHILOSOPHY","text":"<p>\"Data problems aren't boring. They're just badly explained.\"</p> <p>\"If Gen-Z doesn't care about your data problem, you've explained the wrong problem.\"</p> <p>\"Observability is just empathy for future engineers.\"</p> <p>\"Pipelines fail quietly. People fail when no one explains why they exist.\"</p> <p>Maintained by: Sunil Kumar T C</p>"},{"location":"data-architecture/","title":"Data Architecture","text":""},{"location":"data-architecture/#data-architecture","title":"Data Architecture","text":"<p>\"If a data problem can't be explained in one screen, the system is already broken.\"</p> <p>Designing storage systems that are fast, cost-effective, and scalable.</p>"},{"location":"data-architecture/#overview","title":"Overview","text":"<p>Storage is where data lives. Get the architecture right, and queries are fast, costs are low, and operations are smooth. Get it wrong, and you'll pay in performance, cost, and complexity.</p>"},{"location":"data-architecture/#key-topics","title":"Key Topics","text":""},{"location":"data-architecture/#storage","title":"Storage","text":"<p>Data lake vs warehouse, partitioning, formats, lifecycle policies.</p> <p>Learn about: - Data lake vs data warehouse - Storage tiers (hot, warm, cold) - Partitioning strategies - File formats (Parquet, Avro, Delta) - Compression and optimization</p>"},{"location":"data-architecture/#lakehouse","title":"Lakehouse","text":"<p>Modern approach combining lake storage with warehouse capabilities.</p> <p>Learn about: - Lakehouse architecture - Delta Lake, Iceberg, Hudi - Benefits and trade-offs - Implementation patterns</p>"},{"location":"data-architecture/#ingestion-architecture","title":"Ingestion Architecture","text":"<p>How ingestion fits into overall architecture.</p> <p>Learn about: - Ingestion patterns - Storage layer design - Data flow architecture - Scalability patterns</p>"},{"location":"data-architecture/#architecture-patterns","title":"Architecture Patterns","text":""},{"location":"data-architecture/#data-lake","title":"Data Lake","text":"<p>Characteristics: - Schema-on-read (flexible schemas) - File-based storage (S3, GCS, ADLS) - Supports structured, semi-structured, unstructured data - Lower storage cost - Requires compute engine (Spark, Presto) for queries</p> <p>Best for: - Raw data storage - Diverse data types (logs, images, documents) - Cost-sensitive, large volumes - ELT patterns (load first, transform later)</p>"},{"location":"data-architecture/#data-warehouse","title":"Data Warehouse","text":"<p>Characteristics: - Schema-on-write (enforced schemas) - Table-based storage - Optimized for SQL queries - Higher storage cost - Integrated compute</p> <p>Best for: - Curated, analysis-ready data - SQL-heavy workloads - Business intelligence, reporting - Fast query performance</p>"},{"location":"data-architecture/#lakehouse_1","title":"Lakehouse","text":"<p>Characteristics: - Combines lake storage with warehouse capabilities - Cost-effective storage (lake) - Fast queries (warehouse) - Single source of truth</p> <p>Best for: - Modern data platforms - Need both flexibility and performance - Cost-conscious organizations</p>"},{"location":"data-architecture/#storage-tiers","title":"Storage Tiers","text":"Tier Use Case Access Pattern Cost Hot Active queries, dashboards Frequent, low latency High Warm Ad-hoc analysis, reporting Occasional, moderate latency Medium Cold Compliance, historical Rare, high latency acceptable Low <p>Lifecycle Policies</p> <p>Automatically move data between tiers based on age/access patterns. Expected savings: 50-70% on storage costs.</p>"},{"location":"data-architecture/#related-topics","title":"Related Topics","text":"<ul> <li>Data Ingestion - Getting data into storage</li> <li>Data Processing - Processing stored data</li> <li>Data Quality - Ensuring data reliability</li> </ul> <p>Next: Storage \u2192</p>"},{"location":"data-architecture/ingestion-architecture/","title":"Ingestion Architecture","text":""},{"location":"data-architecture/ingestion-architecture/#ingestion-architecture","title":"Ingestion Architecture","text":"<p>How ingestion fits into your overall data architecture.</p>"},{"location":"data-architecture/ingestion-architecture/#overview","title":"Overview","text":"<p>Ingestion architecture defines how data flows from source systems into your platform. It's the foundation that everything else builds on.</p>"},{"location":"data-architecture/ingestion-architecture/#architecture-layers","title":"Architecture Layers","text":"<pre><code>Source Systems\n    \u2193\nIngestion Layer (Batch/Streaming/CDC)\n    \u2193\nRaw Storage (Lake)\n    \u2193\nTransformation Layer\n    \u2193\nCurated Storage (Lakehouse/Warehouse)\n    \u2193\nServing Layer (Analytics/ML/APIs)\n</code></pre>"},{"location":"data-architecture/ingestion-architecture/#ingestion-patterns","title":"Ingestion Patterns","text":""},{"location":"data-architecture/ingestion-architecture/#pattern-1-batch-ingestion","title":"Pattern 1: Batch Ingestion","text":"<p>Architecture: <pre><code>Source \u2192 Scheduled Job \u2192 Raw Storage (Parquet)\n</code></pre></p> <p>Characteristics: - Scheduled execution (hourly, daily) - Full or incremental extracts - Higher latency (minutes to hours) - Lower cost per GB</p> <p>Use when: - Historical loads - Large volumes - No real-time requirement</p>"},{"location":"data-architecture/ingestion-architecture/#pattern-2-streaming-ingestion","title":"Pattern 2: Streaming Ingestion","text":"<p>Architecture: <pre><code>Source \u2192 Message Queue (Kafka) \u2192 Stream Processor \u2192 Raw Storage\n</code></pre></p> <p>Characteristics: - Continuous processing - Low latency (seconds to minutes) - Higher cost per GB (3-5x batch) - More complex failure handling</p> <p>Use when: - Real-time requirements - Event-driven architecture - Low-latency use cases</p>"},{"location":"data-architecture/ingestion-architecture/#pattern-3-change-data-capture-cdc","title":"Pattern 3: Change Data Capture (CDC)","text":"<p>Architecture: <pre><code>Database \u2192 Transaction Log \u2192 CDC Tool \u2192 Message Queue \u2192 Storage\n</code></pre></p> <p>Characteristics: - Captures inserts, updates, deletes - Maintains transaction consistency - Lower overhead than full extracts - Real-time or near real-time</p> <p>Use when: - Database replication - Maintaining current state - Audit trails</p>"},{"location":"data-architecture/ingestion-architecture/#storage-architecture","title":"Storage Architecture","text":""},{"location":"data-architecture/ingestion-architecture/#raw-layer-design","title":"Raw Layer Design","text":"<p>Purpose: Preserve source data exactly as received</p> <p>Design principles: - Immutable - Never modify raw data (append-only) - Schema-on-read - Store in flexible formats - Partitioned - By ingestion time, source - Long retention - 7 years for compliance</p> <p>Format: Parquet (analytics), Avro (streaming), JSON (flexible)</p> <p>Example structure: <pre><code>raw/\n  source=web_events/\n    date=2024-01-15/\n      hour=10/\n        data.parquet\n</code></pre></p>"},{"location":"data-architecture/ingestion-architecture/#curated-layer-design","title":"Curated Layer Design","text":"<p>Purpose: Cleaned, validated, enriched data</p> <p>Design principles: - Schema-on-write - Enforced schemas - Partitioned - By business keys - Optimized - For query patterns - Versioned - Track changes over time</p> <p>Format: Delta Lake, Iceberg, Parquet</p> <p>Example structure: <pre><code>curated/\n  events/\n    date=2024-01-15/\n      data.delta\n</code></pre></p>"},{"location":"data-architecture/ingestion-architecture/#scalability-patterns","title":"Scalability Patterns","text":""},{"location":"data-architecture/ingestion-architecture/#horizontal-scaling","title":"Horizontal Scaling","text":"<p>Multiple ingestion workers: - Partition sources across workers - Each worker handles subset of sources - Scale workers based on load</p>"},{"location":"data-architecture/ingestion-architecture/#vertical-scaling","title":"Vertical Scaling","text":"<p>Larger instances: - More CPU/memory per worker - Handle larger sources - Better for single large sources</p>"},{"location":"data-architecture/ingestion-architecture/#hybrid-approach","title":"Hybrid Approach","text":"<p>Combine both: - Horizontal for many small sources - Vertical for large sources</p>"},{"location":"data-architecture/ingestion-architecture/#error-handling","title":"Error Handling","text":""},{"location":"data-architecture/ingestion-architecture/#retry-strategy","title":"Retry Strategy","text":"<p>Exponential backoff: <pre><code>max_retries = 5\nbase_delay = 1  # seconds\n\nfor attempt in range(max_retries):\n    try:\n        ingest(record)\n        break\n    except TransientError:\n        if attempt &lt; max_retries - 1:\n            delay = base_delay * (2 ** attempt)\n            sleep(delay)\n        else:\n            send_to_dlq(record)  # Dead letter queue\n</code></pre></p>"},{"location":"data-architecture/ingestion-architecture/#dead-letter-queue-dlq","title":"Dead Letter Queue (DLQ)","text":"<p>Purpose: Store records that failed after all retries</p> <p>Implementation: - Separate storage (S3, BigQuery table) - Alert on DLQ size - Manual review and reprocessing</p>"},{"location":"data-architecture/ingestion-architecture/#monitoring","title":"Monitoring","text":""},{"location":"data-architecture/ingestion-architecture/#key-metrics","title":"Key Metrics","text":"<p>Volume: - Records/second - GB/day - Partition count</p> <p>Latency: - End-to-end latency - Processing time per record - Queue depth</p> <p>Quality: - Schema validation failures - Duplicate rate - Missing data rate</p> <p>Reliability: - Success rate - Error rate by type - DLQ size</p>"},{"location":"data-architecture/ingestion-architecture/#cost-optimization","title":"Cost Optimization","text":""},{"location":"data-architecture/ingestion-architecture/#common-cost-traps","title":"Common Cost Traps","text":"<ol> <li>Over-ingestion - Ingesting unused data</li> <li>Inefficient formats - JSON instead of Parquet</li> <li>Redundant ingestion - Multiple pipelines for same source</li> <li>Streaming when batch would suffice - 3-5x cost premium</li> </ol>"},{"location":"data-architecture/ingestion-architecture/#optimization-techniques","title":"Optimization Techniques","text":"<ol> <li>Compression - Use Snappy or Zstd (2-5x reduction)</li> <li>Partitioning - Only process new partitions</li> <li>Incremental loads - Only fetch changed data</li> <li>Lifecycle policies - Move old data to cheaper storage</li> </ol>"},{"location":"data-architecture/ingestion-architecture/#related-topics","title":"Related Topics","text":"<ul> <li>Data Ingestion - Ingestion patterns</li> <li>Storage - Storage design</li> <li>Data Processing - Processing ingested data</li> </ul> <p>Next: Data Orchestration \u2192</p>"},{"location":"data-architecture/lakehouse/","title":"Lakehouse","text":""},{"location":"data-architecture/lakehouse/#lakehouse-architecture","title":"Lakehouse Architecture","text":"<p>Combining the flexibility of data lakes with the performance of data warehouses.</p>"},{"location":"data-architecture/lakehouse/#overview","title":"Overview","text":"<p>The Lakehouse is a modern data architecture that combines the cost-effective storage of data lakes with the performance and capabilities of data warehouses. It provides a single source of truth for all data types while maintaining both flexibility and performance.</p>"},{"location":"data-architecture/lakehouse/#what-is-a-lakehouse","title":"What is a Lakehouse?","text":"<p>Lakehouse = Data lake storage + warehouse capabilities</p> <p>Architecture: <pre><code>Raw Layer (Lake) \u2192 Curated Layer (Warehouse) \u2192 Serving Layer\n</code></pre></p>"},{"location":"data-architecture/lakehouse/#key-characteristics","title":"Key Characteristics","text":"<ul> <li>Open formats - Parquet, Delta, Iceberg (not proprietary)</li> <li>ACID transactions - Reliable updates, deletes</li> <li>Schema enforcement - Data quality at write time</li> <li>Time travel - Query historical versions</li> <li>Upserts - Update existing records efficiently</li> </ul>"},{"location":"data-architecture/lakehouse/#benefits","title":"Benefits","text":""},{"location":"data-architecture/lakehouse/#1-cost-efficiency","title":"1. Cost Efficiency","text":"<ul> <li>Storage: Object storage (S3, GCS) is 10x cheaper than warehouse storage</li> <li>Compute: Pay only for queries, not idle time</li> <li>Lifecycle: Move old data to cheaper tiers automatically</li> </ul>"},{"location":"data-architecture/lakehouse/#2-flexibility","title":"2. Flexibility","text":"<ul> <li>Multiple engines: Query with Spark, Presto, BigQuery, Snowflake</li> <li>Multiple formats: Support structured, semi-structured, unstructured</li> <li>Schema evolution: Handle changing schemas gracefully</li> </ul>"},{"location":"data-architecture/lakehouse/#3-performance","title":"3. Performance","text":"<ul> <li>Columnar formats: Parquet, Delta for fast analytics</li> <li>Partitioning: Query pruning for efficiency</li> <li>Indexing: Fast lookups when needed</li> </ul>"},{"location":"data-architecture/lakehouse/#4-single-source-of-truth","title":"4. Single Source of Truth","text":"<ul> <li>No duplication: One copy of data, multiple access patterns</li> <li>Consistency: Same data for all consumers</li> <li>Lineage: Clear data flow</li> </ul>"},{"location":"data-architecture/lakehouse/#lakehouse-formats","title":"Lakehouse Formats","text":""},{"location":"data-architecture/lakehouse/#delta-lake","title":"Delta Lake","text":"<p>Best for: ACID transactions, time travel, upserts</p> <p>Pros: - \u2705 ACID transactions - \u2705 Time travel (query historical versions) - \u2705 Upserts, deletes - \u2705 Schema evolution - \u2705 Metadata optimization</p> <p>Cons: - \u274c Requires compatible engines - \u274c More complex than Parquet</p> <p>Use when: - Need updates/deletes - Time travel required - Concurrent writes</p>"},{"location":"data-architecture/lakehouse/#apache-iceberg","title":"Apache Iceberg","text":"<p>Best for: Open format, multi-engine support</p> <p>Pros: - \u2705 Open format (not vendor-specific) - \u2705 Multi-engine support - \u2705 Good performance - \u2705 Partition evolution</p> <p>Cons: - \u274c Less mature than Delta - \u274c Smaller ecosystem</p> <p>Use when: - Want open format - Multi-engine requirements - Avoiding vendor lock-in</p>"},{"location":"data-architecture/lakehouse/#apache-hudi","title":"Apache Hudi","text":"<p>Best for: Real-time updates, incremental processing</p> <p>Pros: - \u2705 Real-time updates - \u2705 Incremental processing - \u2705 Good for streaming</p> <p>Cons: - \u274c Less mature - \u274c Smaller ecosystem</p> <p>Use when: - Real-time updates needed - Streaming use cases</p>"},{"location":"data-architecture/lakehouse/#architecture-layers","title":"Architecture Layers","text":""},{"location":"data-architecture/lakehouse/#1-raw-layer-bronze","title":"1. Raw Layer (Bronze)","text":"<p>Purpose: Preserve source data exactly as received</p> <p>Characteristics: - Immutable (append-only) - Schema-on-read - Long retention (7 years) - Partitioned by ingestion time</p> <p>Format: Parquet, JSON, Avro</p>"},{"location":"data-architecture/lakehouse/#2-curated-layer-silver","title":"2. Curated Layer (Silver)","text":"<p>Purpose: Cleaned, validated, enriched data</p> <p>Characteristics: - Schema-on-write - Quality checks applied - Enriched with reference data - Partitioned by business keys</p> <p>Format: Delta Lake, Iceberg, Parquet</p>"},{"location":"data-architecture/lakehouse/#3-serving-layer-gold","title":"3. Serving Layer (Gold)","text":"<p>Purpose: Analysis-ready, aggregated data</p> <p>Characteristics: - Optimized for queries - Pre-aggregated - Denormalized - Indexed</p> <p>Format: Delta Lake, Iceberg, or warehouse tables</p>"},{"location":"data-architecture/lakehouse/#implementation","title":"Implementation","text":""},{"location":"data-architecture/lakehouse/#example-delta-lake-on-s3","title":"Example: Delta Lake on S3","text":"<pre><code># Write to Delta Lake\ndf.write.format(\"delta\") \\\n    .mode(\"overwrite\") \\\n    .option(\"mergeSchema\", \"true\") \\\n    .partitionBy(\"date\") \\\n    .save(\"s3://lakehouse/curated/events\")\n\n# Query with Spark\nspark.read.format(\"delta\") \\\n    .load(\"s3://lakehouse/curated/events\") \\\n    .filter(\"date = '2024-01-15'\") \\\n    .show()\n\n# Upsert\nfrom delta.tables import DeltaTable\n\ndeltaTable = DeltaTable.forPath(spark, \"s3://lakehouse/curated/events\")\ndeltaTable.alias(\"target\").merge(\n    updatesDF.alias(\"source\"),\n    \"target.id = source.id\"\n).whenMatchedUpdateAll() \\\n.whenNotMatchedInsertAll() \\\n.execute()\n</code></pre>"},{"location":"data-architecture/lakehouse/#example-iceberg-on-gcs","title":"Example: Iceberg on GCS","text":"<pre><code># Write to Iceberg\ndf.write.format(\"iceberg\") \\\n    .mode(\"overwrite\") \\\n    .option(\"write-format\", \"parquet\") \\\n    .partitionBy(\"date\") \\\n    .save(\"gs://lakehouse/curated/events\")\n\n# Query with Spark\nspark.read.format(\"iceberg\") \\\n    .load(\"gs://lakehouse/curated/events\") \\\n    .filter(\"date = '2024-01-15'\") \\\n    .show()\n</code></pre>"},{"location":"data-architecture/lakehouse/#best-practices","title":"Best Practices","text":""},{"location":"data-architecture/lakehouse/#1-partitioning-strategy","title":"1. Partitioning Strategy","text":"<p>Time-based partitioning: <pre><code>events/\n  date=2024-01-15/\n    data.parquet\n</code></pre></p> <p>Benefits: - Query pruning - Lifecycle management - Parallel processing</p>"},{"location":"data-architecture/lakehouse/#2-schema-evolution","title":"2. Schema Evolution","text":"<p>Backward-compatible changes: - Add optional fields - Make required fields optional - Never remove fields (deprecate instead)</p>"},{"location":"data-architecture/lakehouse/#3-lifecycle-management","title":"3. Lifecycle Management","text":"<p>Automatically move old data: - Hot (0-30 days): Active queries - Warm (30-90 days): Occasional queries - Cold (90+ days): Archive, compliance</p>"},{"location":"data-architecture/lakehouse/#4-metadata-management","title":"4. Metadata Management","text":"<p>Track: - Schema versions - Partition information - Statistics (min/max values) - Lineage</p>"},{"location":"data-architecture/lakehouse/#comparison-lakehouse-vs-alternatives","title":"Comparison: Lakehouse vs Alternatives","text":"Aspect Data Lake Data Warehouse Lakehouse Storage Cost Low High Low Query Performance Medium High High Schema Flexibility High Low Medium ACID Transactions No Yes Yes Time Travel No Limited Yes Multi-Engine Yes No Yes"},{"location":"data-architecture/lakehouse/#when-to-use-lakehouse","title":"When to Use Lakehouse","text":"<p>Use Lakehouse when: - \u2705 Need cost-effective storage - \u2705 Need fast queries - \u2705 Need schema flexibility - \u2705 Need ACID transactions - \u2705 Want to avoid vendor lock-in</p> <p>Don't use Lakehouse when: - \u274c Simple use case (warehouse is enough) - \u274c No engineering resources (use managed warehouse) - \u274c Small scale (warehouse is simpler)</p>"},{"location":"data-architecture/lakehouse/#related-topics","title":"Related Topics","text":"<ul> <li>Storage - Storage fundamentals</li> <li>Data Ingestion - Getting data into lakehouse</li> <li>Data Processing - Processing lakehouse data</li> </ul> <p>Next: Ingestion Architecture \u2192</p>"},{"location":"data-architecture/storage/","title":"Storage","text":""},{"location":"data-architecture/storage/#storage-data-architecture","title":"Storage &amp; Data Architecture","text":"<p>\"Most data outages are just bad communication bugs.\"</p> <p>Storage is where data lives. Get the architecture right, and queries are fast, costs are low, and operations are smooth. Get it wrong, and you'll pay in performance, cost, and complexity.</p> <p>\"Gen-Z doesn't hate complexity. They hate unclear systems.\"</p>"},{"location":"data-architecture/storage/#data-lake-vs-data-warehouse","title":"Data Lake vs Data Warehouse","text":""},{"location":"data-architecture/storage/#data-lake","title":"Data Lake","text":"<p>Characteristics: - Schema-on-read (flexible schemas) - File-based storage (S3, GCS, ADLS) - Supports structured, semi-structured, unstructured data - Lower storage cost - Requires compute engine (Spark, Presto) for queries</p> <p>Best for: - Raw data storage - Diverse data types (logs, images, documents) - Cost-sensitive, large volumes - ELT patterns (load first, transform later)</p> <p>Tools: S3 + Spark, GCS + BigQuery, ADLS + Databricks</p>"},{"location":"data-architecture/storage/#data-warehouse","title":"Data Warehouse","text":"<p>Characteristics: - Schema-on-write (enforced schemas) - Table-based storage - Optimized for SQL queries - Higher storage cost - Integrated compute</p> <p>Best for: - Curated, analysis-ready data - SQL-heavy workloads - Business intelligence, reporting - Fast query performance</p> <p>Tools: BigQuery, Snowflake, Redshift, Databricks SQL</p>"},{"location":"data-architecture/storage/#modern-approach-lakehouse","title":"Modern Approach: Lakehouse","text":"<p>Lakehouse = Data lake storage + warehouse capabilities</p> <p>Architecture: <pre><code>Raw Layer (Lake) \u2192 Curated Layer (Warehouse) \u2192 Serving Layer\n</code></pre></p> <p>Benefits: - Cost-effective storage (lake) - Fast queries (warehouse) - Single source of truth - Flexible ingestion (lake) + optimized serving (warehouse)</p> <p>Implementation: Delta Lake, Iceberg, Hudi on object storage</p>"},{"location":"data-architecture/storage/#storage-tiers","title":"Storage Tiers","text":""},{"location":"data-architecture/storage/#hot-storage","title":"Hot Storage","text":"<p>Use case: Active queries, dashboards, real-time analytics</p> <p>Characteristics: - SSD-backed - Low latency (&lt; 100ms) - High cost ($0.023/GB/month for BigQuery) - Frequent access</p> <p>Examples: BigQuery active storage, Snowflake standard tier, S3 Standard</p>"},{"location":"data-architecture/storage/#warm-storage","title":"Warm Storage","text":"<p>Use case: Ad-hoc analysis, occasional queries</p> <p>Characteristics: - Standard storage - Moderate latency (&lt; 1s) - Medium cost ($0.01-0.02/GB/month) - Infrequent access</p> <p>Examples: BigQuery long-term storage, S3 Standard-IA, Snowflake transient</p>"},{"location":"data-architecture/storage/#cold-storage","title":"Cold Storage","text":"<p>Use case: Compliance, historical data, archives</p> <p>Characteristics: - Object storage - High latency (seconds to minutes) - Low cost ($0.004/GB/month) - Rare access</p> <p>Examples: S3 Glacier, GCS Coldline, Azure Archive</p>"},{"location":"data-architecture/storage/#lifecycle-policies","title":"Lifecycle Policies","text":"<p>Automatically move data between tiers:</p> <pre><code># Example: S3 lifecycle policy\n{\n  \"Rules\": [\n    {\n      \"Id\": \"Move to IA after 30 days\",\n      \"Status\": \"Enabled\",\n      \"Transitions\": [\n        {\n          \"Days\": 30,\n          \"StorageClass\": \"STANDARD_IA\"\n        }\n      ]\n    },\n    {\n      \"Id\": \"Move to Glacier after 90 days\",\n      \"Status\": \"Enabled\",\n      \"Transitions\": [\n        {\n          \"Days\": 90,\n          \"StorageClass\": \"GLACIER\"\n        }\n      ]\n    }\n  ]\n}\n</code></pre> <p>Expected savings: 50-70% on storage costs.</p>"},{"location":"data-architecture/storage/#partitioning-strategies","title":"Partitioning Strategies","text":""},{"location":"data-architecture/storage/#time-based-partitioning","title":"Time-Based Partitioning","text":"<p>Most common pattern:</p> <pre><code>events/\n  year=2024/\n    month=01/\n      day=15/\n        hour=10/\n          data.parquet\n</code></pre> <p>Benefits: - Query pruning (only scan relevant partitions) - Lifecycle management (delete old partitions) - Parallel processing</p> <p>Partition granularity: - Daily: Most common, good for most use cases - Hourly: High-volume, time-sensitive queries - Monthly: Low-volume, historical data</p>"},{"location":"data-architecture/storage/#key-based-partitioning","title":"Key-Based Partitioning","text":"<p>Use when queries filter on specific keys:</p> <pre><code>users/\n  user_id_hash=12345/\n    data.parquet\n</code></pre> <p>Hash partitioning: - Distributes data evenly - Prevents hot partitions - Good for high-cardinality keys</p> <p>Range partitioning: - Natural ordering (dates, IDs) - Efficient range queries - Risk of skew</p>"},{"location":"data-architecture/storage/#multi-level-partitioning","title":"Multi-Level Partitioning","text":"<p>Combine time + key:</p> <pre><code>events/\n  date=2024-01-15/\n    user_id_hash=12345/\n      data.parquet\n</code></pre> <p>Trade-off: More partitions = better pruning, but more metadata overhead.</p>"},{"location":"data-architecture/storage/#file-formats","title":"File Formats","text":""},{"location":"data-architecture/storage/#parquet","title":"Parquet","text":"<p>Best for: Analytics, columnar queries, large datasets</p> <p>Pros: - Highly compressed (5-10x) - Columnar (only read needed columns) - Schema embedded - Fast scans</p> <p>Cons: - Write overhead - Less flexible than JSON - Requires schema</p> <p>Use when: Analytics workloads, large tables, cost-sensitive</p>"},{"location":"data-architecture/storage/#avro","title":"Avro","text":"<p>Best for: Streaming, schema evolution, row-based access</p> <p>Pros: - Compact binary format - Schema evolution support - Row-based (good for streaming) - Schema embedded</p> <p>Cons: - Slower for analytics - Less compression than Parquet</p> <p>Use when: Streaming pipelines, Kafka, schema evolution needed</p>"},{"location":"data-architecture/storage/#delta-lake-iceberg","title":"Delta Lake / Iceberg","text":"<p>Best for: ACID transactions, time travel, upserts</p> <p>Pros: - ACID transactions - Time travel (query historical versions) - Upserts, deletes - Schema evolution - Metadata optimization</p> <p>Cons: - More complex than Parquet - Requires compatible engines</p> <p>Use when: Need updates/deletes, time travel, concurrent writes</p>"},{"location":"data-architecture/storage/#json","title":"JSON","text":"<p>Best for: Flexible schemas, nested data, APIs</p> <p>Pros: - Human-readable - No schema required - Flexible</p> <p>Cons: - Large size (no compression) - Slow queries - No schema enforcement</p> <p>Use when: Raw ingestion, flexible schemas, small volumes</p> <p>Recommendation: Convert JSON to Parquet post-ingestion.</p>"},{"location":"data-architecture/storage/#cdc-current-state-patterns","title":"CDC + Current State Patterns","text":""},{"location":"data-architecture/storage/#problem","title":"Problem","text":"<p>CDC streams capture changes (inserts, updates, deletes), but analytics often needs current state (latest value per key).</p>"},{"location":"data-architecture/storage/#solution-1-merge-pattern","title":"Solution 1: Merge Pattern","text":"<p>Merge CDC events into current state table:</p> <pre><code>-- Upsert pattern\nMERGE INTO current_state AS target\nUSING cdc_events AS source\nON target.id = source.id\nWHEN MATCHED AND source.op = 'UPDATE' THEN\n  UPDATE SET col1 = source.col1, updated_at = source.timestamp\nWHEN MATCHED AND source.op = 'DELETE' THEN\n  DELETE\nWHEN NOT MATCHED AND source.op = 'INSERT' THEN\n  INSERT (id, col1, updated_at) VALUES (source.id, source.col1, source.timestamp)\n</code></pre> <p>Tools: Spark, Flink, BigQuery MERGE</p>"},{"location":"data-architecture/storage/#solution-2-snapshot-incremental","title":"Solution 2: Snapshot + Incremental","text":"<p>Periodic snapshots + incremental updates:</p> <pre><code>-- Daily snapshot\nCREATE TABLE current_state_2024_01_15 AS\nSELECT * FROM current_state_2024_01_14\nUNION ALL\nSELECT * FROM cdc_events\nWHERE date = '2024-01-15'\n</code></pre> <p>Pros: Simple, easy to reprocess Cons: Storage overhead, slower queries</p>"},{"location":"data-architecture/storage/#solution-3-event-sourcing","title":"Solution 3: Event Sourcing","text":"<p>Store all events, compute current state on read:</p> <pre><code>-- Current state = latest event per key\nSELECT DISTINCT ON (id) *\nFROM events\nORDER BY id, timestamp DESC\n</code></pre> <p>Pros: Full history, audit trail Cons: Expensive queries, complex logic</p> <p>Recommendation: Use merge pattern for most cases. It's efficient and maintains current state.</p>"},{"location":"data-architecture/storage/#external-tables","title":"External Tables","text":""},{"location":"data-architecture/storage/#concept","title":"Concept","text":"<p>Query data in object storage (S3, GCS) without loading into warehouse.</p> <p>Architecture: <pre><code>S3/GCS (Parquet files) \u2192 External Table Definition \u2192 SQL Queries\n</code></pre></p> <p>Benefits: - No data duplication - Lower storage cost - Direct query on lake - Separation of storage and compute</p> <p>Trade-offs: - Slower than native tables - Limited optimization - Requires compatible formats</p>"},{"location":"data-architecture/storage/#implementation","title":"Implementation","text":"<p>BigQuery: <pre><code>CREATE EXTERNAL TABLE events\nOPTIONS (\n  format = 'PARQUET',\n  uris = ['gs://bucket/events/*.parquet']\n);\n</code></pre></p> <p>Snowflake: <pre><code>CREATE EXTERNAL TABLE events\nWITH LOCATION = @s3_stage/events/\nFILE_FORMAT = (TYPE = PARQUET);\n</code></pre></p> <p>Use when: Raw data, infrequent queries, cost-sensitive</p>"},{"location":"data-architecture/storage/#data-modeling-patterns","title":"Data Modeling Patterns","text":""},{"location":"data-architecture/storage/#star-schema-kimball","title":"Star Schema (Kimball)","text":"<p>Structure: - Fact tables: Transactions, events (large, append-only) - Dimension tables: Users, products, time (small, slowly changing)</p> <p>Example: <pre><code>fact_orders (order_id, user_id, product_id, date_id, amount)\ndim_users (user_id, name, email, ...)\ndim_products (product_id, name, category, ...)\ndim_date (date_id, date, month, year, ...)\n</code></pre></p> <p>Pros: - Simple, intuitive - Fast queries (pre-joined) - Standard BI tool support</p> <p>Cons: - Denormalization (storage overhead) - Rigid structure</p>"},{"location":"data-architecture/storage/#data-vault","title":"Data Vault","text":"<p>Structure: - Hubs: Business keys (users, products) - Links: Relationships (user_orders) - Satellites: Attributes (user_details, order_details)</p> <p>Pros: - Auditability (full history) - Flexible (easy to add attributes) - Scalable</p> <p>Cons: - Complex queries (many joins) - Steeper learning curve</p>"},{"location":"data-architecture/storage/#one-big-table-obt","title":"One Big Table (OBT)","text":"<p>Structure: - Single denormalized table - All attributes in one place</p> <p>Pros: - Simple queries (no joins) - Fast for specific use cases</p> <p>Cons: - High storage cost - Update complexity - Less flexible</p> <p>Recommendation: Start with star schema. It's the most practical for analytics.</p>"},{"location":"data-architecture/storage/#compression-optimization","title":"Compression &amp; Optimization","text":""},{"location":"data-architecture/storage/#compression-algorithms","title":"Compression Algorithms","text":"Algorithm Compression Ratio Speed Use Case Snappy 2-3x Fast Real-time, streaming Gzip 3-5x Medium General purpose Zstd 4-6x Fast Best balance LZ4 2-3x Very fast Low latency Brotli 5-7x Slow Archive, cold storage <p>Recommendation: Use Zstd for most cases. Best compression/speed ratio.</p>"},{"location":"data-architecture/storage/#columnar-optimization","title":"Columnar Optimization","text":"<p>For Parquet files: - Sort by: Frequently filtered columns - Dictionary encoding: Low-cardinality columns - Bloom filters: Fast existence checks - Statistics: Min/max for pruning</p> <pre><code>df.write.parquet(\n    path,\n    partitionBy=['date'],\n    sortBy=['user_id'],  # Sort within partitions\n    compression='zstd'\n)\n</code></pre>"},{"location":"data-architecture/storage/#retention-archival","title":"Retention &amp; Archival","text":""},{"location":"data-architecture/storage/#retention-policies","title":"Retention Policies","text":"<p>Define by data type:</p> Data Type Retention Rationale Raw events 7 years Compliance, reprocessing Curated tables 2 years Business needs Aggregations 1 year Historical trends Logs 90 days Debugging, audit"},{"location":"data-architecture/storage/#archival-strategy","title":"Archival Strategy","text":"<ol> <li>Identify candidates: Low access, old data</li> <li>Compress: Use high compression (Brotli)</li> <li>Move to cold storage: Glacier, Coldline</li> <li>Update metadata: Mark as archived</li> <li>Delete from hot storage: After archival confirmed</li> </ol> <p>Automation: Use lifecycle policies or scheduled jobs.</p>"},{"location":"data-architecture/storage/#monitoring-storage","title":"Monitoring Storage","text":""},{"location":"data-architecture/storage/#key-metrics","title":"Key Metrics","text":"<p>Volume: - Total size (GB, TB) - Growth rate (%/month) - Partition count</p> <p>Cost: - Storage cost per GB - Cost by tier (hot/warm/cold) - Cost trends</p> <p>Performance: - Query scan size (GB) - Partition pruning efficiency - Cache hit rate</p> <p>Health: - Orphaned partitions - Small file count (many small files = slow) - Compression ratio</p>"},{"location":"data-architecture/storage/#alerting","title":"Alerting","text":"<p>Critical: - Storage cost spike &gt; 20% - Growth rate &gt; 50%/month - Partition count &gt; 10,000 (may impact performance)</p> <p>Warning: - Small files detected (&gt; 1000 files &lt; 10MB) - Compression ratio dropping - Unused tables (no queries in 90 days)</p>"},{"location":"data-architecture/storage/#next-steps","title":"Next Steps","text":"<ul> <li>Platform &amp; Operating Model - How to organize your platform</li> <li>Cost Efficiency &amp; Scale - Advanced optimization techniques</li> </ul>"},{"location":"data-engineering/","title":"Data Engineering","text":""},{"location":"data-engineering/#data-engineering","title":"Data Engineering","text":"<p>The discipline of designing, building, and operating systems that transform raw data into reliable, accessible, and actionable information at scale.</p> <p>\u0928\u092e\u0938\u094d\u0924\u0947! (Namaste - Welcome in India \ud83c\uddee\ud83c\uddf3)</p>"},{"location":"data-engineering/#overview","title":"Overview","text":"<p>Data Engineering sits at the intersection of infrastructure, reliability, and data product delivery. Unlike data science (which focuses on analysis) or software engineering (which focuses on application logic), data engineering is about building platforms that enable teams to work with data at scale.</p>"},{"location":"data-engineering/#core-concepts","title":"Core Concepts","text":""},{"location":"data-engineering/#what-is-data-engineering","title":"What is Data Engineering?","text":"<p>Modern data engineering is about:</p> <ol> <li>Reliability - Ensuring data arrives on time, in the right format, with the right quality</li> <li>Scale - Handling terabytes to petabytes of data across thousands of pipelines</li> <li>Velocity - Supporting both batch and real-time use cases</li> <li>Governance - Maintaining lineage, quality, and compliance</li> <li>Cost Efficiency - Delivering value without breaking the bank</li> </ol>"},{"location":"data-engineering/#the-shift-from-etl-to-platform","title":"The Shift: From ETL to Platform","text":"<p>Platform Thinking</p> <p>Traditional data engineering focused on ETL pipelines\u2014point-to-point data movement. Modern data engineering is about platforms\u2014self-serve infrastructure that enables teams.</p>"},{"location":"data-engineering/#platform-capabilities","title":"Platform Capabilities","text":"<p>A mature data platform provides integrated capabilities across the data lifecycle:</p> <p>Ingestion &amp; Integration \u2014 Standardized patterns for batch, streaming, and CDC ingestion with schema validation and contract enforcement</p> <p>Compute &amp; Processing \u2014 Managed compute environments (Spark, Flink, Dataflow) with auto-scaling and standardized transformation frameworks</p> <p>Storage &amp; Data Management \u2014 Multi-tier storage abstractions with automated lifecycle policies, partitioning, and data versioning</p> <p>Discovery &amp; Governance \u2014 Centralized metadata catalog with lineage tracking, ownership models, and access control</p> <p>Operations &amp; Reliability \u2014 End-to-end observability, automated alerting, incident management, and SLA compliance tracking</p>"},{"location":"data-engineering/#key-topics","title":"Key Topics","text":""},{"location":"data-engineering/#foundations","title":"Foundations","text":"<p>Modern definition of Data Engineering, core principles, and platform thinking.</p> <p>Learn about: - Data as a Product - Separation of Concerns - Platform Thinking - Cost Awareness - Contract-First Design</p>"},{"location":"data-engineering/#lifecycle","title":"Lifecycle","text":"<p>Complete data journey: ingestion \u2192 transformation \u2192 storage \u2192 serving.</p> <p>Learn about: - Ingestion patterns (batch, streaming, CDC) - Storage layers (raw, curated, archive) - Transformation strategies (ELT vs ETL) - Serving patterns (analytics, ML, operational)</p>"},{"location":"data-engineering/#platform-operating-model","title":"Platform &amp; Operating Model","text":"<p>How to structure your platform organization and processes.</p> <p>Learn about: - Central vs Domain Ownership - Paved Paths and Escape Hatches - Contract-First Ingestion - Cost Attribution - Self-Serve Capabilities</p>"},{"location":"data-engineering/#cost-efficiency","title":"Cost Efficiency","text":"<p>Practical strategies to reduce costs by 20-40% without sacrificing quality.</p> <p>Learn about: - Common cost traps - Optimization strategies - Streaming vs micro-batch - Zombie pipeline detection</p>"},{"location":"data-engineering/#principles","title":"Principles","text":"<p>This playbook is built on these core principles:</p> <ul> <li>\ud83d\udce6 Data as a Product - Treat data assets as first-class products</li> <li>\ud83d\udd00 Separation of Concerns - Clear boundaries between layers</li> <li>\ud83d\ude80 Platform Thinking - Build self-serve capabilities</li> <li>\ud83d\udcb0 Cost Awareness - Every decision considers cost</li> <li>\ud83d\udca1 Opinionated Guidance - Clear recommendations, not generic explanations</li> </ul>"},{"location":"data-engineering/#quick-start","title":"Quick Start","text":"<p>New to Data Engineering?</p> <p>Start with Foundations to understand core concepts and principles.</p> <p>Building a Platform?</p> <p>Read Platform &amp; Operating Model first to design your operating model.</p> <p>Optimizing Costs?</p> <p>Jump to Cost Efficiency for practical optimization strategies.</p>"},{"location":"data-engineering/#related-topics","title":"Related Topics","text":"<ul> <li>Data Ingestion - Getting data into your platform</li> <li>Data Architecture - Storage and data organization</li> <li>Data Quality - Ensuring data reliability</li> </ul> <p>Next: Foundations \u2192</p>"},{"location":"data-engineering/cost-efficiency/","title":"Cost Efficiency","text":""},{"location":"data-engineering/cost-efficiency/#cost-efficiency-scale","title":"Cost Efficiency &amp; Scale","text":"<p>\"Cost explosions happen when no one feels ownership.\"</p> <p>Cost optimization isn't about cutting corners\u2014it's about spending wisely. At scale, small inefficiencies compound into massive waste. This chapter provides practical, actionable guidance on reducing costs by 20-40% without sacrificing quality or performance.</p> <p>\"Every broken pipeline started as 'we'll clean it later.'\"</p>"},{"location":"data-engineering/cost-efficiency/#cost-drivers","title":"Cost Drivers","text":""},{"location":"data-engineering/cost-efficiency/#understanding-your-costs","title":"Understanding Your Costs","text":"<p>Typical cost breakdown:</p> Category % of Total Description Compute 40-60% Query execution, transformations Storage 20-30% Data storage across tiers Network 5-15% Data transfer, egress Operations 5-10% Pipeline orchestration, monitoring <p>Compute costs: - Query execution (BigQuery, Snowflake slots) - Transformation jobs (Spark, Dataflow) - Streaming processing (Flink, Kafka)</p> <p>Storage costs: - Hot storage (frequently accessed) - Warm storage (occasional access) - Cold storage (rarely accessed)</p> <p>Network costs: - Cross-region transfers - Egress to internet - Inter-service communication</p>"},{"location":"data-engineering/cost-efficiency/#common-cost-traps","title":"Common Cost Traps","text":""},{"location":"data-engineering/cost-efficiency/#trap-1-over-ingestion","title":"Trap 1: Over-Ingestion","text":"<p>Problem: Ingesting data that's never used.</p> <p>Symptoms: - Tables with zero queries in 90 days - High storage cost, low usage - \"Just in case\" ingestion</p> <p>Solution: <pre><code>-- Identify unused tables\nSELECT\n  table_name,\n  storage_bytes,\n  last_query_time,\n  days_since_last_query\nFROM table_usage_stats\nWHERE days_since_last_query &gt; 90\nORDER BY storage_bytes DESC\n</code></pre></p> <p>Action: Archive or delete unused data. Expected savings: 10-20%.</p>"},{"location":"data-engineering/cost-efficiency/#trap-2-inefficient-file-formats","title":"Trap 2: Inefficient File Formats","text":"<p>Problem: Using JSON or CSV instead of Parquet.</p> <p>Cost impact: - JSON: 5-10x larger than Parquet - CSV: 3-5x larger than Parquet - Higher storage + compute costs</p> <p>Solution: Convert to Parquet post-ingestion.</p> <pre><code># Convert JSON to Parquet\ndf = spark.read.json(\"s3://raw/events/*.json\")\ndf.write.parquet(\"s3://raw/events_parquet/\", compression=\"zstd\")\n</code></pre> <p>Expected savings: 50-70% on storage, 30-50% on compute.</p>"},{"location":"data-engineering/cost-efficiency/#trap-3-streaming-when-batch-would-suffice","title":"Trap 3: Streaming When Batch Would Suffice","text":"<p>Problem: Using streaming for use cases that don't need real-time.</p> <p>Cost impact: Streaming is 3-5x more expensive than batch.</p> <p>Decision framework: - Real-time requirement (&lt; 1 min)? \u2192 Streaming - Near real-time (1-15 min)? \u2192 Micro-batch - Batch acceptable (15+ min)? \u2192 Batch</p> <p>Example: <pre><code>Use case: Daily reporting dashboard\nCurrent: Streaming (cost: $1000/month)\nBetter: Daily batch (cost: $200/month)\nSavings: $800/month (80%)\n</code></pre></p>"},{"location":"data-engineering/cost-efficiency/#trap-4-no-lifecycle-policies","title":"Trap 4: No Lifecycle Policies","text":"<p>Problem: All data in expensive hot storage.</p> <p>Solution: Automatically move to cheaper tiers.</p> <pre><code># Example: S3 lifecycle policy\nlifecycle:\n  - days: 30\n    move_to: STANDARD_IA  # 50% cheaper\n  - days: 90\n    move_to: GLACIER      # 80% cheaper\n</code></pre> <p>Expected savings: 50-70% on old data.</p>"},{"location":"data-engineering/cost-efficiency/#trap-5-small-files-problem","title":"Trap 5: Small Files Problem","text":"<p>Problem: Many small files (e.g., 10,000 files of 1MB each).</p> <p>Impact: - Slow queries (many file opens) - Higher compute cost (overhead) - Inefficient storage</p> <p>Solution: Compact small files.</p> <pre><code># Compact small files\ndf = spark.read.parquet(\"s3://data/partition=2024-01-15/\")\ndf.coalesce(10).write.parquet(\"s3://data/partition=2024-01-15/\")\n</code></pre> <p>Target: 100-500MB per file (for Parquet).</p>"},{"location":"data-engineering/cost-efficiency/#trap-6-redundant-processing","title":"Trap 6: Redundant Processing","text":"<p>Problem: Multiple pipelines processing same data.</p> <p>Symptoms: - Same source ingested multiple times - Same transformation computed multiple times - Duplicate storage</p> <p>Solution: Centralize, reuse outputs.</p> <pre><code>-- Instead of:\nSELECT * FROM raw.events WHERE date = '2024-01-15'  -- Pipeline A\nSELECT * FROM raw.events WHERE date = '2024-01-15'  -- Pipeline B\n\n-- Do:\nCREATE TABLE shared.events_2024_01_15 AS\nSELECT * FROM raw.events WHERE date = '2024-01-15'\n\n-- Both pipelines use shared table\n</code></pre>"},{"location":"data-engineering/cost-efficiency/#optimization-strategies","title":"Optimization Strategies","text":""},{"location":"data-engineering/cost-efficiency/#1-right-size-compute","title":"1. Right-Size Compute","text":"<p>Problem: Over-provisioned compute (paying for unused capacity).</p> <p>Solution: Match compute to workload.</p> <p>Batch jobs: <pre><code># Start small, scale up if needed\nspark.conf.set(\"spark.executor.instances\", \"10\")\nspark.conf.set(\"spark.executor.cores\", \"4\")\n\n# Monitor and adjust\n# If CPU &lt; 50%: Reduce instances\n# If CPU &gt; 80%: Increase instances\n</code></pre></p> <p>Streaming: <pre><code># Use auto-scaling\nflink_config = {\n    \"parallelism\": \"auto\",\n    \"min_parallelism\": 2,\n    \"max_parallelism\": 20\n}\n</code></pre></p> <p>Expected savings: 20-30%.</p>"},{"location":"data-engineering/cost-efficiency/#2-query-optimization","title":"2. Query Optimization","text":"<p>Partition pruning: <pre><code>-- BAD: Scans all partitions\nSELECT * FROM events\nWHERE user_id = '123'\n\n-- GOOD: Only scans relevant partition\nSELECT * FROM events\nWHERE date = '2024-01-15' AND user_id = '123'\n</code></pre></p> <p>Column selection: <pre><code>-- BAD: Selects all columns\nSELECT * FROM users\n\n-- GOOD: Only needed columns\nSELECT user_id, name, email FROM users\n</code></pre></p> <p>Predicate pushdown: <pre><code>-- BAD: Filter after join\nSELECT * FROM orders o\nJOIN users u ON o.user_id = u.user_id\nWHERE o.date = '2024-01-15'\n\n-- GOOD: Filter before join\nSELECT * FROM (\n  SELECT * FROM orders WHERE date = '2024-01-15'\n) o\nJOIN users u ON o.user_id = u.user_id\n</code></pre></p> <p>Expected savings: 30-50% on query costs.</p>"},{"location":"data-engineering/cost-efficiency/#3-caching-and-materialization","title":"3. Caching and Materialization","text":"<p>Materialized views: <pre><code>-- Pre-compute common aggregations\nCREATE MATERIALIZED VIEW daily_user_stats AS\nSELECT\n  date,\n  user_id,\n  COUNT(*) as event_count,\n  SUM(amount) as total_amount\nFROM events\nGROUP BY date, user_id\n\n-- Refresh incrementally\nREFRESH MATERIALIZED VIEW daily_user_stats;\n</code></pre></p> <p>Application caching: <pre><code># Cache frequent queries\n@cache(ttl=3600)  # 1 hour\ndef get_user_stats(user_id):\n    return query(f\"SELECT * FROM user_stats WHERE user_id = {user_id}\")\n</code></pre></p> <p>Expected savings: 40-60% on repeated queries.</p>"},{"location":"data-engineering/cost-efficiency/#4-compression","title":"4. Compression","text":"<p>Storage compression: - Parquet with Snappy: 2-3x - Parquet with Zstd: 4-6x - Parquet with Brotli: 5-7x (slower)</p> <p>Recommendation: Use Zstd for best balance.</p> <pre><code>df.write.parquet(\n    path,\n    compression=\"zstd\"  # 4-6x compression\n)\n</code></pre> <p>Expected savings: 50-70% on storage.</p>"},{"location":"data-engineering/cost-efficiency/#5-incremental-processing","title":"5. Incremental Processing","text":"<p>Problem: Reprocessing all data every time.</p> <p>Solution: Only process new/changed data.</p> <pre><code># Full reprocess (expensive)\ndf = spark.read.table(\"raw.events\")\nprocessed = transform(df)\nprocessed.write.save(\"curated.events\")\n\n# Incremental (cheap)\nlast_processed = get_last_processed_timestamp()\ndf = spark.read.table(\"raw.events\") \\\n    .filter(f\"ingestion_timestamp &gt; '{last_processed}'\")\nprocessed = transform(df)\nprocessed.write.mode(\"append\").save(\"curated.events\")\nupdate_last_processed_timestamp()\n</code></pre> <p>Expected savings: 80-95% on transformation costs.</p>"},{"location":"data-engineering/cost-efficiency/#6-spot-instances-preemptible","title":"6. Spot Instances / Preemptible","text":"<p>Use for: - Batch jobs (can tolerate interruption) - Non-critical workloads - Cost-sensitive use cases</p> <p>Avoid for: - Streaming (needs continuous availability) - Critical pipelines - Low-latency requirements</p> <p>Expected savings: 60-80% on compute.</p>"},{"location":"data-engineering/cost-efficiency/#streaming-vs-micro-batch","title":"Streaming vs Micro-Batch","text":""},{"location":"data-engineering/cost-efficiency/#when-to-use-streaming","title":"When to Use Streaming","text":"<p>Use streaming when: - Real-time requirement (&lt; 1 minute) - Event-driven architecture - Low-latency use cases (fraud, recommendations)</p> <p>Cost: 3-5x batch</p>"},{"location":"data-engineering/cost-efficiency/#when-to-use-micro-batch","title":"When to Use Micro-Batch","text":"<p>Use micro-batch when: - Near real-time acceptable (1-15 minutes) - Cost-sensitive - Can tolerate small delays</p> <p>Implementation: <pre><code># Micro-batch: Process every 5 minutes\nschedule = \"*/5 * * * *\"  # Every 5 minutes\n\n# Instead of continuous streaming\n# Process accumulated events in batches\n</code></pre></p> <p>Cost: 1.5-2x batch</p> <p>Expected savings: 50-70% vs streaming.</p>"},{"location":"data-engineering/cost-efficiency/#zombie-pipeline-detection","title":"Zombie Pipeline Detection","text":""},{"location":"data-engineering/cost-efficiency/#the-problem","title":"The Problem","text":"<p>Zombie pipelines: Running but producing no value.</p> <p>Symptoms: - Zero downstream consumers - No queries in 90+ days - High cost, zero usage - Still running \"just in case\"</p>"},{"location":"data-engineering/cost-efficiency/#detection","title":"Detection","text":"<pre><code>-- Find zombie pipelines\nSELECT\n  pipeline_name,\n  daily_cost,\n  last_consumer_query,\n  days_since_last_use,\n  CASE\n    WHEN days_since_last_use &gt; 90 THEN 'ZOMBIE'\n    ELSE 'ACTIVE'\n  END as status\nFROM pipeline_usage_stats\nWHERE status = 'ZOMBIE'\nORDER BY daily_cost DESC\n</code></pre>"},{"location":"data-engineering/cost-efficiency/#action-plan","title":"Action Plan","text":"<ol> <li>Identify: Find zombies (query above)</li> <li>Verify: Confirm no usage (check consumers)</li> <li>Notify: Alert owners</li> <li>Archive: Move to cold storage</li> <li>Delete: Remove if truly unused</li> </ol> <p>Expected savings: 5-15% of total cost.</p>"},{"location":"data-engineering/cost-efficiency/#cost-monitoring","title":"Cost Monitoring","text":""},{"location":"data-engineering/cost-efficiency/#key-metrics","title":"Key Metrics","text":"<p>Cost per GB ingested: <pre><code>SELECT\n  source,\n  SUM(ingestion_cost) / SUM(volume_gb) as cost_per_gb\nFROM ingestion_costs\nGROUP BY source\nORDER BY cost_per_gb DESC\n</code></pre></p> <p>Cost per query: <pre><code>SELECT\n  query_type,\n  AVG(cost) as avg_cost,\n  PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY cost) as p95_cost\nFROM query_costs\nGROUP BY query_type\n</code></pre></p> <p>Cost trends: - Week-over-week growth - Month-over-month growth - Anomaly detection (spikes)</p>"},{"location":"data-engineering/cost-efficiency/#cost-attribution","title":"Cost Attribution","text":"<p>By team: <pre><code>SELECT\n  team,\n  SUM(cost) as total_cost,\n  SUM(cost) / SUM(SUM(cost)) OVER () as pct_of_total\nFROM cost_attribution\nGROUP BY team\nORDER BY total_cost DESC\n</code></pre></p> <p>By source: <pre><code>SELECT\n  source,\n  SUM(cost) as total_cost\nFROM cost_attribution\nGROUP BY source\nORDER BY total_cost DESC\n</code></pre></p> <p>By consumer: <pre><code>SELECT\n  consumer,\n  SUM(query_cost) as total_cost\nFROM query_costs\nGROUP BY consumer\nORDER BY total_cost DESC\n</code></pre></p>"},{"location":"data-engineering/cost-efficiency/#alerting","title":"Alerting","text":"<p>Cost alerts: - Daily cost &gt; threshold - Cost spike &gt; 20% (day-over-day) - Unusual usage pattern - Budget exceeded</p>"},{"location":"data-engineering/cost-efficiency/#practical-cost-reduction-plan","title":"Practical Cost Reduction Plan","text":""},{"location":"data-engineering/cost-efficiency/#phase-1-quick-wins-week-1-2","title":"Phase 1: Quick Wins (Week 1-2)","text":"<ol> <li>Identify unused tables \u2192 Archive (10-20% savings)</li> <li>Convert JSON to Parquet \u2192 Storage optimization (50-70% savings)</li> <li>Enable lifecycle policies \u2192 Tier old data (50-70% savings)</li> <li>Compact small files \u2192 Query optimization (20-30% savings)</li> </ol> <p>Expected total: 20-30% reduction</p>"},{"location":"data-engineering/cost-efficiency/#phase-2-optimization-week-3-4","title":"Phase 2: Optimization (Week 3-4)","text":"<ol> <li>Right-size compute \u2192 Match to workload (20-30% savings)</li> <li>Optimize queries \u2192 Partition pruning, column selection (30-50% savings)</li> <li>Incremental processing \u2192 Only process new data (80-95% savings)</li> <li>Materialize views \u2192 Pre-compute aggregations (40-60% savings)</li> </ol> <p>Expected total: Additional 15-25% reduction</p>"},{"location":"data-engineering/cost-efficiency/#phase-3-architecture-month-2","title":"Phase 3: Architecture (Month 2+)","text":"<ol> <li>Evaluate streaming vs batch \u2192 Use batch when possible (50-70% savings)</li> <li>Eliminate zombies \u2192 Remove unused pipelines (5-15% savings)</li> <li>Centralize processing \u2192 Eliminate redundancy (10-20% savings)</li> <li>Spot instances \u2192 For batch jobs (60-80% savings)</li> </ol> <p>Expected total: Additional 10-20% reduction</p>"},{"location":"data-engineering/cost-efficiency/#overall-target","title":"Overall Target","text":"<p>Total expected savings: 40-60% with all optimizations.</p>"},{"location":"data-engineering/cost-efficiency/#cost-benefit-analysis","title":"Cost-Benefit Analysis","text":""},{"location":"data-engineering/cost-efficiency/#when-to-optimize","title":"When to Optimize","text":"<p>Optimize when: - Cost &gt; $10K/month (worth engineering time) - Cost growing &gt; 20%/month (unsustainable) - Specific pain point (e.g., query slowness)</p> <p>Don't optimize when: - Cost &lt; $1K/month (engineering time &gt; savings) - One-time spike (investigate, but don't over-optimize) - Premature (optimize after you have data)</p>"},{"location":"data-engineering/cost-efficiency/#roi-calculation","title":"ROI Calculation","text":"<pre><code># Example: Query optimization project\nengineering_time = 40  # hours\nhourly_rate = 150  # $/hour\nengineering_cost = engineering_time * hourly_rate  # $6,000\n\nmonthly_savings = 5000  # $/month\nannual_savings = monthly_savings * 12  # $60,000\n\nroi = (annual_savings - engineering_cost) / engineering_cost  # 900%\npayback_period = engineering_cost / monthly_savings  # 1.2 months\n</code></pre> <p>Rule of thumb: If payback &lt; 3 months, do it.</p>"},{"location":"data-engineering/cost-efficiency/#next-steps","title":"Next Steps","text":"<ul> <li>Tooling Landscape - Tools for cost optimization</li> <li>Leadership View - Measuring and reporting costs</li> </ul>"},{"location":"data-engineering/foundations/","title":"Foundations","text":""},{"location":"data-engineering/foundations/#foundations","title":"Foundations","text":"<p>\"Data problems aren't boring. They're just badly explained.\"</p>"},{"location":"data-engineering/foundations/#what-is-data-engineering","title":"What is Data Engineering?","text":"<p>Data Engineering is the discipline of designing, building, and operating systems that transform raw data into reliable, accessible, and actionable information at scale. Unlike data science (which focuses on analysis and modeling) or software engineering (which focuses on application logic), data engineering sits at the intersection of infrastructure, reliability, and data product delivery.</p> <p>\"Data engineering isn't plumbing. It's product design with consequences.\"</p>"},{"location":"data-engineering/foundations/#modern-definition","title":"Modern Definition","text":"<p>At its core, data engineering is about:</p> <ol> <li>Reliability: Ensuring data arrives on time, in the right format, with the right quality</li> <li>Scale: Handling terabytes to petabytes of data across thousands of pipelines</li> <li>Velocity: Supporting both batch and real-time use cases</li> <li>Governance: Maintaining lineage, quality, and compliance</li> <li>Cost Efficiency: Delivering value without breaking the bank</li> </ol>"},{"location":"data-engineering/foundations/#the-shift-from-etl-to-platform","title":"The Shift: From ETL to Platform","text":"<p>Traditional data engineering focused on ETL pipelines\u2014point-to-point data movement with transformation logic embedded in the pipeline. Modern data engineering is about platforms\u2014infrastructure that enables teams to:</p> <p>Ingestion \u2014 Standardized patterns and contracts for pipeline creation with minimal friction</p> <p>Transformation \u2014 Managed compute environments supporting multiple tools and reusable frameworks</p> <p>Storage \u2014 Cost-appropriate tiering with automated lifecycle management and flexible formats</p> <p>Serving \u2014 Multiple access patterns (SQL, APIs, feature stores) optimized for analytics, ML, and operational use cases</p>"},{"location":"data-engineering/foundations/#core-principles","title":"Core Principles","text":""},{"location":"data-engineering/foundations/#1-data-as-a-product","title":"1. Data as a Product","text":"<p>Treat data assets as first-class products, not byproducts of applications.</p> <p>Implications: - Clear ownership and accountability - Defined SLAs (freshness, availability, quality) - Versioned schemas and contracts - Documentation and discoverability - Lifecycle management</p> <p>Anti-pattern: \"Just dump the data somewhere and we'll figure it out later.\"</p>"},{"location":"data-engineering/foundations/#2-separation-of-concerns","title":"2. Separation of Concerns","text":"<p>Establish clear boundaries between:</p> <ul> <li>Ingestion: Getting data into the platform</li> <li>Transformation: Shaping data for consumption</li> <li>Storage: Persisting data in appropriate formats/tiers</li> <li>Serving: Delivering data to consumers</li> </ul> <p>Why it matters: Each layer can evolve independently, scale independently, and fail independently.</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Ingestion  \u2502  \u2190 Push/pull, CDC, streaming\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Storage     \u2502  \u2190 Raw, curated, archive tiers\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502Transform    \u2502  \u2190 ELT, streaming transforms\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n       \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Serving    \u2502  \u2190 Analytics, ML, APIs\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"data-engineering/foundations/#3-platform-thinking","title":"3. Platform Thinking","text":"<p>Build self-serve capabilities that enable teams, not bottlenecks.</p> <p>Platform provides: - Standardized ingestion paths - Managed compute (Spark, Flink, etc.) - Storage abstractions (tables, partitions, lifecycle) - Metadata and discovery - Observability and alerting</p> <p>Teams provide: - Business logic - Transformation code - Quality checks - Documentation</p> <p>Anti-pattern: Central team manually creates every pipeline.</p>"},{"location":"data-engineering/foundations/#4-cost-awareness","title":"4. Cost Awareness","text":"<p>Every architectural decision has cost implications. Make them explicit.</p> <p>Key cost drivers: - Compute: Query execution, transformation jobs - Storage: Hot, warm, cold tiers - Network: Cross-region transfers, egress - Operations: Pipeline maintenance, incident response</p> <p>Principle: Start with the cheapest solution that meets requirements. Optimize when you have data.</p>"},{"location":"data-engineering/foundations/#5-contract-first-design","title":"5. Contract-First Design","text":"<p>Define data contracts before ingestion begins.</p> <p>Contract includes: - Schema (with evolution rules) - Freshness SLA - Quality expectations - Ownership and contact - Cost attribution</p> <p>Benefit: Prevents downstream breakage, enables automated validation.</p>"},{"location":"data-engineering/foundations/#platform-maturity-model","title":"Platform Maturity Model","text":""},{"location":"data-engineering/foundations/#level-1-ad-hoc","title":"Level 1: Ad-Hoc","text":"<ul> <li>Manual pipeline creation</li> <li>No standard patterns</li> <li>Limited observability</li> <li>High operational burden</li> </ul>"},{"location":"data-engineering/foundations/#level-2-standardized","title":"Level 2: Standardized","text":"<ul> <li>Common ingestion patterns</li> <li>Standardized storage formats</li> <li>Basic monitoring</li> <li>Some self-serve capabilities</li> </ul>"},{"location":"data-engineering/foundations/#level-3-platform","title":"Level 3: Platform","text":"<ul> <li>Self-serve ingestion</li> <li>Automated quality checks</li> <li>Rich metadata and discovery</li> <li>Cost attribution and optimization</li> </ul>"},{"location":"data-engineering/foundations/#level-4-product","title":"Level 4: Product","text":"<ul> <li>Data contracts enforced</li> <li>Predictive quality monitoring</li> <li>Automated optimization</li> <li>Multi-tenant isolation</li> </ul>"},{"location":"data-engineering/foundations/#key-concepts","title":"Key Concepts","text":""},{"location":"data-engineering/foundations/#data-freshness","title":"Data Freshness","text":"<p>Freshness = Time between when data is generated and when it's available for consumption.</p> <p>Categories: - Real-time: &lt; 1 minute (streaming) - Near real-time: 1-15 minutes (micro-batch) - Batch: 15 minutes - 24 hours - Historical: &gt; 24 hours (backfills, archives)</p> <p>Trade-off: Lower latency = higher cost.</p>"},{"location":"data-engineering/foundations/#data-quality-dimensions","title":"Data Quality Dimensions","text":"<ol> <li>Completeness: Are all expected records present?</li> <li>Accuracy: Does data reflect reality?</li> <li>Consistency: Is data consistent across sources?</li> <li>Timeliness: Is data fresh enough?</li> <li>Validity: Does data conform to schema?</li> <li>Uniqueness: Are there duplicates?</li> </ol>"},{"location":"data-engineering/foundations/#schema-evolution","title":"Schema Evolution","text":"<p>Schemas change. Design for it.</p> <p>Strategies: - Backward compatible: New fields optional, old fields never removed - Versioning: Explicit schema versions with migration paths - Schema registry: Centralized schema management (e.g., Confluent Schema Registry)</p> <p>Anti-pattern: Breaking changes without notice.</p>"},{"location":"data-engineering/foundations/#next-steps","title":"Next Steps","text":"<ul> <li>End-to-End Lifecycle - Understand the complete data journey</li> <li>Platform &amp; Operating Model - Design your platform architecture</li> </ul>"},{"location":"data-engineering/lifecycle/","title":"Lifecycle","text":""},{"location":"data-engineering/lifecycle/#end-to-end-lifecycle","title":"End-to-End Lifecycle","text":"<p>\"Data freshness is just trust, measured in minutes.\"</p> <p>The data lifecycle encompasses the complete journey from source systems to consumption. Understanding this flow is critical for designing scalable, maintainable data platforms.</p> <p>\"Every broken pipeline started as 'we'll clean it later.'\"</p>"},{"location":"data-engineering/lifecycle/#overview","title":"Overview","text":"<pre><code>Source Systems \u2192 Ingestion \u2192 Storage (Raw) \u2192 Transformation \u2192 Storage (Curated) \u2192 Serving\n</code></pre> <p>Each stage has distinct requirements, failure modes, and optimization opportunities.</p>"},{"location":"data-engineering/lifecycle/#stage-1-ingestion","title":"Stage 1: Ingestion","text":"<p>Goal: Get data from source systems into the platform reliably and efficiently.</p>"},{"location":"data-engineering/lifecycle/#ingestion-patterns","title":"Ingestion Patterns","text":""},{"location":"data-engineering/lifecycle/#batch-ingestion","title":"Batch Ingestion","text":"<ul> <li>When: Historical loads, daily/hourly snapshots, large volumes</li> <li>Tools: Airflow, Spark, Dataflow (batch mode)</li> <li>Characteristics: </li> <li>Scheduled execution</li> <li>Full or incremental loads</li> <li>Higher latency (minutes to hours)</li> <li>Lower cost per GB</li> </ul>"},{"location":"data-engineering/lifecycle/#streaming-ingestion","title":"Streaming Ingestion","text":"<ul> <li>When: Real-time analytics, event-driven systems, low-latency requirements</li> <li>Tools: Kafka, Pub/Sub, Kinesis, Flink, Dataflow (streaming)</li> <li>Characteristics:</li> <li>Continuous processing</li> <li>Low latency (seconds to minutes)</li> <li>Higher cost per GB</li> <li>More complex failure handling</li> </ul>"},{"location":"data-engineering/lifecycle/#change-data-capture-cdc","title":"Change Data Capture (CDC)","text":"<ul> <li>When: Database replication, maintaining current state, audit trails</li> <li>Tools: Debezium, Datastream, DMS, Fivetran</li> <li>Characteristics:</li> <li>Captures inserts, updates, deletes</li> <li>Maintains transaction consistency</li> <li>Lower overhead than full extracts</li> <li>Requires source database support</li> </ul>"},{"location":"data-engineering/lifecycle/#push-vs-pull","title":"Push vs Pull","text":"<p>Push (Source-initiated): - Source system sends data to platform - Pros: Real-time, source controls timing - Cons: Source must handle retries, platform must scale for bursts - Use when: Real-time requirements, source has reliable infrastructure</p> <p>Pull (Platform-initiated): - Platform queries source system - Pros: Platform controls rate, easier backpressure - Cons: Polling overhead, may miss real-time events - Use when: Batch processing, source can't push, rate limiting needed</p>"},{"location":"data-engineering/lifecycle/#ingestion-best-practices","title":"Ingestion Best Practices","text":"<ol> <li>Idempotency: Same data ingested multiple times = same result</li> <li>Checkpointing: Track progress to enable resume on failure</li> <li>Backpressure: Handle source unavailability gracefully</li> <li>Schema validation: Validate at ingestion boundary</li> <li>Metadata capture: Record source, timestamp, version</li> </ol>"},{"location":"data-engineering/lifecycle/#stage-2-storage-raw-layer","title":"Stage 2: Storage (Raw Layer)","text":"<p>Goal: Preserve source data exactly as received, with minimal transformation.</p>"},{"location":"data-engineering/lifecycle/#characteristics","title":"Characteristics","text":"<ul> <li>Immutable: Never modify raw data (append-only)</li> <li>Schema-on-read: Store in flexible formats (JSON, Avro, Parquet)</li> <li>Partitioned: By ingestion time, source, or business key</li> <li>Retention: Long-term retention for audit and reprocessing</li> </ul>"},{"location":"data-engineering/lifecycle/#storage-formats","title":"Storage Formats","text":"Format Use Case Pros Cons JSON Flexible schemas, nested data Human-readable, no schema needed Large size, slow queries Avro Schema evolution, streaming Compact, schema embedded Requires schema registry Parquet Analytics, columnar queries Highly compressed, fast scans Write overhead, less flexible CSV Simple tabular data Universal compatibility No schema, poor compression <p>Recommendation: Use Parquet for analytics workloads, Avro for streaming, JSON only when necessary.</p>"},{"location":"data-engineering/lifecycle/#partitioning-strategy","title":"Partitioning Strategy","text":"<p>Time-based partitioning (most common): <pre><code>raw/events/\n  year=2024/\n    month=01/\n      day=15/\n        hour=10/\n          data.parquet\n</code></pre></p> <p>Benefits: - Query pruning (only scan relevant partitions) - Lifecycle management (delete old partitions) - Parallel processing</p> <p>Key-based partitioning: - Use when queries filter on specific keys - Example: <code>user_id</code> hash partitioning for user-specific queries</p>"},{"location":"data-engineering/lifecycle/#stage-3-transformation","title":"Stage 3: Transformation","text":"<p>Goal: Transform raw data into curated, analysis-ready datasets.</p>"},{"location":"data-engineering/lifecycle/#elt-vs-etl","title":"ELT vs ETL","text":"<p>ETL (Extract-Transform-Load): - Transform before loading - Pros: Smaller storage footprint, pre-aggregated - Cons: Rigid, hard to reprocess, transformation logic in pipeline</p> <p>ELT (Extract-Load-Transform): - Load raw data first, transform in place - Pros: Flexible, easy reprocessing, separation of concerns - Cons: Larger storage, compute cost for transformations</p> <p>Modern approach: Prefer ELT. Storage is cheap, flexibility is valuable.</p>"},{"location":"data-engineering/lifecycle/#transformation-types","title":"Transformation Types","text":""},{"location":"data-engineering/lifecycle/#1-cleansing","title":"1. Cleansing","text":"<ul> <li>Remove duplicates</li> <li>Handle nulls</li> <li>Standardize formats</li> <li>Validate ranges</li> </ul>"},{"location":"data-engineering/lifecycle/#2-enrichment","title":"2. Enrichment","text":"<ul> <li>Join with reference data</li> <li>Add computed fields</li> <li>Geocoding, lookups</li> </ul>"},{"location":"data-engineering/lifecycle/#3-aggregation","title":"3. Aggregation","text":"<ul> <li>Rollups (hourly \u2192 daily)</li> <li>Summaries (counts, sums, averages)</li> <li>Window functions</li> </ul>"},{"location":"data-engineering/lifecycle/#4-normalization","title":"4. Normalization","text":"<ul> <li>Flatten nested structures</li> <li>Resolve entity relationships</li> <li>Create dimensional models</li> </ul>"},{"location":"data-engineering/lifecycle/#transformation-patterns","title":"Transformation Patterns","text":"<p>Incremental Processing: <pre><code>-- Only process new/updated records\nSELECT * FROM raw.events\nWHERE ingestion_timestamp &gt; (\n  SELECT MAX(processed_timestamp) FROM curated.events\n)\n</code></pre></p> <p>Upsert Pattern: <pre><code>-- Merge new data with existing\nMERGE INTO curated.users AS target\nUSING transformed.users AS source\nON target.user_id = source.user_id\nWHEN MATCHED THEN UPDATE ...\nWHEN NOT MATCHED THEN INSERT ...\n</code></pre></p> <p>Slowly Changing Dimensions (SCD): - Type 1: Overwrite (no history) - Type 2: Add new row with validity period (full history) - Type 3: Add new column (limited history)</p>"},{"location":"data-engineering/lifecycle/#streaming-transformations","title":"Streaming Transformations","text":"<p>For real-time pipelines:</p> <ul> <li>Windowing: Group events by time windows</li> <li>Stateful processing: Maintain aggregations across events</li> <li>Joins: Stream-to-stream or stream-to-table joins</li> <li>Complex event processing: Pattern matching, event sequences</li> </ul> <p>Tools: Flink, Kafka Streams, Dataflow (streaming)</p>"},{"location":"data-engineering/lifecycle/#stage-4-storage-curated-layer","title":"Stage 4: Storage (Curated Layer)","text":"<p>Goal: Store transformed data optimized for consumption.</p>"},{"location":"data-engineering/lifecycle/#characteristics_1","title":"Characteristics","text":"<ul> <li>Schema-on-write: Enforced schemas (BigQuery, Snowflake, Delta Lake)</li> <li>Partitioned and indexed: Optimized for query patterns</li> <li>Versioned: Track changes over time</li> <li>Documented: Clear ownership, purpose, usage</li> </ul>"},{"location":"data-engineering/lifecycle/#storage-tiers","title":"Storage Tiers","text":"Tier Use Case Access Pattern Cost Hot Active queries, dashboards Frequent, low latency High Warm Ad-hoc analysis, reporting Occasional, moderate latency Medium Cold Compliance, historical Rare, high latency acceptable Low <p>Lifecycle policies: Automatically move data between tiers based on age/access patterns.</p>"},{"location":"data-engineering/lifecycle/#data-models","title":"Data Models","text":"<p>Star Schema (Kimball): - Fact tables (transactions, events) - Dimension tables (users, products, time) - Optimized for analytics queries</p> <p>Data Vault: - Hubs (business keys) - Links (relationships) - Satellites (attributes) - Optimized for auditability and change tracking</p> <p>One Big Table (OBT): - Denormalized, wide tables - Simple queries, no joins - Higher storage cost, simpler queries</p> <p>Choose based on: Query patterns, update frequency, storage budget.</p>"},{"location":"data-engineering/lifecycle/#stage-5-serving","title":"Stage 5: Serving","text":"<p>Goal: Deliver data to consumers in the right format, at the right time.</p>"},{"location":"data-engineering/lifecycle/#serving-patterns","title":"Serving Patterns","text":""},{"location":"data-engineering/lifecycle/#analytics-serving","title":"Analytics Serving","text":"<ul> <li>Tools: BigQuery, Snowflake, Redshift, Databricks</li> <li>Format: SQL queries, dashboards</li> <li>Characteristics: Ad-hoc queries, aggregations, joins</li> </ul>"},{"location":"data-engineering/lifecycle/#ml-serving","title":"ML Serving","text":"<ul> <li>Tools: Feature stores (Feast, Tecton), model serving (Seldon, BentoML)</li> <li>Format: Feature vectors, embeddings</li> <li>Characteristics: Low-latency lookups, point-in-time correctness</li> </ul>"},{"location":"data-engineering/lifecycle/#operational-serving","title":"Operational Serving","text":"<ul> <li>Tools: APIs, key-value stores (Redis, DynamoDB)</li> <li>Format: REST/GraphQL APIs, lookups</li> <li>Characteristics: Sub-second latency, high availability</li> </ul>"},{"location":"data-engineering/lifecycle/#data-sharing","title":"Data Sharing","text":"<ul> <li>Tools: Delta Sharing, BigQuery authorized views, S3 access</li> <li>Format: Direct table access, exports</li> <li>Characteristics: Cross-organization, governed access</li> </ul>"},{"location":"data-engineering/lifecycle/#serving-best-practices","title":"Serving Best Practices","text":"<ol> <li>Materialized views: Pre-compute common aggregations</li> <li>Caching: Cache frequent queries (Redis, application cache)</li> <li>Query optimization: Partition pruning, column selection, predicate pushdown</li> <li>Access control: Row-level security, column masking</li> <li>Rate limiting: Prevent resource exhaustion</li> </ol>"},{"location":"data-engineering/lifecycle/#lifecycle-management","title":"Lifecycle Management","text":""},{"location":"data-engineering/lifecycle/#data-retention","title":"Data Retention","text":"<p>Define retention policies for each layer:</p> <ul> <li>Raw: Long retention (1-7 years) for audit and reprocessing</li> <li>Curated: Medium retention (90 days - 2 years) based on business needs</li> <li>Archive: Compressed, cold storage for compliance</li> </ul>"},{"location":"data-engineering/lifecycle/#data-deletion","title":"Data Deletion","text":"<ul> <li>Soft delete: Mark as deleted, retain for recovery</li> <li>Hard delete: Permanently remove (GDPR, retention expiry)</li> <li>Cascade: Delete dependent datasets when source is deleted</li> </ul>"},{"location":"data-engineering/lifecycle/#versioning","title":"Versioning","text":"<p>Track changes to: - Schemas (evolution history) - Transformations (code versions) - Data (snapshots, change logs)</p>"},{"location":"data-engineering/lifecycle/#monitoring-the-lifecycle","title":"Monitoring the Lifecycle","text":"<p>Key metrics per stage:</p> <ul> <li>Ingestion: Volume, latency, error rate, schema drift</li> <li>Transformation: Processing time, success rate, data quality</li> <li>Storage: Size, growth rate, partition count, tier distribution</li> <li>Serving: Query latency, error rate, cache hit rate, cost per query</li> </ul> <p>Alerting: Set thresholds for SLAs, error rates, cost anomalies.</p>"},{"location":"data-engineering/lifecycle/#next-steps","title":"Next Steps","text":"<ul> <li>Ingestion Architecture - Deep dive into ingestion patterns</li> <li>Storage &amp; Data Architecture - Storage design patterns</li> </ul>"},{"location":"data-engineering/platform-operating-model/","title":"Platform & Operating Model","text":""},{"location":"data-engineering/platform-operating-model/#platform-operating-model","title":"Platform &amp; Operating Model","text":"<p>\"The biggest opportunity for managers isn't better data \u2014 it's making data problems understandable.\"</p> <p>Building a data platform isn't just about technology\u2014it's about creating an operating model that enables teams to move fast while maintaining quality, cost control, and reliability. This chapter covers how to structure your platform organization and processes.</p> <p>\"If Gen-Z doesn't care about your data problem, you've explained the wrong problem.\"</p>"},{"location":"data-engineering/platform-operating-model/#central-platform-vs-domain-ownership","title":"Central Platform vs Domain Ownership","text":""},{"location":"data-engineering/platform-operating-model/#the-spectrum","title":"The Spectrum","text":"<pre><code>Fully Centralized \u2190\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2192 Fully Decentralized\n(Platform Team)                                    (Domain Teams)\n</code></pre>"},{"location":"data-engineering/platform-operating-model/#central-platform-model","title":"Central Platform Model","text":"<p>Structure: - Central platform team owns infrastructure - Domain teams consume platform services - Platform team builds self-serve capabilities</p> <p>Pros: - Consistency across organization - Economies of scale - Centralized expertise - Easier governance</p> <p>Cons: - Can become bottleneck - May not understand domain needs - Slower to adapt</p> <p>Best for: - Large organizations (1000+ engineers) - Need for strong governance - Limited data engineering expertise in domains</p>"},{"location":"data-engineering/platform-operating-model/#domain-ownership-model","title":"Domain Ownership Model","text":"<p>Structure: - Domain teams own their data end-to-end - Platform provides base infrastructure only - Teams responsible for quality, cost, SLAs</p> <p>Pros: - Faster iteration - Domain expertise - Ownership and accountability</p> <p>Cons: - Inconsistency - Duplication - Harder governance</p> <p>Best for: - Smaller organizations - High domain expertise - Need for speed over consistency</p>"},{"location":"data-engineering/platform-operating-model/#hybrid-model-recommended","title":"Hybrid Model (Recommended)","text":"<p>Structure: - Platform team owns: Infrastructure, standards, tooling - Domain teams own: Business logic, transformations, quality - Shared ownership: Governance, cost optimization</p> <p>Responsibilities Matrix:</p> Area Platform Team Domain Teams Shared Infrastructure \u2705 Ingestion pipelines \u2705 (self-serve) \u2705 (business logic) Transformations \u2705 Data quality \u2705 \u2705 (standards) Cost optimization \u2705 (tools) \u2705 (usage) \u2705 Governance \u2705 (framework) \u2705 (compliance) \u2705 <p>Key principle: Platform enables, domains execute.</p>"},{"location":"data-engineering/platform-operating-model/#paved-paths-and-escape-hatches","title":"Paved Paths and Escape Hatches","text":""},{"location":"data-engineering/platform-operating-model/#paved-paths","title":"Paved Paths","text":"<p>Definition: Standardized, supported, well-documented ways to accomplish common tasks.</p> <p>Examples: - Standard ingestion patterns (CDC, batch, streaming) - Pre-configured compute environments (Spark, Flink) - Standard storage formats (Parquet, Delta) - Approved tooling (dbt, Airflow)</p> <p>Benefits: - Faster onboarding - Consistency - Easier maintenance - Better observability</p> <p>Implementation: <pre><code># Example: Standard ingestion template\ningestion_template:\n  type: cdc\n  source: postgres\n  destination: gcs://raw/{source_name}\n  format: parquet\n  partition_by: [date]\n  schema_registry: enabled\n  monitoring: enabled\n</code></pre></p>"},{"location":"data-engineering/platform-operating-model/#escape-hatches","title":"Escape Hatches","text":"<p>Definition: Approved ways to deviate from paved paths when needed.</p> <p>When to use: - Unique requirements not met by standard paths - Performance optimization - Experimental patterns</p> <p>Process: 1. Document why standard path doesn't work 2. Get approval (platform team review) 3. Implement with monitoring 4. Evaluate for promotion to paved path</p> <p>Example: <pre><code>Standard: Use Dataflow for streaming\nEscape hatch: Use Flink for stateful processing (approved use case)\n</code></pre></p> <p>Principle: Make it easy to use paved paths, possible but reviewed to use escape hatches.</p>"},{"location":"data-engineering/platform-operating-model/#contract-first-ingestion","title":"Contract-First Ingestion","text":""},{"location":"data-engineering/platform-operating-model/#the-problem","title":"The Problem","text":"<p>Without contracts, you get: - Schema drift breaking downstream - Unclear SLAs - Ownership confusion - Cost attribution issues</p>"},{"location":"data-engineering/platform-operating-model/#the-solution-data-contracts","title":"The Solution: Data Contracts","text":"<p>Contract definition: <pre><code>source: user_events\nowner: analytics-team@company.com\nsla:\n  freshness: 15 minutes\n  availability: 99.9%\nschema:\n  version: 1.0\n  fields:\n    - name: user_id\n      type: string\n      required: true\n    - name: event_type\n      type: string\n      enum: [click, view, purchase]\n  evolution: backward_compatible\nquality:\n  completeness: &gt; 99%\n  uniqueness: &gt; 99.9%\ncost_attribution: analytics-team\n</code></pre></p>"},{"location":"data-engineering/platform-operating-model/#contract-enforcement","title":"Contract Enforcement","text":"<p>At ingestion: 1. Validate schema matches contract 2. Check quality metrics 3. Reject if contract violated</p> <p>In platform: 1. Store contracts in registry 2. Version contracts 3. Notify on violations 4. Track compliance</p> <p>Tools: DataHub, Great Expectations, custom validators</p>"},{"location":"data-engineering/platform-operating-model/#benefits","title":"Benefits","text":"<ul> <li>Predictability: Downstream knows what to expect</li> <li>Quality: Issues caught early</li> <li>Ownership: Clear accountability</li> <li>Evolution: Controlled schema changes</li> </ul>"},{"location":"data-engineering/platform-operating-model/#cost-attribution-and-accountability","title":"Cost Attribution and Accountability","text":""},{"location":"data-engineering/platform-operating-model/#the-problem_1","title":"The Problem","text":"<p>Without attribution: - \"The platform is expensive\" (but who's using it?) - No incentive to optimize - Hard to justify investments</p>"},{"location":"data-engineering/platform-operating-model/#solution-cost-attribution","title":"Solution: Cost Attribution","text":"<p>Attribution dimensions: - Team: Which team owns the data/pipeline - Project: Which project/business unit - Source: Which source system - Consumer: Which downstream consumers</p> <p>Implementation:</p> <pre><code>-- Example: Cost attribution query\nSELECT\n  team,\n  source,\n  SUM(storage_cost) as storage_cost,\n  SUM(compute_cost) as compute_cost,\n  SUM(total_cost) as total_cost\nFROM cost_attribution\nWHERE date &gt;= CURRENT_DATE - 30\nGROUP BY team, source\nORDER BY total_cost DESC\n</code></pre> <p>Tools:  - Cloud cost management (AWS Cost Explorer, GCP Billing) - Custom attribution tags - DataHub cost tracking</p>"},{"location":"data-engineering/platform-operating-model/#showback-vs-chargeback","title":"Showback vs Chargeback","text":"<p>Showback (recommended): - Show costs to teams - Create awareness - Encourage optimization - No actual billing</p> <p>Chargeback: - Actually bill teams - Stronger incentive - More complex (billing systems) - Can create friction</p> <p>Recommendation: Start with showback. Move to chargeback only if needed.</p>"},{"location":"data-engineering/platform-operating-model/#cost-accountability","title":"Cost Accountability","text":"<p>Monthly reviews: 1. Top spenders by team 2. Cost trends (growth, anomalies) 3. Optimization opportunities 4. ROI of investments</p> <p>Goals: - Teams see their costs - Teams understand cost drivers - Teams optimize proactively</p>"},{"location":"data-engineering/platform-operating-model/#self-serve-capabilities","title":"Self-Serve Capabilities","text":""},{"location":"data-engineering/platform-operating-model/#ingestion-self-serve","title":"Ingestion Self-Serve","text":"<p>Capabilities: - Web UI or CLI to register new sources - Automatic pipeline generation - Schema discovery and validation - Monitoring setup</p> <p>Example flow: <pre><code># Developer registers new source\nplatform ingest register \\\n  --source postgres://db.example.com/users \\\n  --destination gcs://raw/users \\\n  --sla 15min \\\n  --owner analytics-team\n\n# Platform automatically:\n# - Creates CDC pipeline\n# - Sets up monitoring\n# - Creates contract\n# - Provisions resources\n</code></pre></p> <p>Benefits: - Faster time to value (hours vs weeks) - Reduced platform team load - Consistency (standard patterns)</p>"},{"location":"data-engineering/platform-operating-model/#transformation-self-serve","title":"Transformation Self-Serve","text":"<p>Capabilities: - Managed compute (Spark, Flink clusters) - Standard libraries and frameworks - CI/CD integration - Testing frameworks</p> <p>Example: <pre><code># Developer writes transformation\n@platform.transform(\n    input=\"raw.events\",\n    output=\"curated.user_events\",\n    schedule=\"hourly\"\n)\ndef transform_events(df):\n    return df.filter(df.event_type == \"purchase\")\n</code></pre></p> <p>Platform handles: - Resource provisioning - Scheduling - Monitoring - Error handling</p>"},{"location":"data-engineering/platform-operating-model/#discovery-self-serve","title":"Discovery Self-Serve","text":"<p>Capabilities: - Data catalog (search, browse) - Schema documentation - Lineage visualization - Usage statistics</p> <p>Tools: DataHub, Collibra, custom catalogs</p>"},{"location":"data-engineering/platform-operating-model/#platform-team-structure","title":"Platform Team Structure","text":""},{"location":"data-engineering/platform-operating-model/#core-team-roles","title":"Core Team Roles","text":"<p>Platform Engineers: - Build and maintain infrastructure - Develop self-serve capabilities - Optimize platform performance</p> <p>Data Engineers (Platform): - Design ingestion patterns - Build transformation frameworks - Create best practices</p> <p>SRE / DevOps: - Reliability and observability - Incident response - Capacity planning</p> <p>Product Managers: - Platform roadmap - User needs (domain teams) - Success metrics</p>"},{"location":"data-engineering/platform-operating-model/#team-size-guidelines","title":"Team Size Guidelines","text":"<p>Small organization (&lt; 100 engineers): - 2-3 platform engineers - Part-time SRE - No dedicated PM</p> <p>Medium organization (100-500 engineers): - 5-10 platform engineers - 1-2 SRE - 1 PM</p> <p>Large organization (500+ engineers): - 15-30 platform engineers - 3-5 SRE - 2-3 PM - Dedicated cost optimization team</p>"},{"location":"data-engineering/platform-operating-model/#success-metrics","title":"Success Metrics","text":""},{"location":"data-engineering/platform-operating-model/#platform-health","title":"Platform Health","text":"<p>Adoption: - % of data sources using platform - % of transformations on platform - Active users per month</p> <p>Reliability: - Platform uptime (target: 99.9%) - Pipeline success rate (target: &gt; 99%) - Mean time to recovery (MTTR)</p> <p>Performance: - Ingestion latency (p50, p95, p99) - Query performance (p50, p95, p99) - Resource utilization</p>"},{"location":"data-engineering/platform-operating-model/#developer-experience","title":"Developer Experience","text":"<p>Time to value: - Time to first ingestion (target: &lt; 1 day) - Time to first transformation (target: &lt; 2 days)</p> <p>Developer satisfaction: - NPS or survey scores - Support ticket volume - Documentation usage</p> <p>Self-serve adoption: - % of pipelines created via self-serve - % of transformations using standard frameworks</p>"},{"location":"data-engineering/platform-operating-model/#cost-efficiency","title":"Cost Efficiency","text":"<p>Cost per GB ingested: - Track over time - Compare to industry benchmarks - Optimize continuously</p> <p>Cost per query: - Average cost - Cost by query type - Optimization opportunities</p> <p>Total cost of ownership: - Platform infrastructure cost - Operational overhead - Developer time saved</p>"},{"location":"data-engineering/platform-operating-model/#operating-model-maturity","title":"Operating Model Maturity","text":""},{"location":"data-engineering/platform-operating-model/#level-1-ad-hoc","title":"Level 1: Ad-Hoc","text":"<ul> <li>Manual pipeline creation</li> <li>No standards</li> <li>Limited self-serve</li> <li>High operational burden</li> </ul>"},{"location":"data-engineering/platform-operating-model/#level-2-standardized","title":"Level 2: Standardized","text":"<ul> <li>Common patterns documented</li> <li>Some self-serve capabilities</li> <li>Basic governance</li> <li>Platform team bottleneck</li> </ul>"},{"location":"data-engineering/platform-operating-model/#level-3-self-serve-platform","title":"Level 3: Self-Serve Platform","text":"<ul> <li>Most tasks self-serve</li> <li>Clear contracts and SLAs</li> <li>Cost attribution</li> <li>Platform enables, doesn't block</li> </ul>"},{"location":"data-engineering/platform-operating-model/#level-4-product-platform","title":"Level 4: Product Platform","text":"<ul> <li>Full self-serve</li> <li>Predictive quality</li> <li>Automated optimization</li> <li>Platform as competitive advantage</li> </ul>"},{"location":"data-engineering/platform-operating-model/#next-steps","title":"Next Steps","text":"<ul> <li>Quality, Governance &amp; Observability - How to ensure quality and govern data</li> <li>Cost Efficiency &amp; Scale - Advanced cost optimization</li> </ul>"},{"location":"data-ingestion/","title":"Data Ingestion","text":""},{"location":"data-ingestion/#data-ingestion","title":"Data Ingestion","text":"<p>\"Data freshness is just trust, measured in minutes.\"</p> <p>Getting data from source systems into your platform reliably and efficiently.</p>"},{"location":"data-ingestion/#overview","title":"Overview","text":"<p>Ingestion is the foundation of your data platform. Get it wrong, and everything downstream suffers. This section provides deep, opinionated guidance on building reliable, cost-effective ingestion systems.</p>"},{"location":"data-ingestion/#decision-framework","title":"Decision Framework","text":"<p>Before choosing an ingestion pattern, answer these questions:</p> <ol> <li>Freshness requirement: Real-time (&lt; 1 min), near real-time (1-15 min), or batch (15+ min)?</li> <li>Volume: How many records/second? How many GB/day?</li> <li>Source type: Database, API, files, event stream?</li> <li>Change detection: Do you need to capture updates/deletes, or just new records?</li> <li>Cost sensitivity: What's your budget per GB ingested?</li> </ol>"},{"location":"data-ingestion/#key-topics","title":"Key Topics","text":""},{"location":"data-ingestion/#batch-vs-streaming","title":"Batch vs Streaming","text":"<p>When to use batch, streaming, or CDC patterns.</p> <p>Learn about: - Batch ingestion patterns - Streaming ingestion architecture - Change Data Capture (CDC) - Cost vs freshness trade-offs - Tool selection guide</p>"},{"location":"data-ingestion/#change-data-capture-cdc","title":"Change Data Capture (CDC)","text":"<p>Capturing database changes in real-time.</p> <p>Learn about: - Log-based CDC - Trigger-based CDC - Query-based CDC - CDC tools (Debezium, Datastream) - Current state patterns</p>"},{"location":"data-ingestion/#push-vs-pull","title":"Push vs Pull","text":"<p>Source-initiated vs platform-initiated ingestion.</p> <p>Learn about: - Push architecture (webhooks, APIs) - Pull architecture (scheduled queries) - When to use each - Implementation patterns - Error handling</p>"},{"location":"data-ingestion/#ingestion-patterns","title":"Ingestion Patterns","text":""},{"location":"data-ingestion/#batch-ingestion","title":"Batch Ingestion","text":"<p>When to use: - Historical loads, backfills - Large volumes (&gt; 100 GB per run) - No real-time requirement - Source systems that don't support streaming</p> <p>Characteristics: - Scheduled execution (hourly, daily) - Full or incremental extracts - Higher latency (minutes to hours) - Lower cost per GB - Easier to debug and reprocess</p>"},{"location":"data-ingestion/#streaming-ingestion","title":"Streaming Ingestion","text":"<p>When to use: - Real-time analytics requirements - Event-driven architectures - Low-latency use cases (fraud detection, recommendations) - High-volume, continuous data</p> <p>Characteristics: - Continuous processing - Low latency (seconds to minutes) - Higher cost per GB (3-5x batch) - More complex failure handling - Requires message queue/bus</p>"},{"location":"data-ingestion/#change-data-capture-cdc_1","title":"Change Data Capture (CDC)","text":"<p>When to use: - Database replication - Maintaining current state tables - Audit trails - Real-time synchronization</p> <p>Characteristics: - Captures inserts, updates, deletes - Maintains transaction consistency - Lower overhead than full extracts - Requires source database support (WAL, binlog)</p>"},{"location":"data-ingestion/#cost-vs-freshness-trade-offs","title":"Cost vs Freshness Trade-offs","text":"<p>Cost Consideration</p> <p>Every 10x reduction in latency costs 3-5x more.</p> Latency Pattern Cost per GB Use Case &lt; 1 min Streaming $0.10-0.50 Real-time dashboards, fraud 1-15 min Micro-batch $0.05-0.15 Near real-time analytics 15 min - 1 hr Batch (frequent) $0.02-0.05 Hourly reports 1-24 hrs Batch (daily) $0.01-0.02 Daily ETL, data warehouse &gt; 24 hrs Batch (weekly) $0.005-0.01 Historical analysis <p>Optimization strategy: 1. Start with the slowest acceptable latency 2. Measure actual requirements (not perceived) 3. Optimize only when latency becomes a bottleneck 4. Use tiered approach: streaming for critical, batch for rest</p>"},{"location":"data-ingestion/#best-practices","title":"Best Practices","text":""},{"location":"data-ingestion/#idempotency","title":"Idempotency","text":"<p>Same data ingested multiple times = same result.</p>"},{"location":"data-ingestion/#checkpointing","title":"Checkpointing","text":"<p>Track progress to enable resume on failure.</p>"},{"location":"data-ingestion/#backpressure","title":"Backpressure","text":"<p>Handle source unavailability gracefully.</p>"},{"location":"data-ingestion/#schema-validation","title":"Schema Validation","text":"<p>Validate at ingestion boundary.</p>"},{"location":"data-ingestion/#metadata-capture","title":"Metadata Capture","text":"<p>Record source, timestamp, version.</p>"},{"location":"data-ingestion/#related-topics","title":"Related Topics","text":"<ul> <li>Data Architecture - How to store ingested data</li> <li>Data Quality - Ensuring data reliability</li> <li>Data Engineering - Platform fundamentals</li> </ul> <p>Next: Batch vs Streaming \u2192</p>"},{"location":"data-ingestion/batch-vs-streaming/","title":"Batch vs Streaming","text":""},{"location":"data-ingestion/batch-vs-streaming/#ingestion-architecture","title":"Ingestion Architecture","text":"<p>\"If a data problem can't be explained in one screen, the system is already broken.\"</p> <p>Ingestion is the foundation of your data platform. Get it wrong, and everything downstream suffers. This chapter provides deep, opinionated guidance on building reliable, cost-effective ingestion systems.</p> <p>\"Data freshness is just trust, measured in minutes.\"</p>"},{"location":"data-ingestion/batch-vs-streaming/#decision-framework","title":"Decision Framework","text":"<p>Before choosing an ingestion pattern, answer these questions:</p> <ol> <li>Freshness requirement: Real-time (&lt; 1 min), near real-time (1-15 min), or batch (15+ min)?</li> <li>Volume: How many records/second? How many GB/day?</li> <li>Source type: Database, API, files, event stream?</li> <li>Change detection: Do you need to capture updates/deletes, or just new records?</li> <li>Cost sensitivity: What's your budget per GB ingested?</li> </ol>"},{"location":"data-ingestion/batch-vs-streaming/#batch-vs-streaming-vs-cdc","title":"Batch vs Streaming vs CDC","text":""},{"location":"data-ingestion/batch-vs-streaming/#batch-ingestion","title":"Batch Ingestion","text":"<p>When to use: - Historical loads, backfills - Large volumes (&gt; 100 GB per run) - No real-time requirement - Source systems that don't support streaming</p> <p>Characteristics: - Scheduled execution (hourly, daily) - Full or incremental extracts - Higher latency (minutes to hours) - Lower cost per GB - Easier to debug and reprocess</p> <p>Common patterns:</p> <pre><code># Full extract\nSELECT * FROM source_table\nWHERE ingestion_date = CURRENT_DATE\n\n# Incremental extract (timestamp-based)\nSELECT * FROM source_table\nWHERE updated_at &gt; :last_ingestion_time\n\n# Incremental extract (change log)\nSELECT * FROM source_table\nWHERE id IN (\n  SELECT id FROM change_log\n  WHERE processed = FALSE\n)\n</code></pre> <p>Tools: Airflow + Spark, Dataflow (batch), dbt, Fivetran</p>"},{"location":"data-ingestion/batch-vs-streaming/#streaming-ingestion","title":"Streaming Ingestion","text":"<p>When to use: - Real-time analytics requirements - Event-driven architectures - Low-latency use cases (fraud detection, recommendations) - High-volume, continuous data</p> <p>Characteristics: - Continuous processing - Low latency (seconds to minutes) - Higher cost per GB - More complex failure handling - Requires message queue/bus</p> <p>Architecture:</p> <pre><code>Source \u2192 Message Queue (Kafka/Pub/Sub) \u2192 Stream Processor \u2192 Storage\n</code></pre> <p>Tools: Kafka, Pub/Sub, Kinesis, Flink, Dataflow (streaming), Kafka Connect</p> <p>Cost consideration: Streaming is 3-5x more expensive than batch for the same volume. Only use when latency justifies cost.</p>"},{"location":"data-ingestion/batch-vs-streaming/#change-data-capture-cdc","title":"Change Data Capture (CDC)","text":"<p>When to use: - Database replication - Maintaining current state tables - Audit trails - Real-time synchronization</p> <p>Characteristics: - Captures inserts, updates, deletes - Maintains transaction consistency - Lower overhead than full extracts - Requires source database support (WAL, binlog)</p> <p>CDC Patterns:</p> <ol> <li>Log-based CDC: Read database transaction logs</li> <li>Tools: Debezium, Datastream, DMS</li> <li>Pros: Low overhead, captures all changes</li> <li> <p>Cons: Requires database configuration</p> </li> <li> <p>Trigger-based CDC: Database triggers write to change table</p> </li> <li>Pros: Works with any database</li> <li> <p>Cons: Higher overhead, may miss some changes</p> </li> <li> <p>Query-based CDC: Poll for changes using timestamps/version columns</p> </li> <li>Pros: Simple, no database changes</li> <li>Cons: May miss deletes, higher overhead</li> </ol> <p>Recommendation: Use log-based CDC when available. It's the most reliable and efficient.</p>"},{"location":"data-ingestion/batch-vs-streaming/#push-vs-pull","title":"Push vs Pull","text":""},{"location":"data-ingestion/batch-vs-streaming/#push-source-initiated","title":"Push (Source-Initiated)","text":"<p>Architecture: <pre><code>Source System \u2192 Webhook/API \u2192 Platform Ingestion Endpoint\n</code></pre></p> <p>Pros: - Real-time delivery - Source controls timing - No polling overhead</p> <p>Cons: - Source must handle retries - Platform must scale for bursts - Requires source system changes</p> <p>When to use: - Real-time requirements - Source has reliable infrastructure - You control the source system</p> <p>Implementation considerations: - Idempotency keys (deduplicate retries) - Rate limiting (prevent abuse) - Authentication (secure endpoints) - Backpressure (reject when overloaded)</p>"},{"location":"data-ingestion/batch-vs-streaming/#pull-platform-initiated","title":"Pull (Platform-Initiated)","text":"<p>Architecture: <pre><code>Platform Scheduler \u2192 Query Source \u2192 Process Results\n</code></pre></p> <p>Pros: - Platform controls rate - Easier backpressure - No source system changes</p> <p>Cons: - Polling overhead - May miss real-time events - Higher latency</p> <p>When to use: - Batch processing - Source can't push - Rate limiting needed - Legacy systems</p> <p>Optimization: - Incremental queries (only fetch new data) - Parallel pulls (multiple workers) - Adaptive polling (increase frequency when data available)</p>"},{"location":"data-ingestion/batch-vs-streaming/#tool-selection-guide","title":"Tool Selection Guide","text":""},{"location":"data-ingestion/batch-vs-streaming/#ingestion-engines","title":"Ingestion Engines","text":"Tool Type Best For Cost Model Airflow + Spark Batch Large volumes, complex transforms Compute + storage Dataflow Batch/Streaming GCP-native, auto-scaling Per vCPU-hour Fivetran SaaS Database replication, zero maintenance Per connector, per row Stitch SaaS Simple extracts, cost-effective Per row Debezium CDC Kafka-based CDC, open source Infrastructure only Datastream CDC GCP-native CDC, managed Per GB processed Kafka Connect Streaming Kafka ecosystem, extensible Infrastructure only"},{"location":"data-ingestion/batch-vs-streaming/#decision-matrix","title":"Decision Matrix","text":"<p>High volume (&gt; 1 TB/day), batch: \u2192 Airflow + Spark or Dataflow (batch)</p> <p>Real-time, event streams: \u2192 Kafka + Flink or Dataflow (streaming)</p> <p>Database replication, CDC: \u2192 Debezium, Datastream, or Fivetran</p> <p>Multiple sources, zero maintenance: \u2192 Fivetran or Stitch (SaaS)</p> <p>Cost-sensitive, simple extracts: \u2192 Airflow + custom scripts</p>"},{"location":"data-ingestion/batch-vs-streaming/#cost-vs-freshness-trade-offs","title":"Cost vs Freshness Trade-offs","text":"<p>Rule of thumb: Every 10x reduction in latency costs 3-5x more.</p> Latency Pattern Cost per GB Use Case &lt; 1 min Streaming $0.10-0.50 Real-time dashboards, fraud 1-15 min Micro-batch $0.05-0.15 Near real-time analytics 15 min - 1 hr Batch (frequent) $0.02-0.05 Hourly reports 1-24 hrs Batch (daily) $0.01-0.02 Daily ETL, data warehouse &gt; 24 hrs Batch (weekly) $0.005-0.01 Historical analysis <p>Optimization strategy: 1. Start with the slowest acceptable latency 2. Measure actual requirements (not perceived) 3. Optimize only when latency becomes a bottleneck 4. Use tiered approach: streaming for critical, batch for rest</p>"},{"location":"data-ingestion/batch-vs-streaming/#common-patterns","title":"Common Patterns","text":""},{"location":"data-ingestion/batch-vs-streaming/#pattern-1-idempotent-ingestion","title":"Pattern 1: Idempotent Ingestion","text":"<p>Problem: Source may send duplicate data (retries, failures).</p> <p>Solution: Use idempotency keys.</p> <pre><code>def ingest_record(record):\n    idempotency_key = f\"{source}_{record.id}_{record.timestamp}\"\n\n    if exists_in_dedupe_table(idempotency_key):\n        return  # Already processed\n\n    process_record(record)\n    insert_dedupe_table(idempotency_key)\n</code></pre> <p>Storage: Use idempotency table with TTL (e.g., 7 days).</p>"},{"location":"data-ingestion/batch-vs-streaming/#pattern-2-checkpointing","title":"Pattern 2: Checkpointing","text":"<p>Problem: Long-running jobs fail partway through.</p> <p>Solution: Track progress, enable resume.</p> <pre><code>checkpoint = get_last_checkpoint(job_id)\nrecords = source.fetch_since(checkpoint.last_processed_id)\n\nfor record in records:\n    process(record)\n    checkpoint.update(record.id, record.timestamp)\n    checkpoint.save()  # Frequent saves\n</code></pre> <p>Storage: Checkpoint table or file (S3, GCS).</p>"},{"location":"data-ingestion/batch-vs-streaming/#pattern-3-backpressure-handling","title":"Pattern 3: Backpressure Handling","text":"<p>Problem: Source sends data faster than platform can process.</p> <p>Solution: Implement backpressure.</p> <pre><code># Option 1: Rate limiting\nrate_limiter = RateLimiter(max_per_second=1000)\nfor record in stream:\n    rate_limiter.wait()\n    process(record)\n\n# Option 2: Queue with max size\nqueue = Queue(maxsize=10000)\nif queue.full():\n    reject_request()  # Return 503\nelse:\n    queue.put(record)\n</code></pre>"},{"location":"data-ingestion/batch-vs-streaming/#pattern-4-schema-evolution","title":"Pattern 4: Schema Evolution","text":"<p>Problem: Source schema changes break ingestion.</p> <p>Solution: Schema registry + validation.</p> <pre><code>schema_registry = SchemaRegistry()\n\ndef ingest(record):\n    # Validate against latest schema\n    schema = schema_registry.get_latest('source_name')\n    validated = schema.validate(record)\n\n    # Handle backward compatibility\n    if not validated:\n        handle_schema_mismatch(record, schema)\n\n    store(validated)\n</code></pre> <p>Tools: Confluent Schema Registry, AWS Glue Schema Registry</p>"},{"location":"data-ingestion/batch-vs-streaming/#error-handling","title":"Error Handling","text":""},{"location":"data-ingestion/batch-vs-streaming/#retry-strategy","title":"Retry Strategy","text":"<p>Exponential backoff: <pre><code>max_retries = 5\nbase_delay = 1  # seconds\n\nfor attempt in range(max_retries):\n    try:\n        ingest(record)\n        break\n    except TransientError:\n        if attempt &lt; max_retries - 1:\n            delay = base_delay * (2 ** attempt)\n            sleep(delay)\n        else:\n            send_to_dlq(record)  # Dead letter queue\n</code></pre></p> <p>Retryable errors: Network timeouts, rate limits, temporary unavailability Non-retryable errors: Authentication failures, invalid schema, business logic errors</p>"},{"location":"data-ingestion/batch-vs-streaming/#dead-letter-queue-dlq","title":"Dead Letter Queue (DLQ)","text":"<p>Purpose: Store records that failed after all retries.</p> <p>Implementation: - Separate storage (S3, BigQuery table) - Alert on DLQ size - Manual review and reprocessing</p> <p>Monitoring: Track DLQ size, error types, reprocessing success rate.</p>"},{"location":"data-ingestion/batch-vs-streaming/#monitoring-observability","title":"Monitoring &amp; Observability","text":""},{"location":"data-ingestion/batch-vs-streaming/#key-metrics","title":"Key Metrics","text":"<p>Volume metrics: - Records/second - GB/day - Partition count</p> <p>Latency metrics: - End-to-end latency (source \u2192 storage) - Processing time per record - Queue depth</p> <p>Quality metrics: - Schema validation failures - Duplicate rate - Missing data rate</p> <p>Reliability metrics: - Success rate - Error rate by type - DLQ size - Retry count</p>"},{"location":"data-ingestion/batch-vs-streaming/#alerting","title":"Alerting","text":"<p>Critical alerts: - Ingestion stopped (zero records in last N minutes) - Error rate &gt; threshold (e.g., 5%) - Latency &gt; SLA (e.g., 15 minutes for batch) - DLQ size &gt; threshold</p> <p>Warning alerts: - Volume drop &gt; 20% - Schema drift detected - Cost spike &gt; 20%</p>"},{"location":"data-ingestion/batch-vs-streaming/#cost-optimization","title":"Cost Optimization","text":""},{"location":"data-ingestion/batch-vs-streaming/#common-cost-traps","title":"Common Cost Traps","text":"<ol> <li>Over-ingestion: Ingesting data that's never used</li> <li> <p>Solution: Track usage, archive unused sources</p> </li> <li> <p>Inefficient formats: Using JSON instead of Parquet</p> </li> <li> <p>Solution: Convert to columnar formats post-ingestion</p> </li> <li> <p>Redundant ingestion: Multiple pipelines for same source</p> </li> <li> <p>Solution: Centralize, reuse outputs</p> </li> <li> <p>Streaming when batch would suffice: 3-5x cost premium</p> </li> <li>Solution: Measure actual latency requirements</li> </ol>"},{"location":"data-ingestion/batch-vs-streaming/#optimization-techniques","title":"Optimization Techniques","text":"<ol> <li>Compression: Use Snappy or Zstd (2-5x reduction)</li> <li>Partitioning: Only process new partitions</li> <li>Incremental loads: Only fetch changed data</li> <li>Batching: Group small records into batches</li> <li>Lifecycle policies: Move old data to cheaper storage</li> </ol> <p>Expected savings: 20-40% with basic optimizations.</p>"},{"location":"data-ingestion/batch-vs-streaming/#next-steps","title":"Next Steps","text":"<ul> <li>Storage &amp; Data Architecture - How to store ingested data</li> <li>Cost Efficiency &amp; Scale - Advanced cost optimization</li> </ul>"},{"location":"data-ingestion/cdc/","title":"Change Data Capture","text":""},{"location":"data-ingestion/cdc/#change-data-capture-cdc","title":"Change Data Capture (CDC)","text":"<p>Capturing database changes in real-time to maintain synchronized data across systems.</p>"},{"location":"data-ingestion/cdc/#overview","title":"Overview","text":"<p>Change Data Capture (CDC) is a pattern for capturing and propagating changes made to a database. Instead of periodically querying for changes, CDC reads the database transaction log to capture inserts, updates, and deletes as they happen.</p>"},{"location":"data-ingestion/cdc/#why-cdc","title":"Why CDC?","text":""},{"location":"data-ingestion/cdc/#problems-with-traditional-approaches","title":"Problems with Traditional Approaches","text":"<p>Full Extract: - \u274c Inefficient (transfers all data, even unchanged) - \u274c High source system load - \u274c Slow for large tables</p> <p>Timestamp-based Incremental: - \u274c Misses deletes - \u274c May miss updates if timestamp not updated - \u274c Requires source system changes</p> <p>CDC Solution: - \u2705 Captures all changes (inserts, updates, deletes) - \u2705 Low overhead (reads transaction log) - \u2705 Real-time or near real-time - \u2705 Maintains transaction consistency</p>"},{"location":"data-ingestion/cdc/#cdc-patterns","title":"CDC Patterns","text":""},{"location":"data-ingestion/cdc/#1-log-based-cdc","title":"1. Log-Based CDC","text":"<p>How it works: Reads database transaction logs (WAL, binlog, redo logs) to capture changes.</p> <p>Architecture: <pre><code>Database \u2192 Transaction Log \u2192 CDC Tool \u2192 Message Queue \u2192 Storage\n</code></pre></p> <p>Pros: - \u2705 Low overhead (doesn't query tables) - \u2705 Captures all changes - \u2705 Maintains transaction consistency - \u2705 Real-time</p> <p>Cons: - \u274c Requires database configuration - \u274c Database-specific (different for each DB)</p> <p>Tools: - Debezium (Kafka Connect) - AWS DMS (Database Migration Service) - Google Datastream - Striim</p>"},{"location":"data-ingestion/cdc/#2-trigger-based-cdc","title":"2. Trigger-Based CDC","text":"<p>How it works: Database triggers write changes to a change table.</p> <p>Architecture: <pre><code>Database \u2192 Trigger \u2192 Change Table \u2192 CDC Tool \u2192 Storage\n</code></pre></p> <p>Pros: - \u2705 Works with any database - \u2705 Captures all changes - \u2705 No external tools needed</p> <p>Cons: - \u274c Higher overhead (triggers on every change) - \u274c Requires database changes - \u274c May impact source system performance</p>"},{"location":"data-ingestion/cdc/#3-query-based-cdc","title":"3. Query-Based CDC","text":"<p>How it works: Poll for changes using timestamps or version columns.</p> <p>Architecture: <pre><code>CDC Tool \u2192 Query Database \u2192 Process Changes \u2192 Storage\n</code></pre></p> <p>Pros: - \u2705 Simple, no database changes - \u2705 Works with any database - \u2705 Easy to implement</p> <p>Cons: - \u274c May miss deletes - \u274c Polling overhead - \u274c Higher latency - \u274c May miss updates if timestamp not updated</p> <p>When to use: - Legacy systems - Can't modify database - Acceptable to miss some changes</p>"},{"location":"data-ingestion/cdc/#cdc-tools","title":"CDC Tools","text":""},{"location":"data-ingestion/cdc/#debezium","title":"Debezium","text":"<p>Best for: Kafka-based CDC, open source</p> <p>Pros: - \u2705 Open source - \u2705 Kafka-native - \u2705 Many database connectors - \u2705 Reliable</p> <p>Cons: - \u274c Requires Kafka infrastructure - \u274c Self-managed</p> <p>Use when: - Already using Kafka - Need open source solution - Have operations team</p>"},{"location":"data-ingestion/cdc/#google-datastream","title":"Google Datastream","text":"<p>Best for: GCP-native CDC, managed service</p> <p>Pros: - \u2705 Managed service (no ops) - \u2705 GCP-integrated - \u2705 Reliable - \u2705 Easy setup</p> <p>Cons: - \u274c GCP-only - \u274c Expensive - \u274c Limited database support</p> <p>Use when: - GCP stack - Want managed service - Cost acceptable</p>"},{"location":"data-ingestion/cdc/#aws-dms","title":"AWS DMS","text":"<p>Best for: AWS-native replication, migrations</p> <p>Pros: - \u2705 Managed service - \u2705 AWS-integrated - \u2705 Good for migrations</p> <p>Cons: - \u274c AWS-only - \u274c Can be expensive - \u274c Complex configuration</p> <p>Use when: - AWS stack - Need managed service - Database migrations</p>"},{"location":"data-ingestion/cdc/#cdc-current-state-patterns","title":"CDC + Current State Patterns","text":""},{"location":"data-ingestion/cdc/#problem","title":"Problem","text":"<p>CDC streams capture changes, but analytics often needs current state (latest value per key).</p>"},{"location":"data-ingestion/cdc/#solution-1-merge-pattern","title":"Solution 1: Merge Pattern","text":"<p>Merge CDC events into current state table:</p> <pre><code>MERGE INTO current_state AS target\nUSING cdc_events AS source\nON target.id = source.id\nWHEN MATCHED AND source.op = 'UPDATE' THEN\n  UPDATE SET col1 = source.col1, updated_at = source.timestamp\nWHEN MATCHED AND source.op = 'DELETE' THEN\n  DELETE\nWHEN NOT MATCHED AND source.op = 'INSERT' THEN\n  INSERT (id, col1, updated_at) VALUES (source.id, source.col1, source.timestamp)\n</code></pre> <p>Tools: Spark, Flink, BigQuery MERGE</p>"},{"location":"data-ingestion/cdc/#solution-2-snapshot-incremental","title":"Solution 2: Snapshot + Incremental","text":"<p>Periodic snapshots + incremental updates:</p> <pre><code>-- Daily snapshot\nCREATE TABLE current_state_2024_01_15 AS\nSELECT * FROM current_state_2024_01_14\nUNION ALL\nSELECT * FROM cdc_events\nWHERE date = '2024-01-15'\n</code></pre> <p>Pros: Simple, easy to reprocess Cons: Storage overhead, slower queries</p>"},{"location":"data-ingestion/cdc/#solution-3-event-sourcing","title":"Solution 3: Event Sourcing","text":"<p>Store all events, compute current state on read:</p> <pre><code>-- Current state = latest event per key\nSELECT DISTINCT ON (id) *\nFROM events\nORDER BY id, timestamp DESC\n</code></pre> <p>Pros: Full history, audit trail Cons: Expensive queries, complex logic</p> <p>Recommendation</p> <p>Use merge pattern for most cases. It's efficient and maintains current state.</p>"},{"location":"data-ingestion/cdc/#implementation-best-practices","title":"Implementation Best Practices","text":""},{"location":"data-ingestion/cdc/#1-handle-transaction-boundaries","title":"1. Handle Transaction Boundaries","text":"<p>CDC should maintain transaction consistency. Process all changes in a transaction together.</p>"},{"location":"data-ingestion/cdc/#2-handle-schema-changes","title":"2. Handle Schema Changes","text":"<p>Database schema changes can break CDC. Use schema registry to handle evolution.</p>"},{"location":"data-ingestion/cdc/#3-handle-failures","title":"3. Handle Failures","text":"<p>CDC must be resilient to failures: - Checkpoint progress - Handle duplicate events (idempotency) - Retry on failures</p>"},{"location":"data-ingestion/cdc/#4-monitor-lag","title":"4. Monitor Lag","text":"<p>Track CDC lag (time between change and processing): - Alert if lag &gt; threshold - Monitor queue depth - Track processing rate</p>"},{"location":"data-ingestion/cdc/#common-challenges","title":"Common Challenges","text":""},{"location":"data-ingestion/cdc/#challenge-1-high-volume-changes","title":"Challenge 1: High-Volume Changes","text":"<p>Problem: Database with millions of changes per day.</p> <p>Solution: - Partition by table/date - Parallel processing - Batch processing (micro-batch)</p>"},{"location":"data-ingestion/cdc/#challenge-2-schema-evolution","title":"Challenge 2: Schema Evolution","text":"<p>Problem: Database schema changes break CDC.</p> <p>Solution: - Schema registry - Backward-compatible changes - Versioned schemas</p>"},{"location":"data-ingestion/cdc/#challenge-3-transaction-consistency","title":"Challenge 3: Transaction Consistency","text":"<p>Problem: Need to maintain transaction boundaries.</p> <p>Solution: - Use transaction-aware CDC tools - Process transactions atomically - Handle partial transactions</p>"},{"location":"data-ingestion/cdc/#cost-considerations","title":"Cost Considerations","text":"<p>CDC costs: - Infrastructure (Kafka, compute) - Storage (change events) - Processing (transformation)</p> <p>Optimization: - Filter unnecessary changes - Compress change events - Archive old changes</p>"},{"location":"data-ingestion/cdc/#related-topics","title":"Related Topics","text":"<ul> <li>Batch vs Streaming - Other ingestion patterns</li> <li>Data Architecture - How to store CDC data</li> <li>Data Quality - Ensuring CDC data quality</li> </ul> <p>Next: Push vs Pull \u2192</p>"},{"location":"data-ingestion/push-vs-pull/","title":"Push vs Pull","text":""},{"location":"data-ingestion/push-vs-pull/#push-vs-pull","title":"Push vs Pull","text":"<p>Source-initiated vs platform-initiated ingestion patterns.</p>"},{"location":"data-ingestion/push-vs-pull/#overview","title":"Overview","text":"<p>Ingestion can be initiated by either the source system (push) or the platform (pull). Each approach has different trade-offs in terms of latency, complexity, and cost.</p>"},{"location":"data-ingestion/push-vs-pull/#push-source-initiated","title":"Push (Source-Initiated)","text":"<p>Architecture: <pre><code>Source System \u2192 Webhook/API \u2192 Platform Ingestion Endpoint\n</code></pre></p>"},{"location":"data-ingestion/push-vs-pull/#characteristics","title":"Characteristics","text":"<ul> <li>Real-time delivery - Source controls timing</li> <li>Event-driven - Data arrives as events happen</li> <li>Lower latency - No polling delay</li> </ul>"},{"location":"data-ingestion/push-vs-pull/#pros","title":"Pros","text":"<ul> <li>\u2705 Real-time delivery</li> <li>\u2705 Source controls timing</li> <li>\u2705 No polling overhead</li> <li>\u2705 Event-driven architecture</li> </ul>"},{"location":"data-ingestion/push-vs-pull/#cons","title":"Cons","text":"<ul> <li>\u274c Source must handle retries</li> <li>\u274c Platform must scale for bursts</li> <li>\u274c Requires source system changes</li> <li>\u274c More complex error handling</li> </ul>"},{"location":"data-ingestion/push-vs-pull/#when-to-use","title":"When to Use","text":"<ul> <li>Real-time requirements</li> <li>Source has reliable infrastructure</li> <li>You control the source system</li> <li>Event-driven architecture</li> </ul>"},{"location":"data-ingestion/push-vs-pull/#implementation","title":"Implementation","text":"<p>Webhook endpoint: <pre><code>@app.post(\"/ingest/{source}\")\nasync def ingest_webhook(source: str, data: dict):\n    # Validate request\n    if not validate_request(source, data):\n        return {\"error\": \"Invalid request\"}, 400\n\n    # Process data\n    process_data(source, data)\n\n    return {\"status\": \"success\"}\n</code></pre></p> <p>Key considerations: - Idempotency keys - Deduplicate retries - Rate limiting - Prevent abuse - Authentication - Secure endpoints - Backpressure - Reject when overloaded</p>"},{"location":"data-ingestion/push-vs-pull/#pull-platform-initiated","title":"Pull (Platform-Initiated)","text":"<p>Architecture: <pre><code>Platform Scheduler \u2192 Query Source \u2192 Process Results\n</code></pre></p>"},{"location":"data-ingestion/push-vs-pull/#characteristics_1","title":"Characteristics","text":"<ul> <li>Scheduled execution - Platform controls timing</li> <li>Polling-based - Query source periodically</li> <li>Higher latency - Depends on polling frequency</li> </ul>"},{"location":"data-ingestion/push-vs-pull/#pros_1","title":"Pros","text":"<ul> <li>\u2705 Platform controls rate</li> <li>\u2705 Easier backpressure</li> <li>\u2705 No source system changes</li> <li>\u2705 Simpler error handling</li> </ul>"},{"location":"data-ingestion/push-vs-pull/#cons_1","title":"Cons","text":"<ul> <li>\u274c Polling overhead</li> <li>\u274c May miss real-time events</li> <li>\u274c Higher latency</li> <li>\u274c May miss data if source unavailable</li> </ul>"},{"location":"data-ingestion/push-vs-pull/#when-to-use_1","title":"When to Use","text":"<ul> <li>Batch processing</li> <li>Source can't push</li> <li>Rate limiting needed</li> <li>Legacy systems</li> </ul>"},{"location":"data-ingestion/push-vs-pull/#implementation_1","title":"Implementation","text":"<p>Scheduled query: <pre><code>@schedule.every(hours=1)\ndef pull_data():\n    # Query source\n    data = query_source(\"SELECT * FROM table WHERE updated_at &gt; ?\", last_pull_time)\n\n    # Process data\n    process_data(data)\n\n    # Update last pull time\n    update_last_pull_time()\n</code></pre></p> <p>Optimization: - Incremental queries - Only fetch new data - Parallel pulls - Multiple workers - Adaptive polling - Increase frequency when data available</p>"},{"location":"data-ingestion/push-vs-pull/#comparison","title":"Comparison","text":"Aspect Push Pull Latency Low (real-time) Higher (polling delay) Complexity Higher (source changes) Lower (no source changes) Cost Higher (always-on) Lower (scheduled) Reliability Depends on source Platform-controlled Scalability Burst handling needed Easier to scale"},{"location":"data-ingestion/push-vs-pull/#hybrid-approach","title":"Hybrid Approach","text":"<p>Use both patterns: - Push for real-time, critical data - Pull for batch, non-critical data</p> <p>Example: <pre><code># Real-time events (push)\n@app.post(\"/events\")\nasync def ingest_events(data: dict):\n    process_realtime_events(data)\n\n# Batch data (pull)\n@schedule.daily()\ndef pull_batch_data():\n    process_batch_data()\n</code></pre></p>"},{"location":"data-ingestion/push-vs-pull/#decision-framework","title":"Decision Framework","text":"<p>Use Push when: - \u2705 Real-time requirement - \u2705 Source can push - \u2705 Event-driven architecture - \u2705 You control source system</p> <p>Use Pull when: - \u2705 Batch acceptable - \u2705 Source can't push - \u2705 Rate limiting needed - \u2705 Legacy systems</p>"},{"location":"data-ingestion/push-vs-pull/#best-practices","title":"Best Practices","text":""},{"location":"data-ingestion/push-vs-pull/#push-best-practices","title":"Push Best Practices","text":"<ol> <li>Idempotency - Handle duplicate events</li> <li>Rate limiting - Prevent abuse</li> <li>Authentication - Secure endpoints</li> <li>Backpressure - Reject when overloaded</li> <li>Retry logic - Source should retry on failure</li> </ol>"},{"location":"data-ingestion/push-vs-pull/#pull-best-practices","title":"Pull Best Practices","text":"<ol> <li>Incremental queries - Only fetch new data</li> <li>Checkpointing - Track last processed record</li> <li>Error handling - Retry on failures</li> <li>Adaptive polling - Adjust frequency based on data availability</li> <li>Parallel processing - Multiple workers for large sources</li> </ol>"},{"location":"data-ingestion/push-vs-pull/#related-topics","title":"Related Topics","text":"<ul> <li>Batch vs Streaming - Ingestion patterns</li> <li>CDC - Change data capture</li> <li>Data Architecture - Storage patterns</li> </ul> <p>Next: Data Architecture \u2192</p>"},{"location":"data-orchestration/","title":"Data Orchestration","text":""},{"location":"data-orchestration/#data-orchestration","title":"Data Orchestration","text":"<p>\"Most data outages are just bad communication bugs.\"</p> <p>Scheduling, coordinating, and managing data pipelines.</p>"},{"location":"data-orchestration/#overview","title":"Overview","text":"<p>Data orchestration is about coordinating multiple data pipelines, managing dependencies, handling failures, and ensuring data flows reliably through your platform.</p>"},{"location":"data-orchestration/#key-topics","title":"Key Topics","text":""},{"location":"data-orchestration/#airflow","title":"Airflow","text":"<p>Apache Airflow for workflow orchestration.</p> <p>Learn about: - Airflow concepts (DAGs, tasks, operators) - DAG design patterns - Best practices - Monitoring and troubleshooting</p>"},{"location":"data-orchestration/#dbt","title":"dbt","text":"<p>Data Build Tool for SQL-based transformations.</p> <p>Learn about: - dbt concepts (models, tests, macros) - Project structure - Testing framework - Documentation generation</p>"},{"location":"data-orchestration/#orchestration-tools","title":"Orchestration Tools","text":""},{"location":"data-orchestration/#apache-airflow","title":"Apache Airflow","text":"<p>Best for: Complex workflows, Python-based</p> <p>Pros: - \u2705 Mature, widely adopted - \u2705 Rich ecosystem - \u2705 Flexible (Python-based) - \u2705 Good UI and monitoring</p> <p>Cons: - \u274c Requires operations - \u274c Can be complex - \u274c Resource-intensive</p>"},{"location":"data-orchestration/#dbt_1","title":"dbt","text":"<p>Best for: SQL-based transformations, analytics engineering</p> <p>Pros: - \u2705 SQL-based (accessible) - \u2705 Great testing framework - \u2705 Documentation generation - \u2705 Version control friendly</p> <p>Cons: - \u274c SQL-only - \u274c Requires orchestration (Airflow, etc.)</p>"},{"location":"data-orchestration/#prefect-dagster","title":"Prefect / Dagster","text":"<p>Best for: Modern Python orchestration</p> <p>Pros: - \u2705 Better developer experience - \u2705 Modern architecture - \u2705 Good testing support</p> <p>Cons: - \u274c Less mature than Airflow - \u274c Smaller ecosystem</p>"},{"location":"data-orchestration/#related-topics","title":"Related Topics","text":"<ul> <li>Data Processing - What to orchestrate</li> <li>Data Quality - Quality checks in pipelines</li> </ul> <p>Next: Airflow \u2192</p>"},{"location":"data-orchestration/airflow/","title":"Airflow","text":""},{"location":"data-orchestration/airflow/#apache-airflow","title":"Apache Airflow","text":"<p>Workflow orchestration for data pipelines.</p>"},{"location":"data-orchestration/airflow/#overview","title":"Overview","text":"<p>Apache Airflow is an open-source platform for programmatically authoring, scheduling, and monitoring workflows. It's the de facto standard for data pipeline orchestration.</p>"},{"location":"data-orchestration/airflow/#key-concepts","title":"Key Concepts","text":""},{"location":"data-orchestration/airflow/#dags-directed-acyclic-graphs","title":"DAGs (Directed Acyclic Graphs)","text":"<p>DAG = Workflow definition</p> <p>Example: <pre><code>from airflow import DAG\nfrom airflow.operators.bash import BashOperator\nfrom datetime import datetime\n\ndag = DAG(\n    'ingest_data',\n    start_date=datetime(2024, 1, 1),\n    schedule_interval='@daily'\n)\n\nextract = BashOperator(\n    task_id='extract',\n    bash_command='python extract.py',\n    dag=dag\n)\n\ntransform = BashOperator(\n    task_id='transform',\n    bash_command='python transform.py',\n    dag=dag\n)\n\nload = BashOperator(\n    task_id='load',\n    bash_command='python load.py',\n    dag=dag\n)\n\nextract &gt;&gt; transform &gt;&gt; load\n</code></pre></p>"},{"location":"data-orchestration/airflow/#tasks","title":"Tasks","text":"<p>Task = Single unit of work</p> <p>Types: - Operators (Bash, Python, SQL) - Sensors (wait for conditions) - Hooks (connect to external systems)</p>"},{"location":"data-orchestration/airflow/#operators","title":"Operators","text":"<p>BashOperator - Run bash commands PythonOperator - Run Python functions SQLOperator - Run SQL queries</p>"},{"location":"data-orchestration/airflow/#best-practices","title":"Best Practices","text":"<ol> <li>Idempotency - Tasks should be rerunnable</li> <li>Atomicity - Tasks should succeed or fail completely</li> <li>Dependencies - Use clear task dependencies</li> <li>Error handling - Handle failures gracefully</li> <li>Monitoring - Set up alerts for failures</li> </ol>"},{"location":"data-orchestration/airflow/#related-topics","title":"Related Topics","text":"<ul> <li>dbt - SQL-based transformations</li> <li>Data Orchestration - Orchestration overview</li> </ul> <p>Next: dbt \u2192</p>"},{"location":"data-orchestration/dbt/","title":"dbt","text":""},{"location":"data-orchestration/dbt/#dbt-data-build-tool","title":"dbt (Data Build Tool)","text":"<p>SQL-based transformations for analytics engineering.</p>"},{"location":"data-orchestration/dbt/#overview","title":"Overview","text":"<p>dbt (data build tool) enables analytics engineers to transform data in their warehouses using SQL. It's the standard tool for analytics engineering.</p>"},{"location":"data-orchestration/dbt/#key-concepts","title":"Key Concepts","text":""},{"location":"data-orchestration/dbt/#models","title":"Models","text":"<p>Model = SQL transformation</p> <p>Example: <pre><code>-- models/staging/stg_orders.sql\n{{ config(materialized='view') }}\n\nselect\n    order_id,\n    user_id,\n    amount,\n    created_at\nfrom {{ source('raw', 'orders') }}\nwhere status = 'completed'\n</code></pre></p>"},{"location":"data-orchestration/dbt/#tests","title":"Tests","text":"<p>Test = Data quality check</p> <p>Example: <pre><code># models/schema.yml\nmodels:\n  - name: stg_orders\n    columns:\n      - name: order_id\n        tests:\n          - unique\n          - not_null\n</code></pre></p>"},{"location":"data-orchestration/dbt/#macros","title":"Macros","text":"<p>Macro = Reusable SQL</p> <p>Example: <pre><code>-- macros/date_spine.sql\n{% macro date_spine(start_date, end_date) %}\n    select date_day\n    from {{ ref('date_spine') }}\n    where date_day between '{{ start_date }}' and '{{ end_date }}'\n{% endmacro %}\n</code></pre></p>"},{"location":"data-orchestration/dbt/#best-practices","title":"Best Practices","text":"<ol> <li>Staging models - Clean raw data first</li> <li>Intermediate models - Build incrementally</li> <li>Marts - Final business logic</li> <li>Tests - Test everything</li> <li>Documentation - Document all models</li> </ol>"},{"location":"data-orchestration/dbt/#related-topics","title":"Related Topics","text":"<ul> <li>Airflow - Orchestrating dbt</li> <li>Data Orchestration - Orchestration overview</li> </ul> <p>Next: Data Processing \u2192</p>"},{"location":"data-processing/","title":"Data Processing","text":""},{"location":"data-processing/#data-processing","title":"Data Processing","text":"<p>\"Gen-Z doesn't hate complexity. They hate unclear systems.\"</p> <p>Transforming and analyzing data at scale.</p>"},{"location":"data-processing/#overview","title":"Overview","text":"<p>Data processing transforms raw data into analysis-ready datasets. This section covers the tools and patterns for processing data at scale.</p>"},{"location":"data-processing/#key-topics","title":"Key Topics","text":""},{"location":"data-processing/#apache-spark","title":"Apache Spark","text":"<p>Distributed data processing with Spark.</p> <p>Learn about: - Spark architecture - RDDs, DataFrames, Datasets - Optimization techniques - Best practices</p>"},{"location":"data-processing/#google-bigquery","title":"Google BigQuery","text":"<p>Serverless data warehouse and analytics.</p> <p>Learn about: - BigQuery architecture - Query optimization - Partitioning and clustering - Cost optimization</p>"},{"location":"data-processing/#processing-patterns","title":"Processing Patterns","text":""},{"location":"data-processing/#batch-processing","title":"Batch Processing","text":"<p>When to use: - Large volumes - No real-time requirement - Complex transformations</p> <p>Tools: Spark, BigQuery, Snowflake</p>"},{"location":"data-processing/#streaming-processing","title":"Streaming Processing","text":"<p>When to use: - Real-time requirements - Event-driven architecture - Low-latency use cases</p> <p>Tools: Flink, Spark Streaming, Dataflow</p>"},{"location":"data-processing/#related-topics","title":"Related Topics","text":"<ul> <li>Data Architecture - Where to process data</li> <li>Data Orchestration - Scheduling processing jobs</li> </ul> <p>Next: Apache Spark \u2192</p>"},{"location":"data-processing/bigquery/","title":"Google BigQuery","text":""},{"location":"data-processing/bigquery/#google-bigquery","title":"Google BigQuery","text":"<p>Serverless data warehouse and analytics.</p>"},{"location":"data-processing/bigquery/#overview","title":"Overview","text":"<p>Google BigQuery is a serverless, highly scalable data warehouse designed to make data analysis fast and cost-effective.</p>"},{"location":"data-processing/bigquery/#key-features","title":"Key Features","text":""},{"location":"data-processing/bigquery/#serverless","title":"Serverless","text":"<ul> <li>No infrastructure management</li> <li>Auto-scaling</li> <li>Pay per query</li> </ul>"},{"location":"data-processing/bigquery/#sql-interface","title":"SQL Interface","text":"<ul> <li>Standard SQL</li> <li>Powerful analytics functions</li> <li>ML integration</li> </ul>"},{"location":"data-processing/bigquery/#performance","title":"Performance","text":"<ul> <li>Columnar storage</li> <li>Automatic query optimization</li> <li>Cached results</li> </ul>"},{"location":"data-processing/bigquery/#best-practices","title":"Best Practices","text":"<ol> <li>Partitioning - Partition large tables</li> <li>Clustering - Cluster for common filters</li> <li>Query optimization - Select only needed columns</li> <li>Cost control - Use slots, caching, materialized views</li> <li>Lifecycle policies - Move old data to cheaper storage</li> </ol>"},{"location":"data-processing/bigquery/#related-topics","title":"Related Topics","text":"<ul> <li>Data Processing - Processing overview</li> <li>Data Architecture - Storage patterns</li> </ul> <p>Next: Data Quality \u2192</p>"},{"location":"data-processing/spark/","title":"Apache Spark","text":""},{"location":"data-processing/spark/#apache-spark","title":"Apache Spark","text":"<p>Distributed data processing at scale.</p>"},{"location":"data-processing/spark/#overview","title":"Overview","text":"<p>Apache Spark is a unified analytics engine for large-scale data processing. It's the standard for batch and streaming data processing.</p>"},{"location":"data-processing/spark/#key-concepts","title":"Key Concepts","text":""},{"location":"data-processing/spark/#rdds-resilient-distributed-datasets","title":"RDDs (Resilient Distributed Datasets)","text":"<p>RDD = Immutable distributed collection</p> <p>Example: <pre><code>rdd = sc.parallelize([1, 2, 3, 4, 5])\nrdd.map(lambda x: x * 2).collect()\n# [2, 4, 6, 8, 10]\n</code></pre></p>"},{"location":"data-processing/spark/#dataframes","title":"DataFrames","text":"<p>DataFrame = Structured data with schema</p> <p>Example: <pre><code>df = spark.read.parquet(\"s3://data/events/\")\ndf.filter(df.date == \"2024-01-15\") \\\n  .groupBy(\"user_id\") \\\n  .agg(sum(\"amount\").alias(\"total\")) \\\n  .show()\n</code></pre></p>"},{"location":"data-processing/spark/#datasets","title":"Datasets","text":"<p>Dataset = Typed DataFrame (Scala/Java)</p>"},{"location":"data-processing/spark/#best-practices","title":"Best Practices","text":"<ol> <li>Partitioning - Partition data appropriately</li> <li>Caching - Cache frequently used data</li> <li>Broadcast joins - Broadcast small tables</li> <li>Avoid shuffles - Minimize data movement</li> <li>Resource tuning - Tune executor memory/cores</li> </ol>"},{"location":"data-processing/spark/#related-topics","title":"Related Topics","text":"<ul> <li>Data Processing - Processing overview</li> <li>Data Architecture - Storage patterns</li> </ul> <p>Next: BigQuery \u2192</p>"},{"location":"data-quality/","title":"Data Quality","text":""},{"location":"data-quality/#data-quality","title":"Data Quality","text":"<p>\"Observability is just empathy for future engineers.\"</p> <p>Ensuring data is reliable, accurate, and trustworthy.</p> <p>\"Pipelines fail quietly. People fail when no one explains why they exist.\"</p>"},{"location":"data-quality/#overview","title":"Overview","text":"<p>Data quality isn't a nice-to-have\u2014it's a prerequisite for trust. Without quality, your platform becomes a liability, not an asset.</p>"},{"location":"data-quality/#key-topics","title":"Key Topics","text":""},{"location":"data-quality/#governance","title":"Governance","text":"<p>Data governance, SLAs, schema enforcement, observability.</p> <p>Learn about: - SLAs and freshness - Schema enforcement - Metadata and lineage - Ownership and accountability</p>"},{"location":"data-quality/#checks","title":"Checks","text":"<p>Data quality checks and testing.</p> <p>Learn about: - Quality dimensions - Testing frameworks - Automated checks - Quality scores</p>"},{"location":"data-quality/#quality-dimensions","title":"Quality Dimensions","text":"<ol> <li>Completeness - Are all expected records present?</li> <li>Accuracy - Does data reflect reality?</li> <li>Consistency - Is data consistent across sources?</li> <li>Timeliness - Is data fresh enough?</li> <li>Validity - Does data conform to schema?</li> <li>Uniqueness - Are there duplicates?</li> </ol>"},{"location":"data-quality/#best-practices","title":"Best Practices","text":""},{"location":"data-quality/#at-ingestion","title":"At Ingestion","text":"<ul> <li>Schema validation</li> <li>Completeness checks</li> <li>Uniqueness checks</li> </ul>"},{"location":"data-quality/#post-transformation","title":"Post-Transformation","text":"<ul> <li>Business rule validation</li> <li>Referential integrity</li> <li>Statistical checks</li> </ul>"},{"location":"data-quality/#continuous-monitoring","title":"Continuous Monitoring","text":"<ul> <li>Quality scores</li> <li>Anomaly detection</li> <li>Alerting</li> </ul>"},{"location":"data-quality/#related-topics","title":"Related Topics","text":"<ul> <li>Data Engineering - Platform fundamentals</li> <li>Data Architecture - Storage patterns</li> </ul> <p>Next: Governance \u2192</p>"},{"location":"data-quality/checks/","title":"Quality Checks","text":""},{"location":"data-quality/checks/#data-quality-checks","title":"Data Quality Checks","text":"<p>Implementing automated quality checks and testing.</p>"},{"location":"data-quality/checks/#overview","title":"Overview","text":"<p>Data quality checks validate that data meets expectations. They should be automated, run continuously, and alert when issues are detected.</p>"},{"location":"data-quality/checks/#quality-dimensions","title":"Quality Dimensions","text":""},{"location":"data-quality/checks/#1-completeness","title":"1. Completeness","text":"<p>Check: Are all expected records present?</p> <p>Example: <pre><code># Check record count\nexpected_count = 10000\nactual_count = df.count()\n\nif actual_count &lt; expected_count * 0.99:\n    raise QualityCheckFailed(\"Record count below threshold\")\n</code></pre></p>"},{"location":"data-quality/checks/#2-accuracy","title":"2. Accuracy","text":"<p>Check: Does data reflect reality?</p> <p>Example: <pre><code># Check business rules\nif df.filter(df.order_amount &lt; 0).count() &gt; 0:\n    raise QualityCheckFailed(\"Negative order amounts found\")\n</code></pre></p>"},{"location":"data-quality/checks/#3-consistency","title":"3. Consistency","text":"<p>Check: Is data consistent across sources?</p> <p>Example: <pre><code># Compare across sources\nsource1_sum = df1.select(sum(\"amount\")).first()[0]\nsource2_sum = df2.select(sum(\"amount\")).first()[0]\n\nif abs(source1_sum - source2_sum) &gt; threshold:\n    raise QualityCheckFailed(\"Inconsistent totals\")\n</code></pre></p>"},{"location":"data-quality/checks/#4-timeliness-freshness","title":"4. Timeliness (Freshness)","text":"<p>Check: Is data fresh enough?</p> <p>Example: <pre><code># Check data age\nmax_age = df.select(max(\"ingestion_timestamp\")).first()[0]\ncurrent_time = datetime.now()\n\nif (current_time - max_age).total_seconds() &gt; 3600:\n    raise QualityCheckFailed(\"Data is stale\")\n</code></pre></p>"},{"location":"data-quality/checks/#5-validity","title":"5. Validity","text":"<p>Check: Does data conform to schema?</p> <p>Example: <pre><code># Schema validation\nschema = StructType([\n    StructField(\"user_id\", StringType(), False),\n    StructField(\"email\", StringType(), False)\n])\n\ntry:\n    df = spark.createDataFrame(data, schema)\nexcept Exception as e:\n    raise QualityCheckFailed(f\"Schema validation failed: {e}\")\n</code></pre></p>"},{"location":"data-quality/checks/#6-uniqueness","title":"6. Uniqueness","text":"<p>Check: Are there duplicates?</p> <p>Example: <pre><code># Check for duplicates\nduplicate_count = df.groupBy(\"id\").count().filter(\"count &gt; 1\").count()\n\nif duplicate_count &gt; 0:\n    raise QualityCheckFailed(f\"Found {duplicate_count} duplicate IDs\")\n</code></pre></p>"},{"location":"data-quality/checks/#testing-frameworks","title":"Testing Frameworks","text":""},{"location":"data-quality/checks/#great-expectations","title":"Great Expectations","text":"<p>Best for: Comprehensive quality testing</p> <p>Example: <pre><code>import great_expectations as ge\n\ndf = ge.read_csv(\"data.csv\")\n\n# Expectation: No null values\ndf.expect_column_values_to_not_be_null(\"user_id\")\n\n# Expectation: Values in range\ndf.expect_column_values_to_be_between(\"age\", 0, 120)\n\n# Expectation: Unique values\ndf.expect_column_values_to_be_unique(\"user_id\")\n\n# Validate\nresults = df.validate()\n</code></pre></p>"},{"location":"data-quality/checks/#dbt-tests","title":"dbt Tests","text":"<p>Best for: SQL-based quality checks</p> <p>Example: <pre><code>-- models/schema.yml\nmodels:\n  - name: users\n    columns:\n      - name: user_id\n        tests:\n          - unique\n          - not_null\n      - name: email\n        tests:\n          - unique\n          - not_null\n          - accepted_values:\n              values: ['@company.com']\n</code></pre></p>"},{"location":"data-quality/checks/#custom-validators","title":"Custom Validators","text":"<p>Best for: Business-specific rules</p> <p>Example: <pre><code>def validate_order_data(df):\n    checks = [\n        check_completeness(df),\n        check_accuracy(df),\n        check_uniqueness(df)\n    ]\n\n    failures = [c for c in checks if not c.passed]\n    if failures:\n        raise QualityCheckFailed(failures)\n</code></pre></p>"},{"location":"data-quality/checks/#automated-checks","title":"Automated Checks","text":""},{"location":"data-quality/checks/#in-cicd","title":"In CI/CD","text":"<p>Run tests before deployment: <pre><code># .github/workflows/quality.yml\n- name: Run quality tests\n  run: |\n    dbt test\n    pytest tests/quality/\n</code></pre></p>"},{"location":"data-quality/checks/#in-pipelines","title":"In Pipelines","text":"<p>Check as pipeline stage: <pre><code>def quality_check_stage(df):\n    checks = [\n        completeness_check(df),\n        uniqueness_check(df),\n        business_rule_check(df)\n    ]\n\n    if any(check.failed for check in checks):\n        send_to_quarantine(df)\n        alert_owner()\n\n    return df\n</code></pre></p>"},{"location":"data-quality/checks/#quality-scores","title":"Quality Scores","text":""},{"location":"data-quality/checks/#composite-score","title":"Composite Score","text":"<p>Calculate overall quality: <pre><code>quality_score = (\n    completeness_score * 0.3 +\n    accuracy_score * 0.3 +\n    freshness_score * 0.2 +\n    consistency_score * 0.2\n)\n</code></pre></p>"},{"location":"data-quality/checks/#tracking","title":"Tracking","text":"<p>Monitor over time: - Quality trends - Degradation detection - Improvement tracking</p>"},{"location":"data-quality/checks/#best-practices","title":"Best Practices","text":"<ol> <li>Automate - Don't rely on manual checks</li> <li>Fail fast - Catch issues early</li> <li>Alert - Notify when quality drops</li> <li>Document - Document all checks</li> <li>Review - Regularly review and update checks</li> </ol>"},{"location":"data-quality/checks/#related-topics","title":"Related Topics","text":"<ul> <li>Governance - Quality governance framework</li> <li>Data Engineering - Platform fundamentals</li> </ul> <p>Next: Data Engineering \u2192</p>"},{"location":"data-quality/governance/","title":"Governance","text":""},{"location":"data-quality/governance/#quality-governance-observability","title":"Quality, Governance &amp; Observability","text":"<p>\"Observability is just empathy for future engineers.\"</p> <p>Data quality and governance aren't nice-to-haves\u2014they're prerequisites for trust. Without them, your platform becomes a liability, not an asset. This chapter covers how to build quality into your platform and govern data effectively.</p> <p>\"Pipelines fail quietly. People fail when no one explains why they exist.\"</p>"},{"location":"data-quality/governance/#data-quality-framework","title":"Data Quality Framework","text":""},{"location":"data-quality/governance/#quality-dimensions","title":"Quality Dimensions","text":"<p>1. Completeness - Are all expected records present? - Metrics: Record count, null rate, missing partitions</p> <p>2. Accuracy - Does data reflect reality? - Metrics: Validation failures, business rule violations</p> <p>3. Consistency - Is data consistent across sources? - Metrics: Cross-source comparisons, duplicate rates</p> <p>4. Timeliness (Freshness) - Is data fresh enough? - Metrics: Data age, SLA compliance</p> <p>5. Validity - Does data conform to schema? - Metrics: Schema validation failures, type mismatches</p> <p>6. Uniqueness - Are there duplicates? - Metrics: Duplicate count, primary key violations</p>"},{"location":"data-quality/governance/#quality-checks","title":"Quality Checks","text":"<p>At ingestion: <pre><code># Schema validation\nschema = get_contract_schema(source)\nif not schema.validate(record):\n    reject_with_error(record, \"Schema violation\")\n\n# Completeness check\nif record_count &lt; expected_min:\n    alert(\"Low record count\")\n\n# Uniqueness check\nif duplicate_count &gt; threshold:\n    alert(\"High duplicate rate\")\n</code></pre></p> <p>Post-transformation: <pre><code># Business rule validation\nif order_amount &lt; 0:\n    flag_anomaly(\"Negative order amount\")\n\n# Referential integrity\nif user_id not in users_table:\n    flag_anomaly(\"Orphaned record\")\n\n# Statistical checks\nif current_avg &gt; historical_avg * 2:\n    flag_anomaly(\"Unusual spike\")\n</code></pre></p> <p>Tools: Great Expectations, dbt tests, custom validators</p>"},{"location":"data-quality/governance/#slas-and-freshness","title":"SLAs and Freshness","text":""},{"location":"data-quality/governance/#defining-slas","title":"Defining SLAs","text":"<p>SLA components: - Freshness: Maximum acceptable data age - Availability: Uptime target (e.g., 99.9%) - Quality: Minimum quality thresholds - Latency: End-to-end processing time</p> <p>Example SLA: <pre><code>source: user_events\nsla:\n  freshness: 15 minutes  # Data must be &lt; 15 min old\n  availability: 99.9%    # Available 99.9% of time\n  quality:\n    completeness: &gt; 99%\n    accuracy: &gt; 99.5%\n  latency:\n    p50: &lt; 5 minutes\n    p95: &lt; 15 minutes\n    p99: &lt; 30 minutes\n</code></pre></p>"},{"location":"data-quality/governance/#freshness-monitoring","title":"Freshness Monitoring","text":"<p>Track data age: <pre><code>-- Example: Check freshness\nSELECT\n  source,\n  MAX(ingestion_timestamp) as last_ingestion,\n  CURRENT_TIMESTAMP - MAX(ingestion_timestamp) as age,\n  CASE\n    WHEN age &gt; INTERVAL '15 minutes' THEN 'VIOLATED'\n    ELSE 'OK'\n  END as status\nFROM raw.events\nGROUP BY source\n</code></pre></p> <p>Alerting: - Alert when freshness &gt; SLA - Alert on trends (getting slower) - Alert on complete stops</p> <p>Dashboards: - Freshness by source (real-time) - SLA compliance over time - Violation trends</p>"},{"location":"data-quality/governance/#schema-enforcement","title":"Schema Enforcement","text":""},{"location":"data-quality/governance/#schema-evolution","title":"Schema Evolution","text":"<p>Backward compatibility rules: - \u2705 Add optional fields - \u2705 Make required fields optional - \u274c Remove fields (without deprecation period) - \u274c Change field types (without migration)</p> <p>Versioning strategy: <pre><code>schema:\n  version: 2.0\n  changes_from_1.0:\n    - Added: new_field (optional)\n    - Deprecated: old_field (remove in 3.0)\n    - Changed: field_type (with migration path)\n</code></pre></p> <p>Migration process: 1. Deploy new schema version 2. Support both versions (dual-write) 3. Migrate consumers to new version 4. Deprecate old version 5. Remove old version</p>"},{"location":"data-quality/governance/#schema-registry","title":"Schema Registry","text":"<p>Purpose: Centralized schema management</p> <p>Features: - Schema storage and versioning - Compatibility checking - Client libraries (auto-validation)</p> <p>Tools: Confluent Schema Registry, AWS Glue Schema Registry, custom</p> <p>Usage: <pre><code># Register schema\nschema_registry.register(\n    subject=\"user_events\",\n    schema=user_events_schema,\n    compatibility=\"BACKWARD\"\n)\n\n# Validate on ingestion\nschema = schema_registry.get_latest(\"user_events\")\nif not schema.validate(record):\n    reject(\"Schema violation\")\n</code></pre></p>"},{"location":"data-quality/governance/#metadata-and-lineage","title":"Metadata and Lineage","text":""},{"location":"data-quality/governance/#metadata-types","title":"Metadata Types","text":"<p>Technical metadata: - Schema, data types, partitions - Storage location, format - Ingestion timestamps, versions</p> <p>Business metadata: - Description, purpose - Owner, contact - Business glossary terms - Data classification (PII, sensitive)</p> <p>Operational metadata: - Freshness, quality metrics - Usage statistics - Cost attribution - Dependencies</p>"},{"location":"data-quality/governance/#data-catalog","title":"Data Catalog","text":"<p>Purpose: Discoverable, searchable metadata</p> <p>Features: - Search by name, description, tags - Browse by domain, owner - View schema, samples, statistics - See lineage, usage</p> <p>Tools: DataHub, Collibra, AWS Glue Catalog, custom</p> <p>Example entry: <pre><code>name: user_events\ndescription: User interaction events (clicks, views, purchases)\nowner: analytics-team@company.com\ndomain: customer\ntags: [events, user-behavior, pii]\nschema:\n  - name: user_id\n    type: string\n    description: Unique user identifier\n  - name: event_type\n    type: string\n    enum: [click, view, purchase]\nlineage:\n  sources: [web_app, mobile_app]\n  consumers: [analytics_dashboards, ml_models]\n</code></pre></p>"},{"location":"data-quality/governance/#lineage-tracking","title":"Lineage Tracking","text":"<p>Purpose: Understand data flow and dependencies</p> <p>Types: - Upstream: Where data comes from - Downstream: Who uses this data - Transformation: How data is transformed</p> <p>Implementation: <pre><code># Track lineage automatically\n@track_lineage(\n    inputs=[\"raw.events\"],\n    outputs=[\"curated.user_events\"],\n    transformation=\"filter_and_aggregate\"\n)\ndef transform_events():\n    ...\n</code></pre></p> <p>Use cases: - Impact analysis (what breaks if source changes?) - Root cause analysis (where did bad data come from?) - Compliance (data flow documentation)</p> <p>Visualization: <pre><code>raw.events \u2192 transform \u2192 curated.user_events \u2192 dashboard\n                \u2193\n            ml_features \u2192 model\n</code></pre></p>"},{"location":"data-quality/governance/#ownership-and-accountability","title":"Ownership and Accountability","text":""},{"location":"data-quality/governance/#data-ownership-model","title":"Data Ownership Model","text":"<p>Owner responsibilities: - Define and maintain contracts - Ensure quality and freshness - Respond to issues - Approve schema changes - Optimize costs</p> <p>Assignment: - By domain (e.g., analytics team owns analytics data) - By source system (e.g., payments team owns payment data) - Explicit assignment in catalog</p> <p>Escalation: - Owner unresponsive \u2192 manager - Critical issue \u2192 on-call rotation</p>"},{"location":"data-quality/governance/#access-control","title":"Access Control","text":"<p>Principle of least privilege: - Users get minimum access needed - Role-based access control (RBAC) - Row-level security for sensitive data</p> <p>Implementation: <pre><code>-- Example: Row-level security\nCREATE POLICY user_data_policy ON user_events\nFOR SELECT\nUSING (user_id = current_user_id() OR is_admin());\n\n-- Column masking\nCREATE POLICY mask_pii ON users\nFOR SELECT\nUSING (\n  CASE\n    WHEN has_pii_access() THEN email\n    ELSE '***'\n  END\n);\n</code></pre></p> <p>Tools: BigQuery row-level security, Snowflake dynamic masking, custom</p>"},{"location":"data-quality/governance/#observability","title":"Observability","text":""},{"location":"data-quality/governance/#metrics","title":"Metrics","text":"<p>Pipeline metrics: - Volume (records/second, GB/day) - Latency (end-to-end, per stage) - Success/failure rates - Error types and counts</p> <p>Data metrics: - Freshness (data age) - Quality scores (by dimension) - Schema drift - Duplicate rates</p> <p>Infrastructure metrics: - Resource utilization (CPU, memory, storage) - Queue depths - Cache hit rates - Network throughput</p> <p>Business metrics: - Cost per GB, per query - User satisfaction - Time to value</p>"},{"location":"data-quality/governance/#logging","title":"Logging","text":"<p>Structured logging: <pre><code>{\n  \"timestamp\": \"2024-01-15T10:30:00Z\",\n  \"level\": \"INFO\",\n  \"pipeline\": \"user_events_ingestion\",\n  \"stage\": \"ingestion\",\n  \"records_processed\": 10000,\n  \"duration_ms\": 5000,\n  \"status\": \"success\"\n}\n</code></pre></p> <p>Log levels: - DEBUG: Detailed execution info - INFO: Normal operations - WARN: Potential issues - ERROR: Failures that don't stop pipeline - CRITICAL: Failures that stop pipeline</p> <p>Retention: 30-90 days for operational logs</p>"},{"location":"data-quality/governance/#alerting","title":"Alerting","text":"<p>Alert categories:</p> <p>Critical (page on-call): - Pipeline stopped (zero records) - SLA violation (freshness, availability) - Data quality breach (completeness &lt; threshold)</p> <p>Warning (notify team): - Degradation (latency increasing, quality dropping) - Cost anomaly (spike &gt; 20%) - Schema drift detected</p> <p>Info (dashboard only): - Normal operations - Scheduled maintenance - Capacity planning</p> <p>Alert design: - Actionable: Clear what to do - Specific: Not \"something is wrong\" - Rare: Only alert on real issues - Grouped: Related alerts together</p> <p>Example: <pre><code>\u274c BAD: \"Pipeline error\"\n\u2705 GOOD: \"user_events ingestion failed: Schema validation error on field 'user_id' (expected string, got int). Last successful: 10:15 AM. Owner: analytics-team.\"\n</code></pre></p>"},{"location":"data-quality/governance/#dashboards","title":"Dashboards","text":"<p>Operational dashboard: - Pipeline health (all pipelines) - Freshness by source - Quality scores - Error rates - Cost trends</p> <p>Team dashboard: - My pipelines (owned by team) - My data (sources and consumers) - My costs - My SLAs</p> <p>Executive dashboard: - Platform health (high-level) - Total cost and trends - Adoption metrics - SLA compliance</p> <p>Tools: Grafana, Datadog, custom dashboards</p>"},{"location":"data-quality/governance/#root-cause-analysis-rca","title":"Root Cause Analysis (RCA)","text":""},{"location":"data-quality/governance/#process","title":"Process","text":"<p>1. Detect issue: - Alert fires or user reports</p> <p>2. Gather context: - When did it start? - What changed recently? - What's the scope?</p> <p>3. Trace lineage: - Where did bad data come from? - What transformations touched it? - Who consumes it?</p> <p>4. Identify root cause: - Source system change? - Schema drift? - Transformation bug? - Infrastructure issue?</p> <p>5. Fix and prevent: - Immediate fix - Long-term prevention - Update monitoring</p>"},{"location":"data-quality/governance/#rca-template","title":"RCA Template","text":"<pre><code>## Incident: [Title]\n\n**Time**: [When]\n**Impact**: [What broke, who affected]\n**Duration**: [How long]\n\n**Timeline**:\n- 10:00 AM: Alert fired\n- 10:05 AM: Investigation started\n- 10:15 AM: Root cause identified\n- 10:30 AM: Fix deployed\n- 10:35 AM: Verified resolution\n\n**Root Cause**:\n[What actually caused it]\n\n**Fix**:\n[What we did]\n\n**Prevention**:\n[How we'll prevent it]\n</code></pre>"},{"location":"data-quality/governance/#quality-automation","title":"Quality Automation","text":""},{"location":"data-quality/governance/#automated-quality-checks","title":"Automated Quality Checks","text":"<p>In CI/CD: <pre><code># Example: dbt tests in CI\n- name: Run data quality tests\n  run: dbt test\n  on:\n    schedule: daily\n    on_push: true\n</code></pre></p> <p>In pipelines: <pre><code># Quality checks as pipeline stage\ndef quality_check_stage(df):\n    checks = [\n        completeness_check(df),\n        uniqueness_check(df),\n        business_rule_check(df)\n    ]\n\n    if any(check.failed for check in checks):\n        send_to_quarantine(df)\n        alert_owner()\n\n    return df\n</code></pre></p>"},{"location":"data-quality/governance/#quality-scores","title":"Quality Scores","text":"<p>Composite score: <pre><code>quality_score = (\n    completeness_score * 0.3 +\n    accuracy_score * 0.3 +\n    freshness_score * 0.2 +\n    consistency_score * 0.2\n)\n</code></pre></p> <p>Track over time: - Quality trends - Degradation detection - Improvement tracking</p>"},{"location":"data-quality/governance/#compliance-and-privacy","title":"Compliance and Privacy","text":""},{"location":"data-quality/governance/#data-classification","title":"Data Classification","text":"<p>Categories: - Public: No restrictions - Internal: Company use only - Confidential: Restricted access - PII: Personally identifiable information - Sensitive: Financial, health data</p> <p>Tagging: <pre><code>data_classification: PII\nprivacy_level: high\nretention_days: 365\naccess_restrictions: [encryption, masking, audit_logging]\n</code></pre></p>"},{"location":"data-quality/governance/#gdpr-privacy-compliance","title":"GDPR / Privacy Compliance","text":"<p>Requirements: - Right to access - Right to deletion - Data minimization - Consent tracking</p> <p>Implementation: - Tag PII data - Automated deletion (retention policies) - Access logging - Consent management</p>"},{"location":"data-quality/governance/#audit-logging","title":"Audit Logging","text":"<p>Log all access: - Who accessed what data - When - Why (query, purpose) - Result (rows returned)</p> <p>Retention: 7 years for compliance</p> <p>Tools: Cloud audit logs, custom logging</p>"},{"location":"data-quality/governance/#next-steps","title":"Next Steps","text":"<ul> <li>Cost Efficiency &amp; Scale - Optimize costs while maintaining quality</li> <li>Tooling Landscape - Tools for quality and governance</li> </ul>"},{"location":"reference/future-trends/","title":"Future Trends","text":""},{"location":"reference/future-trends/#future-emerging-trends","title":"Future &amp; Emerging Trends","text":"<p>The data engineering landscape evolves rapidly. This chapter covers emerging trends that are shaping the future of data platforms, with a pragmatic, production-focused perspective.</p>"},{"location":"reference/future-trends/#data-contracts","title":"Data Contracts","text":""},{"location":"reference/future-trends/#the-concept","title":"The Concept","text":"<p>Data contracts are formal agreements between data producers and consumers that define: - Schema (with evolution rules) - SLAs (freshness, availability, quality) - Ownership and accountability - Cost attribution</p>"},{"location":"reference/future-trends/#why-now","title":"Why Now?","text":"<p>Problems they solve: - Schema drift breaking downstream - Unclear expectations (what's the SLA?) - Ownership confusion - Cost attribution issues</p> <p>Industry momentum: - Adopted by companies like Netflix, Uber, LinkedIn - Tools emerging (Pydantic, JSON Schema, custom) - Growing recognition of need</p>"},{"location":"reference/future-trends/#implementation","title":"Implementation","text":"<p>Contract definition: <pre><code># Example: Data contract\nsource: user_events\nversion: 1.0\nowner: analytics-team@company.com\nsla:\n  freshness: 15 minutes\n  availability: 99.9%\nschema:\n  type: object\n  properties:\n    user_id:\n      type: string\n      required: true\n    event_type:\n      type: string\n      enum: [click, view, purchase]\n  evolution: backward_compatible\nquality:\n  completeness: &gt; 99%\n  uniqueness: &gt; 99.9%\n</code></pre></p> <p>Enforcement: - Validate at ingestion boundary - Reject violations - Alert on drift - Track compliance</p> <p>Tools: Custom (most common), Pydantic, JSON Schema, emerging SaaS</p>"},{"location":"reference/future-trends/#adoption-path","title":"Adoption Path","text":"<ol> <li>Start small: Define contracts for critical sources</li> <li>Automate validation: Build into ingestion pipeline</li> <li>Expand: Gradually cover all sources</li> <li>Evolve: Refine based on learnings</li> </ol> <p>Timeline: 6-12 months for full adoption</p>"},{"location":"reference/future-trends/#data-mesh-pragmatic-view","title":"Data Mesh (Pragmatic View)","text":""},{"location":"reference/future-trends/#the-hype-vs-reality","title":"The Hype vs Reality","text":"<p>Hype: \"Data mesh will solve all your problems!\"</p> <p>Reality: Data mesh is an organizational and architectural pattern, not a silver bullet.</p>"},{"location":"reference/future-trends/#core-principles","title":"Core Principles","text":"<ol> <li>Domain ownership: Domains own their data end-to-end</li> <li>Data as a product: Treat data as first-class products</li> <li>Self-serve infrastructure: Platform enables, doesn't control</li> <li>Federated governance: Standards and policies, not central control</li> </ol>"},{"location":"reference/future-trends/#when-it-makes-sense","title":"When It Makes Sense","text":"<p>Good fit: - Large organizations (1000+ engineers) - Multiple independent domains - Strong domain expertise - Need for speed and autonomy</p> <p>Not a good fit: - Small organizations (&lt; 100 engineers) - Centralized data team works well - Limited domain expertise - Need for strong central governance</p>"},{"location":"reference/future-trends/#pragmatic-approach","title":"Pragmatic Approach","text":"<p>Don't: Rip and replace everything</p> <p>Do:  1. Start with platform thinking (self-serve, contracts) 2. Gradually shift ownership to domains 3. Maintain central platform for infrastructure 4. Federate governance (standards, not control)</p> <p>Hybrid model (recommended): - Platform team: Infrastructure, standards, tooling - Domain teams: Business logic, transformations, quality - Shared: Governance framework, cost optimization</p>"},{"location":"reference/future-trends/#timeline","title":"Timeline","text":"<p>Full data mesh: 2-3 years (if it makes sense for your org)</p> <p>Pragmatic adoption: Start with platform + contracts, evolve gradually</p>"},{"location":"reference/future-trends/#feature-stores","title":"Feature Stores","text":""},{"location":"reference/future-trends/#the-problem","title":"The Problem","text":"<p>ML feature management challenges: - Features defined in multiple places (inconsistent) - No feature reuse (duplication) - No feature versioning - Hard to serve features at scale (latency)</p>"},{"location":"reference/future-trends/#the-solution-feature-stores","title":"The Solution: Feature Stores","text":"<p>Feature store = Centralized system for: - Feature definition and versioning - Feature computation (batch + streaming) - Feature serving (low-latency lookups) - Feature discovery and reuse</p>"},{"location":"reference/future-trends/#architecture","title":"Architecture","text":"<pre><code>Feature Definitions \u2192 Feature Computation \u2192 Feature Storage \u2192 Feature Serving\n                         (Batch + Stream)      (Online + Offline)    (API)\n</code></pre> <p>Components: - Offline store: Historical features (for training) - Online store: Real-time features (for inference) - Transformation: Batch + streaming computation - Serving API: Low-latency lookups</p>"},{"location":"reference/future-trends/#tools","title":"Tools","text":"<p>Feast (Open Source) - Pros: Open source, flexible, growing - Cons: Requires operations, less mature - Use when: Want open source, have resources</p> <p>Tecton - Pros: Managed, production-ready, great UX - Cons: Expensive, vendor lock-in - Use when: Want managed, production-critical</p> <p>SageMaker Feature Store (AWS) - Pros: AWS-integrated, managed - Cons: AWS-only, less mature - Use when: AWS stack, need managed</p> <p>Custom - Pros: Full control, tailored - Cons: High maintenance - Use when: Unique requirements</p>"},{"location":"reference/future-trends/#when-to-adopt","title":"When to Adopt","text":"<p>Adopt when: - Multiple ML models (need feature reuse) - Real-time inference (need online serving) - Feature complexity (many features, transformations) - Team size (5+ ML engineers)</p> <p>Don't adopt when: - Single model, simple features - Batch-only inference - Small team (&lt; 5 ML engineers)</p> <p>Timeline: 6-12 months to build/buy and adopt</p>"},{"location":"reference/future-trends/#ai-assisted-data-engineering","title":"AI-Assisted Data Engineering","text":""},{"location":"reference/future-trends/#current-state","title":"Current State","text":"<p>AI tools for data engineering: - Code generation: GitHub Copilot, Cursor, ChatGPT - SQL generation: Text-to-SQL (GPT, Claude) - Documentation: Auto-generate from code - Quality: Anomaly detection, auto-fixing</p>"},{"location":"reference/future-trends/#use-cases","title":"Use Cases","text":"<p>1. Code Generation <pre><code># Prompt: \"Create a Spark job that reads from S3, filters by date, and writes to Parquet\"\n# AI generates:\ndf = spark.read.parquet(\"s3://raw/events/\")\ndf.filter(df.date &gt;= \"2024-01-01\").write.parquet(\"s3://curated/events/\")\n</code></pre></p> <p>2. SQL Generation <pre><code>-- Prompt: \"Show me daily revenue by product category for last 30 days\"\n-- AI generates:\nSELECT\n  DATE(order_date) as date,\n  product_category,\n  SUM(amount) as revenue\nFROM orders\nWHERE order_date &gt;= CURRENT_DATE - 30\nGROUP BY DATE(order_date), product_category\nORDER BY date DESC, revenue DESC\n</code></pre></p> <p>3. Documentation - Auto-generate data catalog entries - Generate pipeline documentation - Create data dictionaries</p> <p>4. Quality &amp; Anomaly Detection - Detect schema drift - Identify data quality issues - Suggest fixes</p>"},{"location":"reference/future-trends/#limitations","title":"Limitations","text":"<p>Current limitations: - Not always correct (requires review) - Limited context (may miss edge cases) - Security concerns (code in AI tools) - Cost (API usage)</p> <p>Best practices: - Use as copilot, not autopilot - Always review generated code - Don't put sensitive data in prompts - Measure productivity gains</p>"},{"location":"reference/future-trends/#future-outlook","title":"Future Outlook","text":"<p>Near-term (1-2 years): - Better code generation - More specialized tools - Better integration (IDEs, platforms)</p> <p>Long-term (3-5 years): - Autonomous pipeline generation - Self-healing pipelines - Natural language to pipeline</p>"},{"location":"reference/future-trends/#real-time-everything","title":"Real-Time Everything","text":""},{"location":"reference/future-trends/#trend","title":"Trend","text":"<p>Shift from batch to real-time: - Real-time analytics - Real-time ML inference - Real-time operational systems</p>"},{"location":"reference/future-trends/#drivers","title":"Drivers","text":"<ul> <li>User expectations: Real-time experiences</li> <li>Business needs: Fraud detection, recommendations</li> <li>Technology: Better streaming tools, lower latency</li> </ul>"},{"location":"reference/future-trends/#reality-check","title":"Reality Check","text":"<p>Not everything needs to be real-time: - Real-time is 3-5x more expensive - Adds complexity - May not provide value</p> <p>Decision framework: - Real-time requirement? \u2192 Streaming - Near real-time acceptable? \u2192 Micro-batch - Batch acceptable? \u2192 Batch</p> <p>Recommendation: Start with batch, move to real-time only when needed.</p>"},{"location":"reference/future-trends/#unified-batch-streaming","title":"Unified Batch + Streaming","text":""},{"location":"reference/future-trends/#trend_1","title":"Trend","text":"<p>Unified frameworks for batch and streaming: - Same code for batch and streaming - Same APIs and abstractions - Easier to reason about</p>"},{"location":"reference/future-trends/#tools_1","title":"Tools","text":"<p>Apache Flink - Unified batch + streaming - Same APIs - Good performance</p> <p>Google Dataflow - Unified batch + streaming - Managed service - Auto-scaling</p> <p>Spark Structured Streaming - Streaming API on Spark - Can reuse batch code - Mature</p>"},{"location":"reference/future-trends/#benefits","title":"Benefits","text":"<ul> <li>Code reuse: Same logic for batch and streaming</li> <li>Consistency: Same results</li> <li>Simplicity: One framework to learn</li> </ul>"},{"location":"reference/future-trends/#adoption","title":"Adoption","text":"<p>Adopt when: - Need both batch and streaming - Want code reuse - Team can learn unified framework</p> <p>Timeline: Gradual adoption as needs arise</p>"},{"location":"reference/future-trends/#serverless-managed-services","title":"Serverless &amp; Managed Services","text":""},{"location":"reference/future-trends/#trend_2","title":"Trend","text":"<p>Shift to managed services: - Less operations overhead - Auto-scaling - Pay per use - Faster time to value</p>"},{"location":"reference/future-trends/#examples","title":"Examples","text":"<ul> <li>BigQuery: Serverless data warehouse</li> <li>Dataflow: Managed Spark/Flink</li> <li>Fivetran: Managed ingestion</li> <li>dbt Cloud: Managed dbt</li> </ul>"},{"location":"reference/future-trends/#trade-offs","title":"Trade-offs","text":"<p>Pros: - Less operations - Auto-scaling - Faster development</p> <p>Cons: - Vendor lock-in - Can be expensive at scale - Less control</p>"},{"location":"reference/future-trends/#recommendation","title":"Recommendation","text":"<p>Use managed when: - Small-medium team - Want to move fast - Cost acceptable</p> <p>Use self-managed when: - Large scale (cost matters) - Need control - Have operations team</p>"},{"location":"reference/future-trends/#observability-first","title":"Observability-First","text":""},{"location":"reference/future-trends/#trend_3","title":"Trend","text":"<p>Observability as first-class concern: - Built into platforms - Rich metrics, logs, traces - Proactive alerting - Self-healing</p>"},{"location":"reference/future-trends/#components","title":"Components","text":"<ul> <li>Metrics: Volume, latency, quality, cost</li> <li>Logs: Structured, searchable</li> <li>Traces: End-to-end request flow</li> <li>Profiling: Performance analysis</li> </ul>"},{"location":"reference/future-trends/#tools_2","title":"Tools","text":"<ul> <li>Grafana: Dashboards, alerting</li> <li>Datadog: Full-stack observability</li> <li>OpenTelemetry: Standard for traces</li> <li>Custom: Platform-specific</li> </ul>"},{"location":"reference/future-trends/#adoption_1","title":"Adoption","text":"<p>Start with: - Basic metrics (volume, latency, errors) - Key alerts (failures, SLA violations) - Simple dashboards</p> <p>Evolve to: - Comprehensive observability - Predictive alerting - Self-healing systems</p>"},{"location":"reference/future-trends/#what-to-watch","title":"What to Watch","text":""},{"location":"reference/future-trends/#emerging-technologies","title":"Emerging Technologies","text":"<p>1. DuckDB - In-process analytical database - Fast for analytical queries - Growing adoption</p> <p>2. Apache Arrow - In-memory columnar format - Zero-copy data sharing - Foundation for many tools</p> <p>3. Data Products - Treating data as products - Product thinking applied to data - Growing movement</p>"},{"location":"reference/future-trends/#industry-shifts","title":"Industry Shifts","text":"<p>1. Cost optimization focus - More attention to cost - Better cost tools - Cost as first-class metric</p> <p>2. Developer experience - Better tooling - Faster iteration - Less friction</p> <p>3. Governance &amp; compliance - Stronger requirements - Better tooling - Automated compliance</p>"},{"location":"reference/future-trends/#recommendations","title":"Recommendations","text":""},{"location":"reference/future-trends/#for-your-platform","title":"For Your Platform","text":"<p>Near-term (6-12 months): 1. Adopt data contracts (start small) 2. Improve observability (metrics, alerts) 3. Optimize costs (quick wins) 4. Evaluate feature store (if doing ML)</p> <p>Medium-term (1-2 years): 1. Evolve toward platform thinking (self-serve) 2. Consider data mesh (if org fits) 3. Adopt unified batch + streaming (if needed) 4. Enhance AI-assisted tooling</p> <p>Long-term (2-3 years): 1. Full platform maturity 2. Data mesh (if appropriate) 3. Advanced observability (predictive, self-healing) 4. Stay current with trends</p>"},{"location":"reference/future-trends/#staying-current","title":"Staying Current","text":"<p>Ways to stay informed: - Follow industry blogs (Airbyte, dbt, etc.) - Attend conferences (Data Council, Strata) - Join communities (Data Engineering Podcast, Slack groups) - Experiment with new tools (in non-critical areas)</p> <p>Principle: Don't chase every trend. Adopt when it solves real problems.</p>"},{"location":"reference/future-trends/#next-steps","title":"Next Steps","text":"<ul> <li>Leadership View - How to evaluate and adopt trends</li> <li>Foundations - Back to basics</li> </ul>"},{"location":"reference/leadership-view/","title":"Leadership View","text":""},{"location":"reference/leadership-view/#manager-leadership-view","title":"Manager / Leadership View","text":"<p>\"The next generation doesn't need more dashboards. They need better stories about why the data matters.\"</p> <p>This chapter is for data engineering leaders\u2014managers, directors, and executives who need to build, scale, and measure data platform teams. It provides frameworks for making strategic decisions and evaluating success.</p> <p>\"If Gen-Z doesn't care about your data problem, you've explained the wrong problem.\"</p>"},{"location":"reference/leadership-view/#what-to-measure","title":"What to Measure","text":""},{"location":"reference/leadership-view/#platform-health-metrics","title":"Platform Health Metrics","text":"<p>Adoption Metrics - % of data sources on platform: Target 80%+ within 12 months - % of transformations on platform: Target 90%+ within 18 months - Active users per month: Track growth, target 20%+ MoM early on - Self-serve adoption: % of pipelines created via self-serve (target 70%+)</p> <p>Why it matters: Low adoption = platform isn't delivering value.</p> <p>Reliability Metrics - Platform uptime: Target 99.9% (8.76 hours downtime/year) - Pipeline success rate: Target &gt; 99% (less than 1% failure rate) - Mean time to recovery (MTTR): Target &lt; 1 hour for critical pipelines - Incident frequency: Track and trend (target: decreasing)</p> <p>Why it matters: Unreliable platform = teams won't trust it.</p> <p>Performance Metrics - Ingestion latency: p50, p95, p99 (target: meet SLAs) - Query performance: p50, p95, p99 (target: &lt; 5s p95 for common queries) - Transformation time: Track and optimize (target: &lt; SLA) - Resource utilization: CPU, memory (target: 60-80% for efficiency)</p> <p>Why it matters: Slow platform = poor developer experience.</p>"},{"location":"reference/leadership-view/#developer-experience-metrics","title":"Developer Experience Metrics","text":"<p>Time to Value - Time to first ingestion: Target &lt; 1 day (from request to data available) - Time to first transformation: Target &lt; 2 days - Time to production: Track end-to-end (target: &lt; 1 week for standard use cases)</p> <p>Developer Satisfaction - NPS (Net Promoter Score): Survey quarterly, target 50+ - Support ticket volume: Track and trend (target: decreasing as platform matures) - Documentation usage: Track views, searches (target: high usage = good docs)</p> <p>Self-Serve Adoption - % of pipelines created via self-serve: Target 70%+ - % of transformations using standard frameworks: Target 80%+ - Support tickets per 100 pipelines: Target &lt; 5 (fewer tickets = better self-serve)</p> <p>Why it matters: Poor DX = slow adoption, high support burden.</p>"},{"location":"reference/leadership-view/#cost-metrics","title":"Cost Metrics","text":"<p>Cost Efficiency - Cost per GB ingested: Track and trend (target: decreasing) - Cost per query: Track by query type (target: optimize expensive queries) - Storage cost per GB: Track by tier (target: move to cheaper tiers) - Total cost of ownership: Platform + operations + developer time</p> <p>Cost Attribution - Cost by team: Showback/chargeback (target: teams see and optimize their costs) - Cost by source: Identify expensive sources (target: optimize or deprecate) - Cost by consumer: Identify expensive queries (target: optimize)</p> <p>Budget Management - Monthly spend vs budget: Track and forecast - Cost growth rate: Target &lt; 20% YoY (after initial growth phase) - ROI of optimizations: Track savings from projects</p> <p>Why it matters: Uncontrolled costs = platform becomes unsustainable.</p>"},{"location":"reference/leadership-view/#business-impact-metrics","title":"Business Impact Metrics","text":"<p>Data Availability - % of critical data sources with SLAs: Target 100% - SLA compliance rate: Target &gt; 99% - Data freshness: % of sources meeting freshness SLAs (target &gt; 95%)</p> <p>Data Quality - Quality score: Composite score across dimensions (target &gt; 95%) - Quality incidents: Track and trend (target: decreasing) - Time to detect quality issues: Target &lt; 1 hour</p> <p>Business Value - Downstream consumers: Number of teams/products using platform data - Query volume: Track growth (indicates value) - Data products enabled: New products/features enabled by platform</p> <p>Why it matters: Platform exists to enable business, not as an end in itself.</p>"},{"location":"reference/leadership-view/#how-to-scale-teams","title":"How to Scale Teams","text":""},{"location":"reference/leadership-view/#team-structure","title":"Team Structure","text":"<p>Small Team (&lt; 10 engineers) - Structure: Generalists (everyone does everything) - Roles: 2-3 platform engineers, 1 part-time SRE - Focus: Get platform working, establish patterns</p> <p>Medium Team (10-50 engineers) - Structure: Some specialization (platform, ingestion, transformations) - Roles: 5-10 platform engineers, 1-2 SRE, 1 PM - Focus: Scale platform, improve self-serve, optimize costs</p> <p>Large Team (50+ engineers) - Structure: Specialized teams (platform, ingestion, transformations, quality) - Roles: 15-30 platform engineers, 3-5 SRE, 2-3 PM, cost optimization team - Focus: Platform maturity, advanced capabilities, cost efficiency</p>"},{"location":"reference/leadership-view/#hiring-strategy","title":"Hiring Strategy","text":"<p>Early Stage (0-10 engineers) - Hire: Senior generalists (can do everything) - Skills: Platform engineering, data engineering, operations - Experience: 5+ years, worked at scale</p> <p>Growth Stage (10-50 engineers) - Hire: Mix of generalists and specialists - Skills: Platform engineering, specific domains (ingestion, transformations) - Experience: 3-7 years, relevant domain expertise</p> <p>Mature Stage (50+ engineers) - Hire: Specialists + leaders - Skills: Deep expertise in specific areas, leadership - Experience: 5+ years, leadership experience for senior roles</p>"},{"location":"reference/leadership-view/#team-development","title":"Team Development","text":"<p>Career Paths - Individual Contributor (IC): Engineer \u2192 Senior \u2192 Staff \u2192 Principal - Management: Engineer \u2192 Tech Lead \u2192 Manager \u2192 Director - Hybrid: Tech Lead (IC + leadership)</p> <p>Skills Development - Technical: Platform engineering, data engineering, cloud, tooling - Soft: Communication, collaboration, product thinking - Leadership: For senior roles (influence, strategy, mentoring)</p> <p>Retention - Clear career paths: Growth opportunities - Interesting work: Challenging problems, impact - Compensation: Competitive, aligned with market - Culture: Learning, collaboration, ownership</p>"},{"location":"reference/leadership-view/#evaluating-architecture-maturity","title":"Evaluating Architecture Maturity","text":""},{"location":"reference/leadership-view/#maturity-model","title":"Maturity Model","text":"<p>Level 1: Ad-Hoc - Manual pipeline creation - No standard patterns - Limited observability - High operational burden - Indicators: Everything is custom, high support burden</p> <p>Level 2: Standardized - Common patterns documented - Some self-serve capabilities - Basic monitoring - Platform team bottleneck - Indicators: Some standards, but still manual for many things</p> <p>Level 3: Self-Serve Platform - Most tasks self-serve - Clear contracts and SLAs - Cost attribution - Platform enables, doesn't block - Indicators: 70%+ self-serve, low support burden, teams move fast</p> <p>Level 4: Product Platform - Full self-serve - Predictive quality - Automated optimization - Platform as competitive advantage - Indicators: Minimal platform team involvement, high satisfaction, innovation</p>"},{"location":"reference/leadership-view/#assessment-framework","title":"Assessment Framework","text":"<p>Evaluate across dimensions:</p> Dimension Level 1 Level 2 Level 3 Level 4 Ingestion Manual, custom Some templates Self-serve, standardized Fully automated Transformation Ad-hoc scripts Some frameworks Standard frameworks, self-serve Optimized, automated Storage Ad-hoc, no standards Some standards Tiered, lifecycle policies Optimized, predictive Quality Manual checks Some automated Comprehensive, automated Predictive, self-healing Governance Ad-hoc Basic policies Contracts, automated Federated, self-service Observability Limited Basic metrics Comprehensive Predictive Cost Unattributed Some attribution Full attribution, optimization Automated optimization <p>Scoring: Rate each dimension 1-4, average = maturity level.</p>"},{"location":"reference/leadership-view/#roadmap-to-maturity","title":"Roadmap to Maturity","text":"<p>Level 1 \u2192 2 (6-12 months) - Document common patterns - Create standard templates - Basic monitoring - Establish SLAs</p> <p>Level 2 \u2192 3 (12-18 months) - Build self-serve capabilities - Implement contracts - Cost attribution - Improve observability</p> <p>Level 3 \u2192 4 (18-24 months) - Advanced automation - Predictive capabilities - Self-healing - Innovation</p>"},{"location":"reference/leadership-view/#strategic-decisions","title":"Strategic Decisions","text":""},{"location":"reference/leadership-view/#build-vs-buy","title":"Build vs Buy","text":"<p>Build when: - Unique requirements (no tool fits) - Competitive advantage (platform is differentiator) - Have engineering resources - Cost of buying &gt; cost of building</p> <p>Buy when: - Standard requirements (tools exist) - Want to move fast (time to market) - Limited engineering resources - Cost of building &gt; cost of buying</p> <p>Hybrid approach (recommended): - Buy for standard capabilities (ingestion, orchestration) - Build for unique/differentiating capabilities - Customize bought tools as needed</p>"},{"location":"reference/leadership-view/#central-vs-decentralized","title":"Central vs Decentralized","text":"<p>Central when: - Need strong governance - Limited domain expertise - Want consistency - Large organization (1000+ engineers)</p> <p>Decentralized when: - Need speed and autonomy - Strong domain expertise - Smaller organization (&lt; 100 engineers)</p> <p>Hybrid (recommended for most): - Central platform team (infrastructure, standards) - Domain teams (business logic, transformations) - Shared governance (framework, not control)</p>"},{"location":"reference/leadership-view/#cloud-strategy","title":"Cloud Strategy","text":"<p>Single cloud when: - Simpler operations - Better integration - Cost optimization (commitments) - Team expertise in one cloud</p> <p>Multi-cloud when: - Vendor risk mitigation - Regulatory requirements - Best-of-breed (different clouds for different needs) - Large scale (leverage competition)</p> <p>Recommendation: Start single cloud, consider multi-cloud as you scale.</p>"},{"location":"reference/leadership-view/#budget-planning","title":"Budget Planning","text":""},{"location":"reference/leadership-view/#cost-components","title":"Cost Components","text":"<p>Infrastructure (40-60%) - Compute (queries, transformations) - Storage (hot, warm, cold) - Network (transfer, egress)</p> <p>Tools &amp; Licenses (10-20%) - SaaS tools (Fivetran, dbt Cloud) - Software licenses - Managed services</p> <p>Operations (10-15%) - Platform team (salaries) - On-call, incident response - Maintenance</p> <p>Development (10-15%) - New features - Optimizations - Experiments</p>"},{"location":"reference/leadership-view/#budget-planning-process","title":"Budget Planning Process","text":"<p>1. Baseline current spend - Track all costs (infrastructure, tools, people) - Categorize by team, project, source</p> <p>2. Forecast growth - Volume growth (data, queries, users) - Feature additions (new capabilities) - Team growth (headcount)</p> <p>3. Identify optimizations - Cost reduction opportunities - Efficiency improvements - Tool consolidation</p> <p>4. Build budget - Base: Current spend + growth - Optimizations: Apply savings - Buffer: 10-20% for unknowns</p> <p>5. Track and adjust - Monthly reviews - Quarterly forecasts - Adjust as needed</p>"},{"location":"reference/leadership-view/#roi-framework","title":"ROI Framework","text":"<p>Calculate ROI for investments:</p> <pre><code># Example: Self-serve ingestion project\nengineering_cost = 200  # hours * $150/hour = $30,000\ntime_saved_per_pipeline = 8  # hours (manual \u2192 self-serve)\npipelines_per_month = 10\nhourly_rate = 150  # $/hour\n\nmonthly_savings = time_saved_per_pipeline * pipelines_per_month * hourly_rate\n# = 8 * 10 * 150 = $12,000/month\n\nannual_savings = monthly_savings * 12  # $144,000\nroi = (annual_savings - engineering_cost) / engineering_cost  # 380%\npayback_period = engineering_cost / monthly_savings  # 2.5 months\n</code></pre> <p>Decision rule: If payback &lt; 12 months and ROI &gt; 100%, do it.</p>"},{"location":"reference/leadership-view/#communication-reporting","title":"Communication &amp; Reporting","text":""},{"location":"reference/leadership-view/#executive-reporting","title":"Executive Reporting","text":"<p>Monthly report structure: 1. Platform health: Uptime, success rates, performance 2. Adoption: Users, pipelines, growth 3. Cost: Spend vs budget, trends, optimizations 4. Business impact: Data products enabled, consumers 5. Risks &amp; issues: What's blocking, what needs attention</p> <p>Format: 1-2 pages, visual (charts), actionable</p>"},{"location":"reference/leadership-view/#team-communication","title":"Team Communication","text":"<p>Regular updates: - Weekly: Team standup, platform health - Monthly: All-hands, metrics review - Quarterly: Roadmap, strategy</p> <p>Channels: - Slack/Teams: Day-to-day - Email: Important announcements - Wiki/Docs: Documentation, runbooks</p>"},{"location":"reference/leadership-view/#stakeholder-management","title":"Stakeholder Management","text":"<p>Key stakeholders: - Engineering leaders: Platform capabilities, reliability - Product leaders: Data products, business impact - Finance: Cost, budget, ROI - Executives: Strategy, business value</p> <p>Tailor message to audience: - Engineers: Technical details, capabilities - Product: Business impact, features - Finance: Cost, efficiency - Executives: Strategy, value, risks</p>"},{"location":"reference/leadership-view/#common-pitfalls","title":"Common Pitfalls","text":""},{"location":"reference/leadership-view/#pitfall-1-over-engineering","title":"Pitfall 1: Over-Engineering","text":"<p>Symptom: Building complex solutions for simple problems.</p> <p>Solution: Start simple, add complexity only when needed.</p>"},{"location":"reference/leadership-view/#pitfall-2-ignoring-costs","title":"Pitfall 2: Ignoring Costs","text":"<p>Symptom: Costs growing unchecked.</p> <p>Solution: Track costs from day one, optimize continuously.</p>"},{"location":"reference/leadership-view/#pitfall-3-poor-developer-experience","title":"Pitfall 3: Poor Developer Experience","text":"<p>Symptom: Low adoption, high support burden.</p> <p>Solution: Invest in self-serve, documentation, tooling.</p>"},{"location":"reference/leadership-view/#pitfall-4-no-metrics","title":"Pitfall 4: No Metrics","text":"<p>Symptom: Can't measure success.</p> <p>Solution: Define metrics early, track religiously.</p>"},{"location":"reference/leadership-view/#pitfall-5-chasing-trends","title":"Pitfall 5: Chasing Trends","text":"<p>Symptom: Adopting every new tool/pattern.</p> <p>Solution: Adopt when it solves real problems, not because it's new.</p>"},{"location":"reference/leadership-view/#success-criteria","title":"Success Criteria","text":""},{"location":"reference/leadership-view/#platform-success","title":"Platform Success","text":"<p>Platform is successful when: - Adoption: 80%+ of data sources on platform - Reliability: 99.9% uptime, &lt; 1% failure rate - Developer experience: &lt; 1 day to first ingestion, high satisfaction - Cost: Controlled growth, optimized spend - Business impact: Enabling new products, features, insights</p>"},{"location":"reference/leadership-view/#team-success","title":"Team Success","text":"<p>Team is successful when: - Platform maturity: Level 3+ (self-serve platform) - Team growth: Retaining talent, growing capabilities - Innovation: Building new capabilities, staying current - Impact: Clear business value, recognition</p>"},{"location":"reference/leadership-view/#next-steps","title":"Next Steps","text":"<ul> <li>Review Foundations - Ensure solid foundation</li> <li>Review Platform &amp; Operating Model - Design your operating model</li> <li>Review Cost Efficiency - Optimize costs</li> </ul> <p>Remember: Building a data platform is a journey, not a destination. Start simple, measure everything, iterate based on data.</p>"},{"location":"reference/tooling-landscape/","title":"Tooling Landscape","text":""},{"location":"reference/tooling-landscape/#tooling-landscape","title":"Tooling Landscape","text":"<p>The data engineering tooling ecosystem is vast and rapidly evolving. This chapter provides an opinionated guide to selecting and using tools effectively, organized by category.</p>"},{"location":"reference/tooling-landscape/#tool-selection-principles","title":"Tool Selection Principles","text":""},{"location":"reference/tooling-landscape/#criteria","title":"Criteria","text":"<ol> <li>Fit for purpose: Does it solve your specific problem?</li> <li>Maturity: Is it production-ready? (avoid bleeding edge for critical systems)</li> <li>Ecosystem: Does it integrate with your stack?</li> <li>Cost: Total cost of ownership (license + infrastructure + operations)</li> <li>Vendor lock-in: Can you migrate if needed?</li> <li>Community: Is there support, documentation, talent?</li> </ol>"},{"location":"reference/tooling-landscape/#decision-framework","title":"Decision Framework","text":"<p>For each tool category: 1. Define requirements (must-have vs nice-to-have) 2. Evaluate 2-3 options 3. POC (proof of concept) if significant investment 4. Standardize on one (avoid tool sprawl)</p>"},{"location":"reference/tooling-landscape/#ingestion-tools","title":"Ingestion Tools","text":""},{"location":"reference/tooling-landscape/#managed-saas-zero-maintenance","title":"Managed SaaS (Zero Maintenance)","text":"<p>Fivetran - Best for: Database replication, zero maintenance - Pros: Managed, reliable, many connectors - Cons: Expensive at scale, limited customization - Cost: ~$1-2 per 10K rows/month - Use when: Small-medium volume, want zero ops</p> <p>Stitch - Best for: Simple extracts, cost-effective - Pros: Cheaper than Fivetran, simple - Cons: Fewer connectors, less reliable - Cost: ~$0.5-1 per 10K rows/month - Use when: Cost-sensitive, simple use cases</p> <p>Airbyte (Open Source) - Best for: Self-hosted, customizable - Pros: Free, open source, extensible - Cons: Requires operations, less mature - Cost: Infrastructure only - Use when: Large scale, need customization</p>"},{"location":"reference/tooling-landscape/#self-managed","title":"Self-Managed","text":"<p>Debezium (CDC) - Best for: Kafka-based CDC, database replication - Pros: Open source, reliable, Kafka-native - Cons: Requires Kafka infrastructure - Cost: Infrastructure only - Use when: Already using Kafka, need CDC</p> <p>Kafka Connect - Best for: Kafka ecosystem, extensible connectors - Pros: Many connectors, Kafka-native - Cons: Requires Kafka operations - Cost: Infrastructure only - Use when: Kafka-first architecture</p> <p>Custom Scripts (Airflow + Spark) - Best for: Full control, custom logic - Pros: Complete flexibility - Cons: High maintenance, slower development - Cost: Infrastructure + engineering time - Use when: Unique requirements, have engineering resources</p>"},{"location":"reference/tooling-landscape/#cloud-native","title":"Cloud-Native","text":"<p>Google Datastream - Best for: GCP-native CDC, managed service - Pros: Managed, GCP-integrated, reliable - Cons: GCP-only, expensive - Cost: ~$0.05-0.10 per GB processed - Use when: GCP stack, need managed CDC</p> <p>AWS DMS (Database Migration Service) - Best for: AWS-native replication, migrations - Pros: Managed, AWS-integrated - Cons: AWS-only, can be expensive - Cost: Per instance hour + data transfer - Use when: AWS stack, database replication</p>"},{"location":"reference/tooling-landscape/#orchestration-tools","title":"Orchestration Tools","text":""},{"location":"reference/tooling-landscape/#apache-airflow","title":"Apache Airflow","text":"<p>Best for: Complex workflows, Python-based, open source</p> <p>Pros: - Mature, widely adopted - Rich ecosystem (operators, sensors) - Flexible (Python-based) - Good UI and monitoring</p> <p>Cons: - Requires operations (self-hosted) - Can be complex for simple workflows - Resource-intensive</p> <p>Use when: Complex DAGs, need flexibility, have operations team</p> <p>Alternatives: Prefect, Dagster (modern Python alternatives)</p>"},{"location":"reference/tooling-landscape/#managed-airflow","title":"Managed Airflow","text":"<p>Google Cloud Composer - Managed Airflow on GCP - Pros: No ops, GCP-integrated - Cons: Expensive, GCP-only - Cost: ~$0.10 per vCPU-hour</p> <p>AWS MWAA (Managed Workflows) - Managed Airflow on AWS - Pros: No ops, AWS-integrated - Cons: Expensive, AWS-only - Cost: ~$0.49 per hour base + usage</p>"},{"location":"reference/tooling-landscape/#dbt-data-build-tool","title":"dbt (Data Build Tool)","text":"<p>Best for: SQL-based transformations, analytics engineering</p> <p>Pros: - SQL-based (accessible to analysts) - Great testing framework - Documentation generation - Version control friendly</p> <p>Cons: - SQL-only (limited for complex logic) - Requires orchestration (Airflow, etc.)</p> <p>Use when: SQL-heavy transformations, analytics team</p>"},{"location":"reference/tooling-landscape/#prefect-dagster","title":"Prefect / Dagster","text":"<p>Best for: Modern Python orchestration, better DX than Airflow</p> <p>Pros: - Better developer experience - Modern architecture - Good testing support</p> <p>Cons: - Less mature than Airflow - Smaller ecosystem</p> <p>Use when: Starting new, Python-heavy, want modern tooling</p>"},{"location":"reference/tooling-landscape/#transformation-frameworks","title":"Transformation Frameworks","text":""},{"location":"reference/tooling-landscape/#spark","title":"Spark","text":"<p>Best for: Large-scale batch processing, complex transformations</p> <p>Pros: - Handles petabytes - Rich APIs (SQL, Python, Scala, R) - Mature ecosystem - Good performance</p> <p>Cons: - Complex (steep learning curve) - Resource-intensive - Requires tuning</p> <p>Use when: Large volumes, complex logic, need performance</p> <p>Managed options: Databricks, EMR, Dataproc</p>"},{"location":"reference/tooling-landscape/#flink","title":"Flink","text":"<p>Best for: Streaming, stateful processing, low latency</p> <p>Pros: - Excellent for streaming - Stateful processing - Low latency - Good performance</p> <p>Cons: - Complex - Steeper learning curve than Spark - Requires operations</p> <p>Use when: Real-time streaming, stateful processing</p> <p>Managed options: Ververica, Kinesis Analytics, Dataflow</p>"},{"location":"reference/tooling-landscape/#dbt","title":"dbt","text":"<p>Best for: SQL transformations, analytics engineering</p> <p>Pros: - SQL-based (accessible) - Great testing - Documentation - Modular (macros, models)</p> <p>Cons: - SQL-only - Limited for complex logic</p> <p>Use when: Analytics workloads, SQL-heavy</p>"},{"location":"reference/tooling-landscape/#dataflow-google","title":"Dataflow (Google)","text":"<p>Best for: GCP-native, auto-scaling, batch + streaming</p> <p>Pros: - Managed, auto-scaling - Unified batch/streaming - GCP-integrated - Pay per use</p> <p>Cons: - GCP-only - Can be expensive - Less flexible than Spark/Flink</p> <p>Use when: GCP stack, want managed, batch + streaming</p>"},{"location":"reference/tooling-landscape/#storage-query-engines","title":"Storage &amp; Query Engines","text":""},{"location":"reference/tooling-landscape/#data-warehouses","title":"Data Warehouses","text":"<p>BigQuery - Best for: GCP-native, serverless, analytics - Pros: Serverless, auto-scaling, fast - Cons: GCP-only, can be expensive - Cost: $5/TB queried, $0.02/GB stored</p> <p>Snowflake - Best for: Multi-cloud, performance, enterprise - Pros: Multi-cloud, excellent performance, enterprise features - Cons: Expensive, vendor lock-in - Cost: Per credit (compute) + storage</p> <p>Redshift - Best for: AWS-native, cost-effective at scale - Pros: AWS-integrated, cost-effective - Cons: Requires management, less flexible - Cost: Per instance + storage</p> <p>Databricks SQL - Best for: Lakehouse, Spark ecosystem - Pros: Lakehouse (Delta), Spark integration - Cons: Can be expensive - Cost: Per DBU (Databricks Unit)</p>"},{"location":"reference/tooling-landscape/#data-lakes","title":"Data Lakes","text":"<p>S3 + Spark / Presto - Best for: Cost-effective, flexible - Pros: Very cheap storage, flexible - Cons: Requires compute engine, more complex - Cost: $0.023/GB (S3 Standard)</p> <p>GCS + BigQuery - Best for: GCP-native, lakehouse - Pros: Integrated, can query directly - Cons: GCP-only - Cost: Storage + query costs</p> <p>ADLS + Databricks - Best for: Azure-native, lakehouse - Pros: Azure-integrated, Delta Lake - Cons: Azure-only - Cost: Storage + compute</p>"},{"location":"reference/tooling-landscape/#lakehouse-formats","title":"Lakehouse Formats","text":"<p>Delta Lake - Best for: ACID transactions, time travel, upserts - Pros: ACID, time travel, schema evolution - Cons: Requires compatible engines - Use when: Need updates, time travel</p> <p>Apache Iceberg - Best for: Open format, multi-engine support - Pros: Open, multi-engine, good performance - Cons: Less mature than Delta - Use when: Want open format, multi-engine</p> <p>Apache Hudi - Best for: Real-time updates, incremental processing - Pros: Real-time updates, incremental - Cons: Less mature, smaller ecosystem - Use when: Real-time updates needed</p>"},{"location":"reference/tooling-landscape/#metadata-discovery","title":"Metadata &amp; Discovery","text":""},{"location":"reference/tooling-landscape/#datahub-linkedin","title":"DataHub (LinkedIn)","text":"<p>Best for: Open source, comprehensive metadata</p> <p>Pros: - Open source - Comprehensive (lineage, ownership, usage) - Good UI - Active community</p> <p>Cons: - Requires operations - Can be complex to set up</p> <p>Use when: Want open source, comprehensive solution</p>"},{"location":"reference/tooling-landscape/#collibra","title":"Collibra","text":"<p>Best for: Enterprise governance, compliance</p> <p>Pros: - Enterprise features - Strong governance - Compliance tools</p> <p>Cons: - Expensive - Can be heavy/overkill</p> <p>Use when: Enterprise, need strong governance</p>"},{"location":"reference/tooling-landscape/#aws-glue-catalog","title":"AWS Glue Catalog","text":"<p>Best for: AWS-native, simple metadata</p> <p>Pros: - AWS-integrated - Managed - Simple</p> <p>Cons: - AWS-only - Limited features</p> <p>Use when: AWS stack, simple needs</p>"},{"location":"reference/tooling-landscape/#custom-solutions","title":"Custom Solutions","text":"<p>Build your own: - Pros: Full control, tailored - Cons: High maintenance, reinventing wheel - Use when: Unique requirements, have resources</p>"},{"location":"reference/tooling-landscape/#observability-monitoring","title":"Observability &amp; Monitoring","text":""},{"location":"reference/tooling-landscape/#grafana","title":"Grafana","text":"<p>Best for: Dashboards, visualization</p> <p>Pros: - Excellent dashboards - Many data sources - Open source - Flexible</p> <p>Cons: - Requires setup - Alerting can be complex</p> <p>Use when: Need dashboards, have operations</p>"},{"location":"reference/tooling-landscape/#datadog","title":"Datadog","text":"<p>Best for: Full-stack observability, SaaS</p> <p>Pros: - Comprehensive (metrics, logs, traces) - SaaS (no ops) - Good integrations - Great UI</p> <p>Cons: - Expensive at scale - Vendor lock-in</p> <p>Use when: Want SaaS, comprehensive solution</p>"},{"location":"reference/tooling-landscape/#cloud-native_1","title":"Cloud Native","text":"<p>CloudWatch (AWS) - AWS-native monitoring - Pros: AWS-integrated, managed - Cons: AWS-only, can be expensive</p> <p>Cloud Monitoring (GCP) - GCP-native monitoring - Pros: GCP-integrated, managed - Cons: GCP-only</p> <p>Azure Monitor - Azure-native monitoring - Pros: Azure-integrated, managed - Cons: Azure-only</p>"},{"location":"reference/tooling-landscape/#quality-testing","title":"Quality &amp; Testing","text":""},{"location":"reference/tooling-landscape/#great-expectations","title":"Great Expectations","text":"<p>Best for: Data quality testing, validation</p> <p>Pros: - Comprehensive testing framework - Good integrations - Open source - Active community</p> <p>Cons: - Can be complex - Requires setup</p> <p>Use when: Need comprehensive quality testing</p>"},{"location":"reference/tooling-landscape/#dbt-tests","title":"dbt Tests","text":"<p>Best for: SQL-based quality checks</p> <p>Pros: - Integrated with dbt - SQL-based (accessible) - Simple</p> <p>Cons: - dbt-only - Limited compared to Great Expectations</p> <p>Use when: Using dbt, simple quality checks</p>"},{"location":"reference/tooling-landscape/#custom-validators","title":"Custom Validators","text":"<p>Build your own: - Pros: Tailored to needs - Cons: Maintenance overhead - Use when: Unique requirements</p>"},{"location":"reference/tooling-landscape/#recommendation-matrix","title":"Recommendation Matrix","text":""},{"location":"reference/tooling-landscape/#small-team-10-engineers","title":"Small Team (&lt; 10 engineers)","text":"<p>Ingestion: Fivetran or Stitch (SaaS) Orchestration: Managed Airflow (Composer/MWAA) or Prefect Cloud Transformation: dbt + BigQuery/Snowflake Storage: BigQuery or Snowflake Metadata: DataHub (self-hosted) or Glue Catalog Monitoring: Cloud native (CloudWatch/Cloud Monitoring)</p>"},{"location":"reference/tooling-landscape/#medium-team-10-50-engineers","title":"Medium Team (10-50 engineers)","text":"<p>Ingestion: Mix (Fivetran for simple, Debezium/Kafka for complex) Orchestration: Airflow (self-hosted or managed) Transformation: Spark + dbt Storage: Lakehouse (S3/GCS + Spark + warehouse) Metadata: DataHub Monitoring: Grafana + Prometheus or Datadog</p>"},{"location":"reference/tooling-landscape/#large-team-50-engineers","title":"Large Team (50+ engineers)","text":"<p>Ingestion: Kafka Connect + Debezium + custom Orchestration: Airflow (self-hosted, multiple instances) Transformation: Spark + Flink + dbt Storage: Multi-tier (lake + warehouse) Metadata: DataHub or Collibra Monitoring: Full stack (Grafana + Datadog + custom)</p>"},{"location":"reference/tooling-landscape/#tool-sprawl-prevention","title":"Tool Sprawl Prevention","text":""},{"location":"reference/tooling-landscape/#principles","title":"Principles","text":"<ol> <li>Standardize: One tool per category (avoid 3 different orchestration tools)</li> <li>Justify exceptions: Require approval for new tools</li> <li>Regular review: Audit tools annually, deprecate unused</li> <li>Documentation: Clear guidance on when to use what</li> </ol>"},{"location":"reference/tooling-landscape/#governance","title":"Governance","text":"<p>Tool registry: - Approved tools (standard) - Approved with justification (exceptions) - Deprecated (being phased out) - Prohibited (security/compliance issues)</p>"},{"location":"reference/tooling-landscape/#next-steps","title":"Next Steps","text":"<ul> <li>Future &amp; Emerging Trends - What's coming next</li> <li>Leadership View - Evaluating tool choices</li> </ul>"}]}